{"title":"[under development] Web scraping with Rvest package","markdown":{"yaml":{"title":"[under development] Web scraping with Rvest package","description":"\"Rvest\" is a package for web scraping and harvesting which is developed by Hadley Wickham. He states that he was inspired by python libraries such as \"beautiful soup\" and \"RoboBrowser\".\n","author":[{"name":"Ali Emre Karagül","orcid":"0000-0002-5820-8643","email":"aliemrekaragul@gmail.com","affiliations":[{"name":"TOBB ETU- University of Economics & Technology"}]}],"date":"2023-01-06","categories":["Rvest","Wordcloud","Data-viz"],"image":"image.png"},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = FALSE)\n```\n\n\nIn this post, we will delve into harvesting a web page, [Ekşi Sözlük](https://eksisozluk.com/). This process won't include the automation of the process. Ekşi Sözlük is a reddit-like web site where users share their ideas on certain topics. Our target topic is \"veri bilimi\" (a.k.a. data science in English).\n\nThe R packages that we use in this post are as follows:\n\n```{r eval=TRUE, echo=FALSE, code_folding=FALSE}\n#| warning: false\n#| eval: true\n#| echo: true\n#| code-fold: false\nlibrary(rvest)\nlibrary(dplyr)\nlibrary(wordcloud)\nlibrary(tm)\n```\n\n## Processing\n\nFirst things first; we start by introducing the webpage that we want to harvest.\n\n```{r}\n#| warning: false\n#| eval: true\n#| echo: true\n#| code-fold: false\nhtml <- read_html(\"https://eksisozluk.com/veri-bilimi--3426406\")\n```\n\nRvest allos us to collect any type of HTML tag from the current page. Let's suppose we would like to collect all the links in a topic page:\n\n```{r}\n#| warning: false\n#| eval: true\n#| echo: true\n#| code-fold: false\n\nlinks <- html %>% html_nodes(\"a.url\")  %>%  html_attr(\"href\")\nlinks\n```\n\nOr maybe we would like to collect all the entries on the given page:\n\n```{r}\n#| warning: false\n#| eval: true\n#| echo: true\n#| code-fold: false\nentries        <- html %>% html_nodes(\".content\") %>%html_text()    \n\n#Let's see the first three entries:\nhead(entries, 3)\n```\n\nNow that you have all the entries in a page, it is easy to carry out a text analysis with it. Let's simply create a word cloud:\n\n```{r}\n#| warning: false\n#| eval: true\n#| echo: true\n#| code-fold: false\n#| \n#turn entries into corpus\n\nentries<-Corpus(VectorSource(entries))\n\n#apply several functions such as remove punctuation or numbers etc.\n\nentries <- entries %>%\n  tm_map(removeNumbers) %>%\n  tm_map(removePunctuation) %>%\n  tm_map(stripWhitespace)\nentries <- tm_map(entries, content_transformer(tolower))\n\n#turn into a matrix\nterm_matrix <- as.matrix(TermDocumentMatrix(entries) )\n\n#frequency table:\nword_freqs <- sort(rowSums(term_matrix),decreasing=TRUE) \nword_freqs <- data.frame(word=names(word_freqs),freq=word_freqs )\n\n## word cloud:\nwordcloud(words = word_freqs$word, freq = word_freqs$freq, min.freq = 1,          \n          max.words=200, random.order=FALSE, rot.per=0.35,            \n          colors=brewer.pal(8, \"Dark2\"))\n```\n\n## Conclusion\n\nThis post is not complete, yet will be completed soon.\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"toc-depth":4,"include-after-body":["../../footer.html"],"output-file":"index.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.269","editor":"visual","theme":"literia","toc-title":"Contents","citations-hover":true,"footnotes-hover":true,"mainfont":"roboto","background":"rgb(239, 239, 239)","fontcolor":"rgb(17, 43, 60)","fontsize":"1.1em","linestretch":1.5,"title-block-banner":true,"author":["Ali Emre Karagül",{"name":"Ali Emre Karagül","orcid":"0000-0002-5820-8643","email":"aliemrekaragul@gmail.com","affiliations":[{"name":"TOBB ETU- University of Economics & Technology"}]}],"page-layout":"article","comments":{"utterances":{"repo":"aliemrekaragul/blog-comments"}},"title":"[under development] Web scraping with Rvest package","description":"\"Rvest\" is a package for web scraping and harvesting which is developed by Hadley Wickham. He states that he was inspired by python libraries such as \"beautiful soup\" and \"RoboBrowser\".\n","date":"2023-01-06","categories":["Rvest","Wordcloud","Data-viz"],"image":"image.png"},"extensions":{"book":{"multiFile":true}}}}}