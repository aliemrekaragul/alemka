{"title":"Web scraping with Rvest package","markdown":{"yaml":{"title":"Web scraping with Rvest package","description":"In this post, we will delve into harvesting a web page, Ekşi Sözlük. This doesn't include the automation of the process. Yet, a completed web application for harvesting Ekşi Sözlük is available.\n","author":[{"name":"Ali Emre Karagül","orcid":"0000-0002-5820-8643","email":"aliemrekaragul@gmail.com","affiliations":[{"name":"TOBB ETU- University of Economics & Technology"}]}],"date":"2023-01-06","categories":["Rvest","Wordcloud","Data-viz"],"image":"image.gif"},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = FALSE)\n```\n\n\n#### In this post, we will delve into harvesting a web page, [Ekşi Sözlük](https://eksisozluk.com/). This process won't include the automation of the process. Yet, [a completed web application](https://aliemrekaragul.shinyapps.io/eksi-kazaR/) for harvesting Ekşi Sözlük is available.\n\nEkşi Sözlük is a reddit-like satirical web site where users share their ideas on certain topics in Turkish. Our target topic is \"veri bilimi\" (a.k.a. data science in English). The R packages that we use in this post are as follows:\n\n```{r eval=TRUE, echo=FALSE, code_folding=FALSE}\n#| warning: false\n#| eval: true\n#| echo: true\n#| code-fold: false\nlibrary(rvest)\nlibrary(dplyr)\nlibrary(tm)\nlibrary(stopwords)\nlibrary(wordcloud)\nlibrary(wordcloud2)\n```\n\n## Processing\n\n\"Rvest\" is a package for web scraping and harvesting which was developed by Hadley Wickham. He has stated that he was inspired by a few of the python libraries such as \"beautiful soup\" and \"RoboBrowser\".\n\nFirst things first; we start by introducing the webpage that we want to harvest.\n\n```{r}\n#| warning: false\n#| eval: true\n#| echo: true\n#| code-fold: false\nhtml <- read_html(\"https://eksisozluk.com/veri-bilimi--3426406\")\n```\n\nRvest allows us to collect any type of HTML tag from the current page. Let's suppose we would like to collect all the links in a topic page. First, go to the page and press F12 to open the \"devtools\" tab. Then turn inspect mode on. Hover on an item and check the node name for a random link. This process is shown below.\n\n![](images/image-1666293050.png){fig-align=\"center\" width=\"1045\"}\n\nThen we use the functions `html_nodes()` on the node name and `html_attr()` on \"href\" HTML tags. This will get all the href attributes with the node name \"a.url\" on the given link and turn them into a list:\n\n```{r}\n#| warning: false\n#| eval: true\n#| echo: true\n#| code-fold: false\n\nlinks <- html %>% html_nodes(\"a.url\")  %>%  html_attr(\"href\")\nlinks\n```\n\nIf we would like to collect all the entries on the given page, we will need the `div` attribute name for entries (also shown below).\n\n![](images/image-37950438.png)\n\nNow, we will use the function `html_text()` to collect the entries in these `div` attributes. There are about 10 entries some of which are quite long. Therefore, let me print only the first three entries via the `head()` function.\n\n```{r}\n#| warning: false\n#| eval: true\n#| echo: true\n#| code-fold: false\nentries <- html %>% html_nodes(\".content\") %>%html_text()    \n\n#Let's see the first three entries:\nhead(entries, 3)\n```\n\nNow that you have all the entries in a page, it is easy to carry out a text analysis with it. Let's simply create a word cloud. Here we'll use the `wordcloud` and `tm` packages. Functions in `tm` package works on a corpus type input. Hence, we change our entry list to a corpus using `Corpus()` and `VectorSource()` functions.\n\n```{r}\n#| warning: false\n#| eval: true\n#| echo: true\n#| code-fold: false\n#| \n#turn entries into corpus\n\nentries<-Corpus(VectorSource(entries))\n```\n\nAs we do not want unnecessary items in our word cloud, we will get rid of numbers, punctuation marks and spaces with `removeNumbers()`, `removePunctuation()`, and `stripWhitespace()`.\n\nAlso all letters will be transformed into lower case characters. We use `tm_map()` function to apply all the above mentioned functions on the corpus items.\n\n```{r}\n#| warning: false\n#| eval: true\n#| echo: true\n#| code-fold: false\n\n#apply several functions such as remove punctuation or numbers etc.\n\nentries <- entries %>%\n  tm_map(removeNumbers) %>%\n  tm_map(removePunctuation) %>%\n  tm_map(stripWhitespace)\nentries <- tm_map(entries, content_transformer(tolower))\n```\n\nOf course we would like to remove all the words that do not contribute to the context such as linkers, conjunctions, articles etc. For that, we can use the package `stopwords`. Turkish stopwords are defined in it and if you like you can add some other words to remove from your corpus via the `append()` function. We need to `unlist()` our corpus to apply `removeWords()` on it.\n\n```{r}\n#| warning: false\n#| eval: true\n#| echo: true\n#| code-fold: false\n\n\ntr_stopwords<-stopwords::stopwords(\"tr\", source = \"stopwords-iso\")    \ntr_stopwords<-append(tr_stopwords, c(\"bkz\",\"var\",\"vardir\",\"icin\", \"iste\", \"işte\",\"rn\",\"crn\"))\nentries<-unlist(entries)\nentries <- removeWords(entries, words=tr_stopwords)\n```\n\nActually, right now our corpus is ready, filled with all the meaningful words in the entries. However, to create a word cloud we need it as a frequency table. That's because a word's frequency will be used while determining its colour and size. To get a frequency table, we need a term matrix first. Following lines of codes will do the magic in this regard:\n\n```{r}\n#| warning: false\n#| eval: true\n#| echo: true\n#| code-fold: false\n\n#turn into a matrix\nterm_matrix <- as.matrix(TermDocumentMatrix(entries) )\n\n#frequency table:\nword_freqs <- sort(rowSums(term_matrix),decreasing=TRUE) \nword_freqs <- data.frame(word=names(word_freqs),freq=word_freqs )\n\n```\n\nWe will use two different packages for creating our word clouds. Initially, the classic word cloud takes the column names in the frequency table as `words` and `freq` arguments. Other arguments are also defined next to them as notes:\n\n```{r}\n#| warning: false\n#| eval: true\n#| echo: true\n#| code-fold: false\n\n\nwordcloud::wordcloud(words = word_freqs$word, \n                     freq = word_freqs$freq, \n                     min.freq = 1,  ## freqs lower than that wonW't be in the cloud\n                     max.words=200, ## Total number of words in the cloud\n                     random.order=FALSE,  ## words will be ordered according to their freqs.\n                     rot.per=0.35, ## rotation degree\n                     colors=brewer.pal(8, \"Dark2\")) ## some custimization for colors\n\n```\n\nThe second option is a more modern alternative. It is developed based on the JS framework with the same name. It gives you the opportunity to hover over the words and see their frequencies. Also, you can decide on a `shape` among several options such as `'circle'`, `'diamond'`, `'triangle-forward'`, `'triangle'`, `'pentagon'`, and `'star'`. Yet, some of these features won't be available if you want your cloud in .png format. That is because `wordcloud2` package provides us with clouds in HTML format which makes it easier for web page embedding but harder for saving on your local disk as .png or any other format.\n\n```{r}\n#| warning: false\n#| eval: true\n#| echo: true\n#| code-fold: false\n\nwordcloud2::wordcloud2 (word_freqs, \n                  size = 0.4,  ## this is basically zooming in the cloud. default is 1\n                  shape = \"star\", \n                  color='random-light', ## another option is 'random-dark'\n                  backgroundColor=\"black\" ) ## another option is 'white'\n```\n\n## Conclusion\n\nThere are many ways to use `Rvest` and scrap a website. We tried one of them in this post. In the end you can do many things with your output such as content analysis, clustering, sentimental analysis or even survival analysis. Here, in this post, we do not bare any pragmatic purposes so focused on creating something fun and eye-catching: word clouds. Also, scraping depends highly on your computer's processor and the number of pages you want to scrap. If you want to automate this process, it may take some time to complete. You can find [my Rshiny web application](https://aliemrekaragul.shinyapps.io/eksi-kazaR/) automating specifically the steps discussed here.\n\nAlthough we haven't mentioned here, `Rvest` works in alignment with the `polite` package. It allows you to scrap a web page in a polite way with permissions and greetings. The motto of the package `polite` sums it all for itself: *Be responsible when scraping data from websites by following polite principles: introduce yourself, ask for permission, take slowly and never ask twice.*\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"show","code-overflow":"scroll","code-link":true,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"toc-depth":4,"include-after-body":["../../footer.html"],"output-file":"web scraping with Rvest.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.269","editor":"visual","theme":"literia","toc-title":"Contents","citations-hover":true,"footnotes-hover":true,"mainfont":"roboto","background":"rgb(239, 239, 239)","fontcolor":"rgb(17, 43, 60)","fontsize":"1.1em","linestretch":1.5,"code-block-border-left":true,"title-block-banner":true,"page-layout":"article","comments":{"utterances":{"repo":"aliemrekaragul/blog-comments"}},"title":"Web scraping with Rvest package","description":"In this post, we will delve into harvesting a web page, Ekşi Sözlük. This doesn't include the automation of the process. Yet, a completed web application for harvesting Ekşi Sözlük is available.\n","author":[{"name":"Ali Emre Karagül","orcid":"0000-0002-5820-8643","email":"aliemrekaragul@gmail.com","affiliations":[{"name":"TOBB ETU- University of Economics & Technology"}]}],"date":"2023-01-06","categories":["Rvest","Wordcloud","Data-viz"],"image":"image.gif"},"extensions":{"book":{"multiFile":true}}}}}