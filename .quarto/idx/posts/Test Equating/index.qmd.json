{"title":"Test Equating","markdown":{"yaml":{"title":"Test Equating","description":"Equating two test forms:\nThis is a simple test equating study. The data used in this study is simulated from real data. We don't use the real data for privacy purposes here.\n","author":[{"name":"Ali Emre Karag√ºl","orcid":"0000-0002-5820-8643","email":"aliemrekaragul@gmail.com","affiliations":[{"name":"TOBB ETU- University of Economics & Technology"}]}],"date":"09.20.2022","categories":["equate"],"image":"image.jpg","output":{"self_contained":false,"code_folding":false}},"headingText":"Calculate total scores based on unique items","containsRefs":false,"markdown":"\n\nThis is a simple test equating study. The data used in this study is simulated from real data. We don't use the real data for privacy purposes here.\n\n2020-2021 Fall Term A Level's first quiz has 40 items. 2022-2023 Fall Term A Level's first quiz has 40 items. 35 of the items in each test forms are unique items while 5 of them are common, thus will be called as \"anchor items\" in this study. For the readers interest, the items belong to four main domains (listening, structure, vocabbulary and reading), yet the common items are only in the reading section. This is obviously a violation of assumptions of test equating. Still, this study is conveyed for demonstration purposes. Therefore, let's continue:\n\nTo ensure statistical equation of these two forms, we first introduced the data in R Studio and the first five rows can be seen below:\n\n```{r  echo=TRUE, code_folding=TRUE}\nQ1 <- read.csv(\"kitap1.csv\", header = TRUE)\nhead(Q1)\n\n```\n\nLater, we introduced the unique and anchor items separately. First 35 items are unique items, and the last 5 items are anchor items.\n\n```{r  echo=TRUE, code_folding=TRUE}\n\nQ1$total <- rowSums(Q1[, 1:35])\n\n# Calculate scores based on anchor items\nQ1$anchor <- rowSums(Q1[, 36:40])\n```\n\nAs we will use the ***equate*** package, the data should be contained as frequency tables: Form x (20-21 fall) had a sample of 200 while form y (22-23 fall) had a sample of 133 students. They are defined as:\n\n```{r  echo=TRUE, code_folding=TRUE}\n#first introduce the equate package:\nlibrary(equate)\n# Create frequency tables (total score range: 0-35; anchor score range: 0-5)\nQ1_x <- freqtab(Q1[1:200, c(\"total\", \"anchor\")], scales = list(0:35, 0:5))\nQ1_y <- freqtab(Q1[201:334, c(\"total\", \"anchor\")], scales = list(0:35, 0:5))\n\n```\n\nTo consideration of the reader one more time, we must state that these forms are the first quizzes of the students. They usually get high marks. For instance, you can see below that the students' scores are distributed left-skewed in both forms. Their total correct answers are distributed between 20 and 35 for the unique items , and between 3 and 5 for anchor items in form X. The situation isn't different for form y.\n\n```{r  echo=TRUE, code_folding=TRUE}\n#distrubution of the data among forms and unique/common items\nplot(Q1_x, xlab = \"Total Scores Form X\", ylab = \"Common Anchor Scores Form X\")\nplot(Q1_y, xlab = \"Total Scores Form y\", ylab = \"Common Anchor Scores Form y\")\n```\n\nStill, let's continue... with the smoothing procedure. For both forms, we utilized loglinear presmoothing. Of course there are several other methods, yet literature shows not much a big difference between them, thus not much care given to this issue. again as this is a study with demonstration purposes. After the smoothing, it can be realized that the distribution is highly eye-pleasing right now. Also it is much easier to match the scores even if there isn't an equivalent of it in the other form.\n\n```{r   echo=TRUE, code_folding=TRUE}\n#PRESMOOTHING\nsmooth_x <- presmoothing(Q1_x, smoothmethod = \"loglinear\")\nsmooth_y <- presmoothing(Q1_y, smoothmethod = \"loglinear\")\nplot(smooth_x, xlab = \"Total Scores Form X\", ylab = \"Common Anchor Scores Form X\")\nplot(smooth_y, xlab = \"Total Scores Form y\", ylab = \"Common Anchor Scores Form y\")\n \n```\n\nNow, it can be roughly said that the forms are ready to be equated. Before we try several methods, lets see the results of the Tucker method as it can produce equating error as well:\n\n```{r   echo=TRUE, code_folding=TRUE}\n## Linear Tucker Equating\nQ1_tucker <- equate(Q1_x, Q1_y, type = \"linear\", method = \"tucker\")\nQ1_tucker$concordance\n```\n\nYou will see that the equating errors are above 1 before the score of 25 as there isn't much data in the low scores. Also, as we investigate the lower marks, we see that the gap between equated scores are increasing. For instance, 0 on form X is equal to 6.597617 on form Y. This is because there isn't data in these regions of the scores. Despite that, equated scores get more meaningful after 20. Especially after the total score 30, the equated scores are too close and the equation error is too low, which would be quite better if the situation was like that on all total score ranges. Let's see some other equating methods:\n\n```{r   echo=TRUE, code_folding=TRUE}\n## Comparing Multiple Methods\n# Nominal method with mean equating\nQ1_nom <- equate(Q1_x, Q1_y, type = \"mean\", method = \"nom\")\n\n# Frequency method with equipercentile\nQ1_freq <- equate(Q1_x, Q1_y, type = \"equip\", method = \"freq\")\n\n# Braun method with linear equating\nQ1_braun <- equate(Q1_x, Q1_y, type = \"linear\", method = \"braun\")\n\n# Compare equated scores\nround(cbind(xscale = 0:35, \n            nominal = Q1_nom$concordance$yx,\n            tucker = Q1_tucker$concordance$yx, \n            freq = Q1_freq$concordance$yx, \n            braun = Q1_braun$concordance$yx), 2)\n\n```\n\nAlthough the equating methods vary, the results are similar to those of Tucker method. Especially Frequency Estimation method shows how important it is to have data in different score ranges because there is no meaningful equation before the scale score of 20 and all lower scores are equated to -.5 in this method. Let's also see the plotting of the chart above:\n\n```{r  echo=TRUE, code_folding=TRUE}\n# Plot the results\nplot(Q1_tucker, Q1_nom, Q1_freq, Q1_braun, lty=c(1,2,3,4),\n     col=c(\"blue\", \"black\", \"red\", \"forestgreen\"), addident = FALSE)\n```\n\nAs also can be seen in the plot above, after the scale score of 20, all equating methods are quite similar to each other. Scores lower than 20 are equated with linear methods much better than the equi-percentile method as there isn't adequate data in those score ranges.\n\nThis study is conducted for demonstrative purposes and still we can say that scale scores over 30 can be equated in the given forms.\n","srcMarkdownNoYaml":"\n\nThis is a simple test equating study. The data used in this study is simulated from real data. We don't use the real data for privacy purposes here.\n\n2020-2021 Fall Term A Level's first quiz has 40 items. 2022-2023 Fall Term A Level's first quiz has 40 items. 35 of the items in each test forms are unique items while 5 of them are common, thus will be called as \"anchor items\" in this study. For the readers interest, the items belong to four main domains (listening, structure, vocabbulary and reading), yet the common items are only in the reading section. This is obviously a violation of assumptions of test equating. Still, this study is conveyed for demonstration purposes. Therefore, let's continue:\n\nTo ensure statistical equation of these two forms, we first introduced the data in R Studio and the first five rows can be seen below:\n\n```{r  echo=TRUE, code_folding=TRUE}\nQ1 <- read.csv(\"kitap1.csv\", header = TRUE)\nhead(Q1)\n\n```\n\nLater, we introduced the unique and anchor items separately. First 35 items are unique items, and the last 5 items are anchor items.\n\n```{r  echo=TRUE, code_folding=TRUE}\n\n# Calculate total scores based on unique items\nQ1$total <- rowSums(Q1[, 1:35])\n\n# Calculate scores based on anchor items\nQ1$anchor <- rowSums(Q1[, 36:40])\n```\n\nAs we will use the ***equate*** package, the data should be contained as frequency tables: Form x (20-21 fall) had a sample of 200 while form y (22-23 fall) had a sample of 133 students. They are defined as:\n\n```{r  echo=TRUE, code_folding=TRUE}\n#first introduce the equate package:\nlibrary(equate)\n# Create frequency tables (total score range: 0-35; anchor score range: 0-5)\nQ1_x <- freqtab(Q1[1:200, c(\"total\", \"anchor\")], scales = list(0:35, 0:5))\nQ1_y <- freqtab(Q1[201:334, c(\"total\", \"anchor\")], scales = list(0:35, 0:5))\n\n```\n\nTo consideration of the reader one more time, we must state that these forms are the first quizzes of the students. They usually get high marks. For instance, you can see below that the students' scores are distributed left-skewed in both forms. Their total correct answers are distributed between 20 and 35 for the unique items , and between 3 and 5 for anchor items in form X. The situation isn't different for form y.\n\n```{r  echo=TRUE, code_folding=TRUE}\n#distrubution of the data among forms and unique/common items\nplot(Q1_x, xlab = \"Total Scores Form X\", ylab = \"Common Anchor Scores Form X\")\nplot(Q1_y, xlab = \"Total Scores Form y\", ylab = \"Common Anchor Scores Form y\")\n```\n\nStill, let's continue... with the smoothing procedure. For both forms, we utilized loglinear presmoothing. Of course there are several other methods, yet literature shows not much a big difference between them, thus not much care given to this issue. again as this is a study with demonstration purposes. After the smoothing, it can be realized that the distribution is highly eye-pleasing right now. Also it is much easier to match the scores even if there isn't an equivalent of it in the other form.\n\n```{r   echo=TRUE, code_folding=TRUE}\n#PRESMOOTHING\nsmooth_x <- presmoothing(Q1_x, smoothmethod = \"loglinear\")\nsmooth_y <- presmoothing(Q1_y, smoothmethod = \"loglinear\")\nplot(smooth_x, xlab = \"Total Scores Form X\", ylab = \"Common Anchor Scores Form X\")\nplot(smooth_y, xlab = \"Total Scores Form y\", ylab = \"Common Anchor Scores Form y\")\n \n```\n\nNow, it can be roughly said that the forms are ready to be equated. Before we try several methods, lets see the results of the Tucker method as it can produce equating error as well:\n\n```{r   echo=TRUE, code_folding=TRUE}\n## Linear Tucker Equating\nQ1_tucker <- equate(Q1_x, Q1_y, type = \"linear\", method = \"tucker\")\nQ1_tucker$concordance\n```\n\nYou will see that the equating errors are above 1 before the score of 25 as there isn't much data in the low scores. Also, as we investigate the lower marks, we see that the gap between equated scores are increasing. For instance, 0 on form X is equal to 6.597617 on form Y. This is because there isn't data in these regions of the scores. Despite that, equated scores get more meaningful after 20. Especially after the total score 30, the equated scores are too close and the equation error is too low, which would be quite better if the situation was like that on all total score ranges. Let's see some other equating methods:\n\n```{r   echo=TRUE, code_folding=TRUE}\n## Comparing Multiple Methods\n# Nominal method with mean equating\nQ1_nom <- equate(Q1_x, Q1_y, type = \"mean\", method = \"nom\")\n\n# Frequency method with equipercentile\nQ1_freq <- equate(Q1_x, Q1_y, type = \"equip\", method = \"freq\")\n\n# Braun method with linear equating\nQ1_braun <- equate(Q1_x, Q1_y, type = \"linear\", method = \"braun\")\n\n# Compare equated scores\nround(cbind(xscale = 0:35, \n            nominal = Q1_nom$concordance$yx,\n            tucker = Q1_tucker$concordance$yx, \n            freq = Q1_freq$concordance$yx, \n            braun = Q1_braun$concordance$yx), 2)\n\n```\n\nAlthough the equating methods vary, the results are similar to those of Tucker method. Especially Frequency Estimation method shows how important it is to have data in different score ranges because there is no meaningful equation before the scale score of 20 and all lower scores are equated to -.5 in this method. Let's also see the plotting of the chart above:\n\n```{r  echo=TRUE, code_folding=TRUE}\n# Plot the results\nplot(Q1_tucker, Q1_nom, Q1_freq, Q1_braun, lty=c(1,2,3,4),\n     col=c(\"blue\", \"black\", \"red\", \"forestgreen\"), addident = FALSE)\n```\n\nAs also can be seen in the plot above, after the scale score of 20, all equating methods are quite similar to each other. Scores lower than 20 are equated with linear methods much better than the equi-percentile method as there isn't adequate data in those score ranges.\n\nThis study is conducted for demonstrative purposes and still we can say that scale scores over 30 can be equated in the given forms.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":{"self_contained":false,"code_folding":false},"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":true,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"toc-depth":4,"include-after-body":["../../footer.html"],"output-file":"index.html"},"language":{"toc-title-document":"Table of Contents","toc-title-website":"Table of Contents","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit","repo-action-links-source":"Source","repo-action-links-issue":"Report","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Libraries / Packages","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.555","editor":"source","theme":"literia","citations-hover":true,"footnotes-hover":true,"mainfont":"roboto","background":"rgb(239, 239, 239)","fontcolor":"rgb(17, 43, 60)","fontsize":"1.1em","linestretch":1.5,"code-block-bg":true,"code-block-border-left":"#31BAE9","title-block-banner":true,"page-layout":"article","comments":{"utterances":{"repo":"aliemrekaragul/blog-comments"}},"title":"Test Equating","description":"Equating two test forms:\nThis is a simple test equating study. The data used in this study is simulated from real data. We don't use the real data for privacy purposes here.\n","author":[{"name":"Ali Emre Karag√ºl","orcid":"0000-0002-5820-8643","email":"aliemrekaragul@gmail.com","affiliations":[{"name":"TOBB ETU- University of Economics & Technology"}]}],"date":"09.20.2022","categories":["equate"],"image":"image.jpg"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}