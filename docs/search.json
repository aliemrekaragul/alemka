[
  {
    "objectID": "posts/Web scraping with Rvest/index.html",
    "href": "posts/Web scraping with Rvest/index.html",
    "title": "Web scraping with Rvest package",
    "section": "",
    "text": "In this post, we will delve into harvesting a web page, Ekşi Sözlük. This process won’t include the automation of the process. Yet, a completed web application for harvesting Ekşi Sözlük is available.\nEkşi Sözlük is a reddit-like satirical web site where users share their ideas on certain topics in Turkish. Our target topic is “veri bilimi” (a.k.a. data science in English). The R packages that we use in this post are as follows:\n\nlibrary(rvest)\nlibrary(dplyr)\nlibrary(tm)\nlibrary(stopwords)\nlibrary(wordcloud)\nlibrary(wordcloud2)"
  },
  {
    "objectID": "posts/Web scraping with Rvest/index.html#introduction",
    "href": "posts/Web scraping with Rvest/index.html#introduction",
    "title": "Web scraping with Rvest package",
    "section": "",
    "text": "In this post, we will delve into harvesting a web page, Ekşi Sözlük. This process won’t include the automation of the process. Yet, a completed web application for harvesting Ekşi Sözlük is available.\nEkşi Sözlük is a reddit-like satirical web site where users share their ideas on certain topics in Turkish. Our target topic is “veri bilimi” (a.k.a. data science in English). The R packages that we use in this post are as follows:\n\nlibrary(rvest)\nlibrary(dplyr)\nlibrary(tm)\nlibrary(stopwords)\nlibrary(wordcloud)\nlibrary(wordcloud2)"
  },
  {
    "objectID": "posts/Web scraping with Rvest/index.html#processing",
    "href": "posts/Web scraping with Rvest/index.html#processing",
    "title": "Web scraping with Rvest package",
    "section": "Processing",
    "text": "Processing\n“Rvest” is a package for web scraping and harvesting which was developed by Hadley Wickham. He has stated that he was inspired by a few of the python libraries such as “beautiful soup” and “RoboBrowser”.\nFirst things first; we start by introducing the webpage that we want to harvest.\n\nhtml &lt;- read_html(\"https://eksisozluk1923.com/veri-bilimi--3426406\")\n\nRvest allows us to collect any type of HTML tag from the current page. Let’s suppose we would like to collect all the links in a topic page. First, go to the page and press F12 to open the “devtools” tab. Then turn inspect mode on. Hover on an item and check the node name for a random link. This process is shown below.\n\n\n\n\nThen we use the functions html_nodes() on the node name and html_attr() on “href” HTML tags. This will get all the href attributes with the node name “a.url” on the given link and turn them into a list:\n\nlinks &lt;- html %&gt;% html_nodes(\"a.url\")  %&gt;%  html_attr(\"href\")\nlinks\n\n[1] \"http://e-k.in/introduction-to-data-science/\"                                                     \n[2] \"https://www.edx.org/course/introduction-big-data-apache-spark-uc-berkeleyx-cs100-1x#.VMK0q3v-t3U\"\n\n\nIf we would like to collect all the entries on the given page, we will need the div attribute name for entries (also shown below).\n\nNow, we will use the function html_text() to collect the entries in these div attributes. There are about 10 entries some of which are quite long. Therefore, let me print only the first three entries via the head() function.\n\nentries &lt;- html %&gt;% html_nodes(\".content\") %&gt;%html_text()    \n\n#Let's see the first three entries:\nhead(entries, 3)\n\n[1] \"\\r\\n    internette milyonlarca veri var. bu milyonlarca veriyi işleyen milyonlarca uygulama var. işte bu uygulamalar sayesinde var olan veriyi kullanıp yeni veri uygulamaları yaratma işine veri bilimi denir.\\r\\n  \"\n[2] \"\\r\\n    istatistik ile cok alakali bilim. (bkz: veri madenciligi)\\r\\n  \"                                                                                                                                              \n[3] \"\\r\\n    bana veri olmadan çalışan bir adet bilim türü gösterildiği anda varlığını ve özgünlüğünü kabul edeceğim ilim kategorisi.yoksa şununla yarışır bence:(bkz: deney bilimi) *\"                                    \n\n\nNow that you have all the entries in a page, it is easy to carry out a text analysis with it. Let’s simply create a word cloud. Here we’ll use the wordcloud and tm packages. Functions in tm package works on a corpus type input. Hence, we change our entry list to a corpus using Corpus() and VectorSource() functions.\n\n#turn entries into corpus\n\nentries&lt;-Corpus(VectorSource(entries))\n\nAs we do not want unnecessary items in our word cloud, we will get rid of numbers, punctuation marks and spaces with removeNumbers(), removePunctuation(), and stripWhitespace().\nAlso all letters will be transformed into lower case characters. We use tm_map() function to apply all the above mentioned functions on the corpus items.\n\n#apply several functions such as remove punctuation or numbers etc.\n\nentries &lt;- entries %&gt;%\n  tm_map(removeNumbers) %&gt;%\n  tm_map(removePunctuation) %&gt;%\n  tm_map(stripWhitespace)\nentries &lt;- tm_map(entries, content_transformer(tolower))\n\nOf course we would like to remove all the words that do not contribute to the context such as linkers, conjunctions, articles etc. For that, we can use the package stopwords. Turkish stopwords are defined in it and if you like you can add some other words to remove from your corpus via the append() function. We need to unlist() our corpus to apply removeWords() on it.\n\ntr_stopwords&lt;-stopwords::stopwords(\"tr\", source = \"stopwords-iso\")    \ntr_stopwords&lt;-append(tr_stopwords, c(\"bkz\",\"var\",\"vardir\",\"icin\", \"iste\", \"işte\",\"rn\",\"crn\"))\nentries&lt;-unlist(entries)\nentries &lt;- removeWords(entries, words=tr_stopwords)\n\nActually, right now our corpus is ready, filled with all the meaningful words in the entries. However, to create a word cloud we need it as a frequency table. That’s because a word’s frequency will be used while determining its colour and size. To get a frequency table, we need a term matrix first. Following lines of codes will do the magic in this regard:\n\n#turn into a matrix\nterm_matrix &lt;- as.matrix(TermDocumentMatrix(entries) )\n\n#frequency table:\nword_freqs &lt;- sort(rowSums(term_matrix),decreasing=TRUE) \nword_freqs &lt;- data.frame(word=names(word_freqs),freq=word_freqs )\n\nWe will use two different packages for creating our word clouds. Initially, the classic word cloud takes the column names in the frequency table as words and freq arguments. Other arguments are also defined next to them as notes:\n\nwordcloud::wordcloud(words = word_freqs$word, \n                     freq = word_freqs$freq, \n                     min.freq = 1,  ## freqs lower than that wonW't be in the cloud\n                     max.words=200, ## Total number of words in the cloud\n                     random.order=FALSE,  ## words will be ordered according to their freqs.\n                     rot.per=0.35, ## rotation degree\n                     colors=brewer.pal(8, \"Dark2\")) ## some custimization for colors\n\n\n\n\n\n\n\nThe second option is a more modern alternative. It is developed based on the JS framework with the same name. It gives you the opportunity to hover over the words and see their frequencies. Also, you can decide on a shape among several options such as 'circle', 'diamond', 'triangle-forward', 'triangle', 'pentagon', and 'star'. Yet, some of these features won’t be available if you want your cloud in .png format. That is because wordcloud2 package provides us with clouds in HTML format which makes it easier for web page embedding but harder for saving on your local disk as .png or any other format.\n\nwordcloud2::wordcloud2 (word_freqs, \n                  size = 0.4,  ## this is basically zooming in the cloud. default is 1\n                  shape = \"star\", \n                  color='random-light', ## another option is 'random-dark'\n                  backgroundColor=\"black\" ) ## another option is 'white'"
  },
  {
    "objectID": "posts/Web scraping with Rvest/index.html#conclusion",
    "href": "posts/Web scraping with Rvest/index.html#conclusion",
    "title": "Web scraping with Rvest package",
    "section": "Conclusion",
    "text": "Conclusion\nThere are many ways to use Rvest and scrap a website. We tried one of them in this post. In the end you can do many things with your output such as content analysis, clustering, sentimental analysis or even survival analysis. Here, in this post, we do not bare any pragmatic purposes so focused on creating something fun and eye-catching: word clouds. Also, scraping depends highly on your computer’s processor and the number of pages you want to scrap. If you want to automate this process, it may take some time to complete. You can find my Rshiny web application automating specifically the steps discussed here.\nAlthough we haven’t mentioned here, Rvest works in alignment with the polite package. It allows you to scrap a web page in a polite way with permissions and greetings. The motto of the package polite sums it all for itself: Be responsible when scraping data from websites by following polite principles: introduce yourself, ask for permission, take slowly and never ask twice."
  },
  {
    "objectID": "posts/Visualisation of My Personal Google Data (1)/index.html",
    "href": "posts/Visualisation of My Personal Google Data (1)/index.html",
    "title": "Visualisation of My Personal Google Data (1): My Locations",
    "section": "",
    "text": "If you give permission to Google to store your location data, they will keep them in their databases forever. You can also allow them to store it for a while and then ask them delete it. They will directly do so.\nWhat makes this study a fun project is its being very personal. I decided to analyze my personal data in August, 2022. Therefore, I granted many new permissions to Google along with many previously granted permissions. They keep them in various formats including .csv, .json, .mbox etc. When you query for your personal data, they provide it within a couple of days depending on the size of the data you queried.\nUsually, I provide the readers with the data in my posts. However, in this series, the data are very personal and so I will not."
  },
  {
    "objectID": "posts/Visualisation of My Personal Google Data (1)/index.html#introduction-to-the-series-visualisation-of-my-personal-google-data",
    "href": "posts/Visualisation of My Personal Google Data (1)/index.html#introduction-to-the-series-visualisation-of-my-personal-google-data",
    "title": "Visualisation of My Personal Google Data (1): My Locations",
    "section": "",
    "text": "If you give permission to Google to store your location data, they will keep them in their databases forever. You can also allow them to store it for a while and then ask them delete it. They will directly do so.\nWhat makes this study a fun project is its being very personal. I decided to analyze my personal data in August, 2022. Therefore, I granted many new permissions to Google along with many previously granted permissions. They keep them in various formats including .csv, .json, .mbox etc. When you query for your personal data, they provide it within a couple of days depending on the size of the data you queried.\nUsually, I provide the readers with the data in my posts. However, in this series, the data are very personal and so I will not."
  },
  {
    "objectID": "posts/Visualisation of My Personal Google Data (1)/index.html#introduction-my-locations",
    "href": "posts/Visualisation of My Personal Google Data (1)/index.html#introduction-my-locations",
    "title": "Visualisation of My Personal Google Data (1): My Locations",
    "section": "Introduction: “My Locations”",
    "text": "Introduction: “My Locations”\nIn this part of the series, we will investigate my personal location data. We will visualize the spots I visited within a period of time. This way, I personally will gain insights about how boring my days are :)\nThe R packages that we use in this post are as follows: rjson, tidyr, dplyr, purrr, lubridate, sp and leaflet.\n\nCode##packs for data processing\nlibrary(rjson)      # to read .JSON files.\nlibrary(tidyr)      # to process data\nlibrary(dplyr)      # to process data\nlibrary(purrr)      # to process data\nlibrary(lubridate)  # to deal with date variables\n#packs for data viz\nlibrary(sp)         # a pack for spatial objects\nlibrary(leaflet)    # map and its functions"
  },
  {
    "objectID": "posts/Visualisation of My Personal Google Data (1)/index.html#understand-the-data",
    "href": "posts/Visualisation of My Personal Google Data (1)/index.html#understand-the-data",
    "title": "Visualisation of My Personal Google Data (1): My Locations",
    "section": "Understand the Data",
    "text": "Understand the Data\nInside the takeout folder that I received from Google, there is a folder named “Location History”. Inside it, “Semantic Location History” contains the location data based on the months and years. From that folder, I have called the locations I visited in November. Thus, we will use 2022_NOVEMBER.json file. Let’s investigate the data. Start with reading the file into R environment.\n\nmy_locations &lt;- fromJSON(file = \"2022_NOVEMBER.json\")\n\nThen, let’s try to understand the structure of the data, how and what kind of information is stored into its cells. The list object my_locations contains many lists inside it. Let’s try to understand each one of them one by one:\n\nsummary(my_locations[[1]])\n\n      Length Class  Mode\n [1,] 1      -none- list\n [2,] 1      -none- list\n [3,] 1      -none- list\n [4,] 1      -none- list\n [5,] 1      -none- list\n [6,] 1      -none- list\n [7,] 1      -none- list\n [8,] 1      -none- list\n [9,] 1      -none- list\n[10,] 1      -none- list\n[11,] 1      -none- list\n[12,] 1      -none- list\n[13,] 1      -none- list\n[14,] 1      -none- list\n[15,] 1      -none- list\n[16,] 1      -none- list\n[17,] 1      -none- list\n\n\nThere are many smaller lists in the first indexed list. Let’s try the first one and see what’s inside:\n\nsummary(my_locations[[1]][[1]])\n\n           Length Class  Mode\nplaceVisit 11     -none- list\n\n\nThere is a single list inside. Sad :( Let’s dive one more step:\n\nsummary(my_locations[[1]][[1]][[1]])\n\n                        Length Class  Mode     \nlocation                8      -none- list     \nduration                2      -none- list     \nplaceConfidence         1      -none- character\ncenterLatE7             1      -none- numeric  \ncenterLngE7             1      -none- numeric  \nvisitConfidence         1      -none- numeric  \notherCandidateLocations 4      -none- list     \neditConfirmationStatus  1      -none- character\nlocationConfidence      1      -none- numeric  \nplaceVisitType          1      -none- character\nplaceVisitImportance    1      -none- character\n\n\nFinally, here we have several items. There is a list called location containing 8 items inside. There is duration with 2 items and otherCandidateLocations with 4 items. Other lists contain only one item each. Let’s check these one by one:\n\nsummary(my_locations[[1]][[1]][[1]]$location)\n\n                      Length Class  Mode     \nlatitudeE7            1      -none- numeric  \nlongitudeE7           1      -none- numeric  \nplaceId               1      -none- character\naddress               1      -none- character\nsemanticType          1      -none- character\nsourceInfo            1      -none- list     \nlocationConfidence    1      -none- numeric  \ncalibratedProbability 1      -none- numeric  \n\n\n\nsummary(my_locations[[1]][[1]][[1]]$duration)\n\n               Length Class  Mode     \nstartTimestamp 1      -none- character\nendTimestamp   1      -none- character\n\n\n\nsummary(my_locations[[1]][[1]][[1]]$otherCandidateLocations)\n\n     Length Class  Mode\n[1,] 7      -none- list\n[2,] 7      -none- list\n[3,] 7      -none- list\n[4,] 7      -none- list\n\n\nWe can obtain much information through this investigation process. For instance, inside the location I can see information about the latitude, longitude, address, the confidence that I have to this place, and some other. Here, if you are following along with me, please spare some time to understand your data. Delve into them and digest as much information as you can. I will see you in the next section: data processing."
  },
  {
    "objectID": "posts/Visualisation of My Personal Google Data (1)/index.html#pre-processing",
    "href": "posts/Visualisation of My Personal Google Data (1)/index.html#pre-processing",
    "title": "Visualisation of My Personal Google Data (1): My Locations",
    "section": "Pre-processing",
    "text": "Pre-processing\nYou can use as many items as you want in your work. You should decide the meaningful information while understanding your data. Now let’s re-define our lists as a dataframe.\n\ndf &lt;- map_dfr(my_locations[[\"timelineObjects\"]], as.data.frame)\nView(df)\n# there is one empty row after each entry. Let's drop them through one of the complete columns:\ndf &lt;- drop_na(df, placeVisit.location.latitudeE7)\n\nThere are many columns, some of which I won’t need. Especially, I am not interested in the locations defined as “candidate”. I will exclude them from my study. They are probably the locations that might be the place that I visited ordered by possibility. I just need the one with the highest possibility, which is tagged with placeVisit.location. . These locations are also defined as “HIGH CONFIDENCE”. Let’s continue the analysis with these locations, only.\nAlso, there are some columns with no entry. Let me exclude them with a function. Let the function be called not_all_na. This is a function that drops all the columns which are completely empty:\n\nnot_all_na &lt;- function(x)\n  any(!is.na(x))\n#use the function on the dataframe:\ndf &lt;- df %&gt;% select(where(not_all_na))\n\nNow, I have a dataframe with 150+ columns. However, I just need the information about latitude, altitude, date and address of the locations that I visited. Let’s write a query to get this data into a new dataframe:\n\nlat &lt;- select(df, contains(\"placeVisit.location.latitudeE7\"))\nlon &lt;- select(df, contains(\"placeVisit.location.longitudeE7\"))\naddress &lt;- select(df, contains(\"placeVisit.location.address\"))\ndate &lt;- select(df, contains(\"placeVisit.duration.startTimestamp\"))\n\nThe chunks of code above ask for columns whose names contain the extensions written in quotation marks in them. Still, this raw information isn’t enough for several reasons. Firstly, lat and lot are coordinates in E7 format. With a quick research on the internet, I learned that they simply need to be divided by 10000000. Also, date contains day, month, year, hour, minute, second and time zone (which is in GMT+0 format) information all in the same column. They need to be handled. Let’s start with the second issue (the one about date):\n\n#re-name the only column:\nnames(date) &lt;- \"Date\"\nhead(date)\n\n                      Date\n1 2022-11-06T13:12:38.091Z\n2 2022-11-06T13:24:32.338Z\n3 2022-11-06T13:59:00.539Z\n4 2022-11-06T14:17:15.462Z\n5 2022-11-06T14:39:29.138Z\n6 2022-11-07T05:32:14.277Z\n\n\nAs can be seen above, there are two separators: One is “T” separating day and time info. The other is “.” separating time and time zone info. Follow the notes in the code to grasp the process:\n\n#divide the day and hour info from the time zone info, then drop the time zone:\ndate &lt;-\n  separate(\n    data = date,\n    col = Date,\n    into = c(\"Date\", \"zone\"),\n    sep = \"\\\\.\"\n  )\ndate &lt;- date[-c(2)]\n\n#Now, transform the time in local time zone which is GMT+3:\ndate$Date&lt;-as.POSIXct(date$Date, format=\"%Y-%m-%dT%H:%M:%S\", tz=Sys.timezone())+ hours(3)\n\n#divide the day and hour info:\ndate &lt;-\n  separate(\n    data = date,\n    col = Date,\n    into = c(\"Day\", \"Hour\"),\n    sep = \" \"\n  )\n#see the new format:\nhead(date)\n\n         Day     Hour\n1 2022-11-06 16:12:38\n2 2022-11-06 16:24:32\n3 2022-11-06 16:59:00\n4 2022-11-06 17:17:15\n5 2022-11-06 17:39:29\n6 2022-11-07 08:32:14\n\n\nNicely done! Now gather all the information that we need into a dataframe. Again follow along the notes in the code:\n\ncoords &lt;-\n  drop_na(data.frame(\n    lat = unlist(lat, use.names = FALSE) / 10000000, #divide lat and lon by 10000000 to get rid of the E7 format\n    lon = unlist(lon, use.names = FALSE) / 10000000, \n    address = unlist(address, use.names = FALSE),\n    date # we processed this before\n  ))\n\nSo far, we have worked to prepare for the data visualization process. Our data is ready with the name coords. Let’s continue with the visualization."
  },
  {
    "objectID": "posts/Visualisation of My Personal Google Data (1)/index.html#data-visualization",
    "href": "posts/Visualisation of My Personal Google Data (1)/index.html#data-visualization",
    "title": "Visualisation of My Personal Google Data (1): My Locations",
    "section": "Data Visualization",
    "text": "Data Visualization\nAt this point, we will visualize the locations I visited in November of 2022 on a world map. You can’t be as disappointed as me when you see that I live a life between home and work. Yet, the point here is to see the process of visualization. We owe this beautiful project to the R package leaflet. It is actually a javascript library, all its arguments are deployed into R environment too. Therefore, we can work with it. If you are still with me, I mhighly recommend you to read the documentation of the package leaflet. Then, follow along the notes in the code and try to understand it if you are not familiar with it.\n\ncoordinates(coords) &lt;- ~ lon + lat\nleaflet(coords,\n\n# formating the outer of the map:\n        width = \"800px\",\n        height = \"400px\", \n        padding = 10) %&gt;% \n  addTiles() %&gt;%\n\n#formating the markers on the map:\n  addCircleMarkers(\n    color = \"tomato\", #my favorite colour\n    fillOpacity = 1,\n    radius = 7,\n    stroke = FALSE,\n    \n#address pops up when you click on a marker:\n    popup = coords$address,\n\n#the date and hour shows up with a fancy personal note when you hover on a marker:\n    label =  paste0(\"I have been around here on \", coords$Day, \" at around \", coords$Hour),\n\n#formating the label that shows up when you hover:\n    labelOptions = labelOptions(\n      noHide = F,\n      direction = \"top\",\n      style = list(\n        \"color\" = \"black\",\n        \"font-family\" = \"calibri\", #I love calibri\n        \"box-shadow\" = \"3px 3px rgba(0,0,0,0.25)\",\n        \"font-size\" = \"12px\",\n        \"border-color\" = \"rgba(0,0,0,0.5)\"\n      )\n    )\n  )"
  },
  {
    "objectID": "posts/Visualisation of My Personal Google Data (1)/index.html#conclusion",
    "href": "posts/Visualisation of My Personal Google Data (1)/index.html#conclusion",
    "title": "Visualisation of My Personal Google Data (1): My Locations",
    "section": "Conclusion",
    "text": "Conclusion\nWorking with your personal data gives you the opportunity to understand your own habits, likes, dislikes, and maybe future expectations. Here, you can only see my locations in November. When I worked on longer periods, I realized that I need to travel and see new places more often. Even if they are in my own city, a new place is a new vision of life.\nVisualizing data on spatial environments is a new challenge for me. Rather than graphs and charts, working with maps are more attractive obviously. While visualizing location data on maps, leaflet is an amazing, open source library. There are other options. One needs a mention here: ggmap. Yet, to use this package you need an API key obtained from Google. For more information about API keys, visit here. As of the package, you can visit the CRAN page of ggmap. Under the title “Google Maps API key”, you will see the procedure to buy a personal API key. It reads as follows:\nGOOGLE MAPS API KEY [@ggmap]\nA few years ago Google has changed its API requirements, and ggmap users are now required to register with Google. From a user’s perspective, there are essentially three ramifications of this:\n\nUsers must register with Google. You can do this at https://mapsplatform.google.com. While it will require a valid credit card (sorry!), there seems to be a fair bit of free use before you incur charges, and even then the charges are modest for light use.\nUsers must enable the APIs they intend to use. What may appear to ggmap users as one overarching “Google Maps” product, Google in fact has several services that it provides as geo-related solutions. For example, the Maps Static API provides map images, while the Geocoding API provides geocoding and reverse geocoding services. Apart from the relevant Terms of Service, generally ggmap users don’t need to think about the different services. For example, you just need to remember that get_googlemap() gets maps, geocode() geocodes (with Google, DSK is done), etc., and ggmap handles the queries for you. However, you do need to enable the APIs before you use them. You’ll only need to do that once, and then they’ll be ready for you to use. Enabling the APIs just means clicking a few radio buttons on the Google Maps Platform web interface listed above, so it’s easy.\nInside R, after loading the new version of ggmap, you’ll need provide ggmap with your API key, a hash value (think string of jibberish) that authenticates you to Google’s servers. This can be done on a temporary basis with register_google(key = \"[your key]\") or permanently using register_google(key = \"[your key]\", write = TRUE) (note: this will overwrite your ~/.Renviron file by replacing/adding the relevant line). If you use the former, know that you’ll need to re-do it every time you reset R.\n\nYour API key is private and unique to you, so be careful not to share it online, for example in a GitHub issue or saving it in a shared R script file. If you share it inadvertantly, just get on Google’s website and regenerate your key - this will retire the old one. Keeping your key private is made a bit easier by ggmap scrubbing the key out of queries by default, so when URLs are shown in your console, they’ll look something like key=xxx. (Read the details section of the register_google() documentation for a bit more info on this point.)\nStay tuned!\nThis series continues with the visualization of my Google Fit data. We will delve into my exercise habbits."
  },
  {
    "objectID": "posts/Test Equating/index.html",
    "href": "posts/Test Equating/index.html",
    "title": "Test Equating",
    "section": "",
    "text": "This is a simple test equating study. The data used in this study is simulated from real data. We don’t use the real data for privacy purposes here.\n2020-2021 Fall Term A Level’s first quiz has 40 items. 2022-2023 Fall Term A Level’s first quiz has 40 items. 35 of the items in each test forms are unique items while 5 of them are common, thus will be called as “anchor items” in this study. For the readers interest, the items belong to four main domains (listening, structure, vocabbulary and reading), yet the common items are only in the reading section. This is obviously a violation of assumptions of test equating. Still, this study is conveyed for demonstration purposes. Therefore, let’s continue:\nTo ensure statistical equation of these two forms, we first introduced the data in R Studio and the first five rows can be seen below:\n\nCodeQ1 &lt;- read.csv(\"kitap1.csv\", header = TRUE)\nhead(Q1)\n\n  L1 L2 L3 L4 L5 L6 L7 L8 L9 L10 S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 V1 V2 V3 V4 V5\n1  0  1  0  1  1  1  0  1  1   0  1  1  0  1  0  0  0  0  0   1  0  1  1  0  1\n2  1  1  1  1  1  1  1  1  1   1  0  1  1  0  1  1  0  0  0   0  1  1  1  1  0\n3  1  1  1  1  1  1  0  1  1   0  1  1  0  1  1  0  0  1  1   0  0  1  0  1  0\n4  1  1  1  1  1  1  1  0  1   0  1  1  0  1  1  0  1  1  0   0  0  1  1  0  1\n5  1  1  1  1  1  1  1  0  1   1  1  1  1  1  0  1  0  1  0   0  1  1  1  0  0\n6  1  1  1  1  0  0  1  1  1   1  0  0  0  0  0  1  1  1  1   1  1  1  1  0  0\n  V6 V7 V8 V9 V10 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 form\n1  0  1  1  1   1  1  1  0  1  1  1  1  1  1   1    x\n2  0  1  1  0   0  1  0  1  1  0  1  1  0  1   1    x\n3  1  0  1  1   1  1  0  0  1  1  1  0  1  1   1    x\n4  1  1  1  0   1  1  1  1  1  0  1  0  0  1   1    x\n5  1  1  1  0   1  1  1  1  0  1  1  1  0  1   1    x\n6  1  1  1  1   1  1  1  1  1  1  1  1  1  1   1    x\n\n\nLater, we introduced the unique and anchor items separately. First 35 items are unique items, and the last 5 items are anchor items.\n\nCode# Calculate total scores based on unique items\nQ1$total &lt;- rowSums(Q1[, 1:35])\n\n# Calculate scores based on anchor items\nQ1$anchor &lt;- rowSums(Q1[, 36:40])\n\n\nAs we will use the equate package, the data should be contained as frequency tables: Form x (20-21 fall) had a sample of 200 while form y (22-23 fall) had a sample of 133 students. They are defined as:\n\nCode#first introduce the equate package:\nlibrary(equate)\n# Create frequency tables (total score range: 0-35; anchor score range: 0-5)\nQ1_x &lt;- freqtab(Q1[1:200, c(\"total\", \"anchor\")], scales = list(0:35, 0:5))\nQ1_y &lt;- freqtab(Q1[201:334, c(\"total\", \"anchor\")], scales = list(0:35, 0:5))\n\n\nTo consideration of the reader one more time, we must state that these forms are the first quizzes of the students. They usually get high marks. For instance, you can see below that the students’ scores are distributed left-skewed in both forms. Their total correct answers are distributed between 20 and 35 for the unique items , and between 3 and 5 for anchor items in form X. The situation isn’t different for form y.\n\nCode#distrubution of the data among forms and unique/common items\nplot(Q1_x, xlab = \"Total Scores Form X\", ylab = \"Common Anchor Scores Form X\")\n\n\n\n\n\n\nCodeplot(Q1_y, xlab = \"Total Scores Form y\", ylab = \"Common Anchor Scores Form y\")\n\n\n\n\n\n\n\nStill, let’s continue… with the smoothing procedure. For both forms, we utilized loglinear presmoothing. Of course there are several other methods, yet literature shows not much a big difference between them, thus not much care given to this issue. again as this is a study with demonstration purposes. After the smoothing, it can be realized that the distribution is highly eye-pleasing right now. Also it is much easier to match the scores even if there isn’t an equivalent of it in the other form.\n\nCode#PRESMOOTHING\nsmooth_x &lt;- presmoothing(Q1_x, smoothmethod = \"loglinear\")\n\nWarning: glm.fit: fitted rates numerically 0 occurred\n\nCodesmooth_y &lt;- presmoothing(Q1_y, smoothmethod = \"loglinear\")\n\nWarning: glm.fit: algorithm did not converge\n\nWarning: glm.fit: fitted rates numerically 0 occurred\n\nCodeplot(smooth_x, xlab = \"Total Scores Form X\", ylab = \"Common Anchor Scores Form X\")\n\n\n\n\n\n\nCodeplot(smooth_y, xlab = \"Total Scores Form y\", ylab = \"Common Anchor Scores Form y\")\n\n\n\n\n\n\n\nNow, it can be roughly said that the forms are ready to be equated. Before we try several methods, lets see the results of the Tucker method as it can produce equating error as well:\n\nCode## Linear Tucker Equating\nQ1_tucker &lt;- equate(Q1_x, Q1_y, type = \"linear\", method = \"tucker\")\nQ1_tucker$concordance\n\n   scale        yx      se.n      se.g\n1      0  6.597617 2.0378528 3.3547355\n2      1  7.404998 1.9751403 3.2551073\n3      2  8.212379 1.9124541 3.1554875\n4      3  9.019759 1.8497970 3.0558770\n5      4  9.827140 1.7871721 2.9562768\n6      5 10.634520 1.7245827 2.8566879\n7      6 11.441901 1.6620331 2.7571116\n8      7 12.249282 1.5995277 2.6575492\n9      8 13.056662 1.5370720 2.5580024\n10     9 13.864043 1.4746724 2.4584730\n11    10 14.671423 1.4123363 2.3589634\n12    11 15.478804 1.3500723 2.2594761\n13    12 16.286185 1.2878911 2.1600141\n14    13 17.093565 1.2258052 2.0605812\n15    14 17.900946 1.1638299 1.9611818\n16    15 18.708326 1.1019838 1.8618212\n17    16 19.515707 1.0402899 1.7625060\n18    17 20.323088 0.9787772 1.6632444\n19    18 21.130468 0.9174818 1.5640464\n20    19 21.937849 0.8564507 1.4649252\n21    20 22.745229 0.7957445 1.3658973\n22    21 23.552610 0.7354438 1.2669847\n23    22 24.359991 0.6756570 1.1682166\n24    23 25.167371 0.6165338 1.0696330\n25    24 25.974752 0.5582849 0.9712903\n26    25 26.782132 0.5012154 0.8732696\n27    26 27.589513 0.4457784 0.7756933\n28    27 28.396894 0.3926659 0.6787528\n29    28 29.204274 0.3429596 0.5827655\n30    29 30.011655 0.2983669 0.4882941\n31    30 30.819035 0.2615166 0.3964236\n32    31 31.626416 0.2360629 0.3094791\n33    32 32.433797 0.2258919 0.2330408\n34    33 33.241177 0.2330134 0.1809526\n35    34 34.048558 0.2559883 0.1763087\n36    35 34.855938 0.2910866 0.2221053\n\n\nYou will see that the equating errors are above 1 before the score of 25 as there isn’t much data in the low scores. Also, as we investigate the lower marks, we see that the gap between equated scores are increasing. For instance, 0 on form X is equal to 6.597617 on form Y. This is because there isn’t data in these regions of the scores. Despite that, equated scores get more meaningful after 20. Especially after the total score 30, the equated scores are too close and the equation error is too low, which would be quite better if the situation was like that on all total score ranges. Let’s see some other equating methods:\n\nCode## Comparing Multiple Methods\n# Nominal method with mean equating\nQ1_nom &lt;- equate(Q1_x, Q1_y, type = \"mean\", method = \"nom\")\n\n# Frequency method with equipercentile\nQ1_freq &lt;- equate(Q1_x, Q1_y, type = \"equip\", method = \"freq\")\n\n# Braun method with linear equating\nQ1_braun &lt;- equate(Q1_x, Q1_y, type = \"linear\", method = \"braun\")\n\n# Compare equated scores\nround(cbind(xscale = 0:35, \n            nominal = Q1_nom$concordance$yx,\n            tucker = Q1_tucker$concordance$yx, \n            freq = Q1_freq$concordance$yx, \n            braun = Q1_braun$concordance$yx), 2)\n\n      xscale nominal tucker  freq braun\n [1,]      0   -0.01   6.60 -0.50  5.65\n [2,]      1    0.99   7.40 -0.50  6.49\n [3,]      2    1.99   8.21 -0.50  7.32\n [4,]      3    2.99   9.02 -0.50  8.16\n [5,]      4    3.99   9.83 -0.50  9.00\n [6,]      5    4.99  10.63 -0.50  9.83\n [7,]      6    5.99  11.44 -0.50 10.67\n [8,]      7    6.99  12.25 -0.50 11.50\n [9,]      8    7.99  13.06 -0.50 12.34\n[10,]      9    8.99  13.86 -0.50 13.17\n[11,]     10    9.99  14.67 -0.50 14.01\n[12,]     11   10.99  15.48 -0.50 14.84\n[13,]     12   11.99  16.29 -0.50 15.68\n[14,]     13   12.99  17.09 -0.50 16.51\n[15,]     14   13.99  17.90 -0.50 17.35\n[16,]     15   14.99  18.71 -0.50 18.19\n[17,]     16   15.99  19.52 -0.50 19.02\n[18,]     17   16.99  20.32 -0.50 19.86\n[19,]     18   17.99  21.13 -0.50 20.69\n[20,]     19   18.99  21.94 -0.50 21.53\n[21,]     20   19.99  22.75 -0.50 22.36\n[22,]     21   20.99  23.55 23.66 23.20\n[23,]     22   21.99  24.36 23.82 24.03\n[24,]     23   22.99  25.17 24.10 24.87\n[25,]     24   23.99  25.97 24.38 25.71\n[26,]     25   24.99  26.78 24.57 26.54\n[27,]     26   25.99  27.59 25.35 27.38\n[28,]     27   26.99  28.40 27.28 28.21\n[29,]     28   27.99  29.20 29.14 29.05\n[30,]     29   28.99  30.01 30.65 29.88\n[31,]     30   29.99  30.82 31.34 30.72\n[32,]     31   30.99  31.63 31.76 31.55\n[33,]     32   31.99  32.43 32.35 32.39\n[34,]     33   32.99  33.24 33.09 33.22\n[35,]     34   33.99  34.05 33.88 34.06\n[36,]     35   34.99  34.86 34.86 34.90\n\n\nAlthough the equating methods vary, the results are similar to those of Tucker method. Especially Frequency Estimation method shows how important it is to have data in different score ranges because there is no meaningful equation before the scale score of 20 and all lower scores are equated to -.5 in this method. Let’s also see the plotting of the chart above:\n\nCode# Plot the results\nplot(Q1_tucker, Q1_nom, Q1_freq, Q1_braun, lty=c(1,2,3,4),\n     col=c(\"blue\", \"black\", \"red\", \"forestgreen\"), addident = FALSE)\n\n\n\n\n\n\n\nAs also can be seen in the plot above, after the scale score of 20, all equating methods are quite similar to each other. Scores lower than 20 are equated with linear methods much better than the equi-percentile method as there isn’t adequate data in those score ranges.\nThis study is conducted for demonstrative purposes and still we can say that scale scores over 30 can be equated in the given forms."
  },
  {
    "objectID": "posts/Multi Facet Rasch Modelling/index.html",
    "href": "posts/Multi Facet Rasch Modelling/index.html",
    "title": "Multi-Facet Rasch Models with R",
    "section": "",
    "text": "While a basic Rasch model focuses on item difficulty and person ability, the Multi-Facet Rasch Model (MFRM) allows us to incorporate additional factors, or facets, such as:\n\nPerson Ability (e.g., the skill level of test-takers),\nItem Difficulty (e.g., how hard the test items are),\nRater Severity (e.g., how lenient or strict raters are),\nTask or Stimulus Differences (e.g., variation in tasks given).\n\nMFRM is said to be IRT version of generalizability theory and it is particularly useful when assessments involve subjective judgments, like in essay grading or performance evaluation, where raters’ subjectivity can introduce bias.\n# Load the libraries\nlibrary(readr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(dplyr)\n\nlibrary(tidyverse)\nlibrary(TAM)\nlibrary(cowplot)"
  },
  {
    "objectID": "posts/Multi Facet Rasch Modelling/index.html#introduction",
    "href": "posts/Multi Facet Rasch Modelling/index.html#introduction",
    "title": "Multi-Facet Rasch Models with R",
    "section": "",
    "text": "While a basic Rasch model focuses on item difficulty and person ability, the Multi-Facet Rasch Model (MFRM) allows us to incorporate additional factors, or facets, such as:\n\nPerson Ability (e.g., the skill level of test-takers),\nItem Difficulty (e.g., how hard the test items are),\nRater Severity (e.g., how lenient or strict raters are),\nTask or Stimulus Differences (e.g., variation in tasks given).\n\nMFRM is said to be IRT version of generalizability theory and it is particularly useful when assessments involve subjective judgments, like in essay grading or performance evaluation, where raters’ subjectivity can introduce bias.\n# Load the libraries\nlibrary(readr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(dplyr)\n\nlibrary(tidyverse)\nlibrary(TAM)\nlibrary(cowplot)"
  },
  {
    "objectID": "posts/Multi Facet Rasch Modelling/index.html#understand-the-data",
    "href": "posts/Multi Facet Rasch Modelling/index.html#understand-the-data",
    "title": "Multi-Facet Rasch Models with R",
    "section": "1. Understand the data",
    "text": "1. Understand the data\nFor MFRM analysis, we are going to use a dataset of essay scores scored on an analytical rubric. There are four domains of the rubric: Content, Prompt Adherence, Language, and Narrativity. Let’s load the data and see the head of them. You can download the data for your own use from here.\n\ndata &lt;- read_csv(\"MFRM_data.csv\", show_col_types = FALSE)\nhead(data)\n\n# A tibble: 6 × 6\n    Con ProAd  Lang   Nar rater essayId\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1     4     4     3     3     1       1\n2     2     2     4     3     1       2\n3     4     4     4     4     1       3\n4     3     2     4     4     1       4\n5     4     4     4     4     1       5\n6     1     1     1     2     1       6\n\n\nLet’s see the structure and summary of the data too.\n\nstr(data)\n\nspc_tbl_ [5,400 × 6] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ Con    : num [1:5400] 4 2 4 3 4 1 1 2 4 3 ...\n $ ProAd  : num [1:5400] 4 2 4 2 4 1 1 2 4 3 ...\n $ Lang   : num [1:5400] 3 4 4 4 4 1 1 4 4 4 ...\n $ Nar    : num [1:5400] 3 3 4 4 4 2 1 4 4 2 ...\n $ rater  : num [1:5400] 1 1 1 1 1 1 1 1 1 1 ...\n $ essayId: num [1:5400] 1 2 3 4 5 6 7 8 9 10 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   Con = col_double(),\n  ..   ProAd = col_double(),\n  ..   Lang = col_double(),\n  ..   Nar = col_double(),\n  ..   rater = col_double(),\n  ..   essayId = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\nsummary(data)\n\n      Con            ProAd            Lang            Nar            rater  \n Min.   :0.000   Min.   :0.000   Min.   :0.000   Min.   :0.000   Min.   :1  \n 1st Qu.:1.000   1st Qu.:1.000   1st Qu.:1.000   1st Qu.:1.000   1st Qu.:1  \n Median :2.000   Median :2.000   Median :2.000   Median :2.000   Median :2  \n Mean   :1.878   Mean   :1.784   Mean   :2.062   Mean   :1.958   Mean   :2  \n 3rd Qu.:3.000   3rd Qu.:3.000   3rd Qu.:3.000   3rd Qu.:3.000   3rd Qu.:3  \n Max.   :4.000   Max.   :4.000   Max.   :4.000   Max.   :4.000   Max.   :3  \n    essayId      \n Min.   :   1.0  \n 1st Qu.: 450.8  \n Median : 900.5  \n Mean   : 900.5  \n 3rd Qu.:1350.2  \n Max.   :1800.0  \n\n\nThere are 1800 rows of data. Each domain is scored between 0 and 4. Perfect as zero must exist in the ordinal data for MFRM. The data set consists of scores from three raters on four domains, so we need to account for the three key facets: person ability (the essays), item difficulty (the domains), and rater severity (the three raters)."
  },
  {
    "objectID": "posts/Multi Facet Rasch Modelling/index.html#fit-the-model",
    "href": "posts/Multi Facet Rasch Modelling/index.html#fit-the-model",
    "title": "Multi-Facet Rasch Models with R",
    "section": "2. Fit the model",
    "text": "2. Fit the model\nNow that we’ve explored our data set, it’s time to fit the Multi-Facet Rasch Model (MFRM). To do this, we’ll use the TAM package in R, which provides functions for fitting various Rasch models, including MFRM. The formulaA provided into the mfr function decides on the model. For PCM, we define interaction between item and step along with the rater facet:\n\nfacets &lt;- data[, \"rater\", drop=FALSE]  # define facet (rater)\npid &lt;- data$essayId  # define person identifier\nresp &lt;- data[, -c(5:6)]  # item response data\nformulaA &lt;- ~item*step + rater   # formula for PCM\n\n\nmod &lt;- TAM::tam.mml.mfr(resp=resp, facets=facets, formulaA=formulaA, pid=data$essayId)"
  },
  {
    "objectID": "posts/Multi Facet Rasch Modelling/index.html#model-diagnostics",
    "href": "posts/Multi Facet Rasch Modelling/index.html#model-diagnostics",
    "title": "Multi-Facet Rasch Models with R",
    "section": "3. Model diagnostics",
    "text": "3. Model diagnostics\nNow that we’ve fitted our Multi-Facet Rasch Model (MFRM) with PCM, let’s take a closer look at the results and explore some diagnostics.\n\nfit_stats &lt;- TAM::tam.fit(mod)\n\nItem fit calculation based on 15 simulations\n|**********|\n|----------|\n\nprint(fit_stats)\n\n$itemfit\n     parameter    Outfit   Outfit_t      Outfit_p  Outfit_pholm     Infit\n1          Con 2.0585470  39.400899  0.000000e+00  0.000000e+00 2.1997279\n2        ProAd 1.5369865  22.163909 7.660384e-109 6.894345e-108 1.6258684\n3         Lang 2.2662066  47.841294  0.000000e+00  0.000000e+00 2.3468649\n4          Nar 1.8419808  33.231069 3.832185e-242 4.215404e-241 1.9461538\n5        step1 0.9567419  -3.116628  1.829322e-03  3.658643e-03 0.9437283\n6        step2 0.4912645 -53.179892  0.000000e+00  0.000000e+00 0.4085188\n7        step3 0.2717863 -95.710904  0.000000e+00  0.000000e+00 0.1844266\n8        step4 0.9264361  -5.793804  6.880989e-09  2.752395e-08 0.6006956\n9       rater1 0.2621579 -84.439666  0.000000e+00  0.000000e+00 0.2011799\n10      rater2 0.2102910 -97.073351  0.000000e+00  0.000000e+00 0.1288749\n11      rater3 0.3424868 -71.428762  0.000000e+00  0.000000e+00 0.1956220\n12   Con:step1 0.8383549  -8.767845  1.821186e-18  9.105928e-18 0.8567601\n13 ProAd:step1 0.6746071 -17.860710  2.385854e-71  1.908684e-70 0.7093789\n14  Lang:step1 0.5168563 -29.428336 2.384008e-190 2.384008e-189 0.5191105\n15   Nar:step1 0.6848650 -16.325045  6.549193e-60  4.584435e-59 0.7059203\n16   Con:step2 0.5306292 -34.037821 6.146371e-254 7.375646e-253 0.4422111\n17 ProAd:step2 0.4301068 -42.502850  0.000000e+00  0.000000e+00 0.3495629\n18  Lang:step2 0.1968146 -75.897830  0.000000e+00  0.000000e+00 0.1635975\n19   Nar:step2 0.3706675 -46.198429  0.000000e+00  0.000000e+00 0.3078140\n20   Con:step3 0.2786884 -67.291049  0.000000e+00  0.000000e+00 0.2258991\n21 ProAd:step3 0.2765834 -63.618697  0.000000e+00  0.000000e+00 0.2089943\n22  Lang:step3 0.1940462 -83.250195  0.000000e+00  0.000000e+00 0.1146795\n23   Nar:step3 0.2486903 -65.156140  0.000000e+00  0.000000e+00 0.1831799\n24   Con:step4 0.9454987  -3.062781  2.192904e-03  3.658643e-03 0.5753692\n25 ProAd:step4 0.8972583  -5.504223  3.708007e-08  1.112402e-07 0.5197597\n26  Lang:step4 0.4646499 -36.807248 1.413705e-296 1.837816e-295 0.2790765\n27   Nar:step4 0.7695600 -12.352597  4.716384e-35  2.829830e-34 0.4601422\n       Infit_t       Infit_p   Infit_pholm\n1    43.518674  0.000000e+00  0.000000e+00\n2    25.305265 2.795231e-141 1.397615e-140\n3    50.193787  0.000000e+00  0.000000e+00\n4    36.569488 8.740425e-293 9.614468e-292\n5    -4.072490  4.651324e-05  4.651324e-05\n6   -65.039270  0.000000e+00  0.000000e+00\n7  -117.030438  0.000000e+00  0.000000e+00\n8   -35.943187 6.467215e-283 6.467215e-282\n9   -97.120190  0.000000e+00  0.000000e+00\n10 -118.521551  0.000000e+00  0.000000e+00\n11  -99.762812  0.000000e+00  0.000000e+00\n12   -7.714673  1.212932e-14  2.425863e-14\n13  -15.707699  1.339532e-55  5.358129e-55\n14  -29.253821 4.014915e-188 2.810441e-187\n15  -15.092688  1.809200e-51  5.427601e-51\n16  -42.570434  0.000000e+00  0.000000e+00\n17  -51.243782  0.000000e+00  0.000000e+00\n18  -82.205358  0.000000e+00  0.000000e+00\n19  -53.275846  0.000000e+00  0.000000e+00\n20  -75.825584  0.000000e+00  0.000000e+00\n21  -74.171526  0.000000e+00  0.000000e+00\n22 -101.621224  0.000000e+00  0.000000e+00\n23  -75.808557  0.000000e+00  0.000000e+00\n24  -27.893121 3.233260e-171 1.939956e-170\n25  -30.422787 2.744866e-203 2.195893e-202\n26  -56.571162  0.000000e+00  0.000000e+00\n27  -33.694854 6.876231e-249 6.188608e-248\n\n$time\n[1] \"2024-11-20 23:37:39 +03\" \"2024-11-20 23:37:39 +03\"\n\n$CALL\nTAM::tam.fit(tamobj = mod)\n\nattr(,\"class\")\n[1] \"tam.fit\"\n\n\n\n\nInfit and Outfit Values:\n\nInfit and Outfit near 1: Indicates that the item fits well with the model.\nInfit/Outfit significantly &gt;1: Indicates that the item is underfitting, meaning there is more variability in the responses than the model expects (perhaps caused by noise or misfitting responses).\nInfit/Outfit significantly &lt;1: Indicates that the item is overfitting, meaning the responses are too predictable, and there’s less variability than expected by the model (possibly due to redundancy or lack of challenge).\n\nLet’s break down a few examples from the output:\n\n\nContent (Con):\n\nOutfit MNSQ: 2.05, Infit MNSQ: 2.18\nThese values are well above 1, indicating underfit. The item “Con” might be too noisy or not behaving consistently with the model.\n\n\n\nPrompt Adherence (ProAd):\n\nOutfit MNSQ: 1.54, Infit MNSQ: 1.63\nThese values are higher than 1 but still in the acceptable range, meaning there’s some noise, but it’s not excessive.\n\n\n\nLanguage (Lang):\n\nOutfit MNSQ: 2.26, Infit MNSQ: 2.34\nThese values suggest significant underfit, similar to “Con”, indicating that responses to this domain might be less consistent or more unpredictable than the model expects.\n\n\n\nSteps (step1 to step4):\n\nSome steps, such as step1, have Infit and Outfit values closer to 1 (e.g., Outfit MNSQ: 0.95, Infit MNSQ: 0.94). These are acceptable and suggest that step1 is fitting well.\nHowever, step2, step3, and step4 show extremely low values, especially for Outfit (e.g., step3 has an Outfit MNSQ of 0.26), indicating overfit, meaning these categories are too predictable and might not differentiate well between respondents.\n\n\n\n\n\nt-statistics and p-values:\n\nThe t-statistics (Outfit_t, Infit_t) are standardized fit statistics that test whether the Infit/Outfit values significantly differ from 1. Large positive or negative t-values indicate significant deviation from expected values.\nThe p-values (Outfit_p, Infit_p) show whether these deviations are statistically significant. Nearly all p-values are extremely low (close to 0), suggesting that most of the items are statistically misfitting according to the model.\n\n\n\nRater Severity:\n\n\nRater1, Rater2, Rater3 all have very low Infit and Outfit values (e.g., Rater1 Outfit: 0.26), which suggest that these raters may be overfitting. This could mean that they are scoring in a highly predictable way, possibly being too strict or lenient consistently.\n\n\n\n\nreliability &lt;- mod$EAP.rel\nreliability\n\n[1] 0.9716057\n\n\nThe final WLE Reliability = 0.97 is an excellent reliability score, meaning that the person ability estimates are very consistent. WLE reliability, similar to other reliability coefficients like Cronbach’s alpha, indicates the precision of the estimates:\nA 0.97 reliability means that 97% of the variance in the person ability estimates is due to true differences in ability rather than measurement error.\n\npersons.mod &lt;- TAM::tam.wle(mod)\n\nIteration in WLE/MLE estimation  1   | Maximal change  2.8113 \nIteration in WLE/MLE estimation  2   | Maximal change  2.8413 \nIteration in WLE/MLE estimation  3   | Maximal change  2.5741 \nIteration in WLE/MLE estimation  4   | Maximal change  2.1203 \nIteration in WLE/MLE estimation  5   | Maximal change  2.3278 \nIteration in WLE/MLE estimation  6   | Maximal change  0.4228 \nIteration in WLE/MLE estimation  7   | Maximal change  0.0637 \nIteration in WLE/MLE estimation  8   | Maximal change  0.0071 \nIteration in WLE/MLE estimation  9   | Maximal change  8e-04 \nIteration in WLE/MLE estimation  10   | Maximal change  1e-04 \n----\n WLE Reliability= 0.97 \n\nggplot(data.frame(theta = persons.mod$theta), aes(x = theta)) +\n  geom_histogram(binwidth = 0.2, fill = \"steelblue\", color = \"black\") +\n  labs(title = \"Distribution of Person Ability Estimates\", x = \"Ability (Theta)\", y = \"Frequency\")\n\n\n\n\n\n\n\nAlso see the theta distributions in the chart. They do not look nice as this is a study run on simulated data.\n\nthr &lt;- TAM::tam.threshold(mod)\nthr\n\n                    Cat1       Cat2       Cat3     Cat4\nCon-rater1    -8.3634338 -4.8121033 -1.2173767 2.406464\nCon-rater2    -4.3898621 -0.8385315  2.7560120 6.380035\nCon-rater3    -0.5052795  3.0462341  6.6957092       NA\nProAd-rater1  -8.5340881 -4.4764709 -0.6612854 3.438995\nProAd-rater2  -4.5606995 -0.5028992  3.3122864 7.412567\nProAd-rater3  -0.6759338  3.3818665  7.2305603       NA\nLang-rater1  -10.0357361 -5.8611145 -1.8104553 2.433380\nLang-rater2   -6.0621643 -1.8877258  2.1631165 6.406952\nLang-rater3   -2.1775818  1.9970398  6.0768127       NA\nNar-rater1    -9.4873352 -5.3533630 -1.3221130 2.769928\nNar-rater2    -5.5137634 -1.3799744  2.6514587 6.743317\nNar-rater3    -1.6289978  2.5047913  6.5700989       NA\n\n\nOrdered thresholds are crucial to ensure that the categories are functioning properly. For example, for Con-rater1, the thresholds are: -8.36, -4.81, -1.21, and 2.41\nThese thresholds are in increasing order, which indicates that the rating scale is working as intended for Con-rater1—the higher categories represent more difficult levels to achieve.\nRaters 1, 2, and 3 Comparison:\n\n\nRater Differences: There are noticeable differences between raters in their thresholds. For example:\n\nCon-rater1 has very negative thresholds, starting at -8.36 for Cat1, while Con-rater3 starts much higher, with thresholds beginning at -0.50.\nThis suggests that Rater1 is much stricter or uses a harsher scale, while Rater3 is more lenient, with easier transitions between categories. For instance, it is harder for essays to move from a “1” to a “2” under Rater1’s scoring compared to Rater3.\nThere are some NAs which actually I have no idea about. Peobably these inconsistancies occur due to the simulated data."
  },
  {
    "objectID": "posts/Multi Facet Rasch Modelling/index.html#visualizing-the-results",
    "href": "posts/Multi Facet Rasch Modelling/index.html#visualizing-the-results",
    "title": "Multi-Facet Rasch Models with R",
    "section": "4. Visualizing the Results",
    "text": "4. Visualizing the Results\nTo make the interpretation more intuitive, we can visualize the item difficulty and rater severity using a dot plot for the difficulty estimates, which can help us compare how each domain and rater behaves. Here’s how we can generate these plots in R.\n\nfacet_params&lt;-mod[[\"xsi.facets\"]][[\"parameter\"]]\ndomain_params&lt;-facet_params[1:4]\n\nf1 &lt;- ggplot(data = persons.mod, aes(x = theta))+\n  geom_dotplot(binwidth = .1, stackdir = \"down\") + \n  theme_bw()  +\n  scale_y_continuous(name = \"\", breaks = NULL) +\n  scale_x_continuous(breaks=seq(-6, 6, .6), limits=c(-6, 6), \n                     position =  \"top\") + \n  theme(axis.title.y = element_blank(), \n        axis.title.x=element_blank(),\n        axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())+\n  labs(title = \"Persons\") + \n  coord_flip()\n\nf2 &lt;- mod$xsi.facets %&gt;%\n  filter(str_starts(parameter, \"rater\")) %&gt;%\n  ggplot(aes(x = xsi)) +\n  geom_text(aes(y = 0, label = parameter), nudge_y = 0.05, size = 3) +\n  theme_bw() +\n  scale_y_continuous(name = \"\", breaks = NULL) +\n  scale_x_continuous(breaks = seq(-6, 6, .5), limits = c(-6, 6), position = \"top\") + \n  theme(axis.title.y = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        axis.title.x = element_blank()) +\n  labs(title = \"Raters\") + \n  coord_flip()\n\nf3 &lt;- mod$xsi.facets %&gt;%\n  filter(parameter %in% domain_params) %&gt;%\n  ggplot(aes(x = xsi)) +\n  geom_text(aes(y = 0, label = parameter), nudge_y = 0.05, size = 3) + \n    theme_bw()  +\n    scale_y_continuous(name = \"\", breaks = NULL) +\n    scale_x_continuous(breaks=seq(-2, 2, .2), limits=c(-2, 2), \n                       position =  \"top\") + \n    theme(axis.title.y=element_blank(),\n          axis.text.y=element_blank(),\n          axis.ticks.y=element_blank(),\n          axis.title.x= element_blank())+\n    labs(title = \"Domain\") + \n    coord_flip()\n\nplot_grid(f1, f2, f3, nrow = 1, rel_widths = c(0.7, .15, .15))\n\n\n\n\n\n\n\nThis final chart is developed as an alternative to wrightmap. Each facet can be seen easily on it. There are four grids. The first is the person thetas. We have seen this above. The second is the rater facet. The strictness of the raters are very distinctive. Actually the only real data here is rater 2 and the others were simulated using it to be stricter and lenient. So we exactly see what the data is about. The third grid is about the domains/item difficulty. Pompt Adherence is the most difficult domain. Content, Narrative and Language follows it respectively."
  },
  {
    "objectID": "posts/Multi Facet Rasch Modelling/index.html#conclusion",
    "href": "posts/Multi Facet Rasch Modelling/index.html#conclusion",
    "title": "Multi-Facet Rasch Models with R",
    "section": "4. Conclusion",
    "text": "4. Conclusion\nIn this post, we explored the Multi-Facet Rasch Model (MFRM) using simulated essay scores rated by multiple raters across four different domains: Content, Prompt Adherence, Language, and Narrativity. The model helped us account for the varying levels of item difficulty and the potential differences in rater severity. By fitting the MFRM and examining key model outputs—like Infit/Outfit statistics and thresholds—we identified areas where raters were either more lenient or more severe, and items that displayed more noise or predictability than expected.\nThe high WLE reliability of 0.97 indicates that the model provides consistent and accurate estimates of person abilities. However, the rater-specific thresholds revealed some important differences in how each rater scored the essays, with certain raters being significantly stricter or more lenient. This highlights the importance of accounting for rater bias in assessments that rely on subjective judgments, such as essay scoring.\nGoing forward, addressing these rater differences and ensuring well-functioning rating categories can further refine the assessment process. By doing so, we can ensure that the scores are fairer and more representative of true essay quality, free from the influence of individual rater biases. Overall, the MFRM proves to be a valuable tool in maintaining the validity and reliability of assessments involving subjective judgments."
  },
  {
    "objectID": "posts/Item Analysis with Item Response Theory (in Turkish)/index.html",
    "href": "posts/Item Analysis with Item Response Theory (in Turkish)/index.html",
    "title": "Item Analysis with Item Response Theory (in Turkish)",
    "section": "",
    "text": "Bu çalışmada çok kategorili puanlanan maddelerden elde edilen bir veri seti kullanılmıştır. Çalışmanın ilk kısmında çok kategorili maddelere yönelik MTK analizleri yürütülmüştür. Daha sonra aynı veri seti iki kategorili verilere dönüştürülmüştür. Yine MTK süreçleri bu sefer de iki kategorili maddeler için yürütülmüştür. İzlenen adımlar şu şekildedir:\n1. Çoklu puanlanan maddelere yönelik olarak;\n\nUygun MTK modeli nedir?\nBu modele göre madde ve test parametreleri nasıldır?\nİdeal ve sorunlu madde örnekleri nasıldır?\nTest bilgi fonksiyonunu nasıldır?\nBirey yetenek puanlarının dağılımı nasıldır?\n\n2. Her bir maddeyi, kendi madde ortalamasından keserek 1-0 verisine dönüştürünüz. Buna göre iki kategorili puanlanan maddelere yönelik olarak;\n\nUygun MTK modeli nedir?\nBu modele göre madde ve test parametreleri nasıldır?\nİdeal ve sorunlu madde örnekleri nasıldır?\nTest bilgi fonksiyonunu nasıldır?\nBirey yetenek puanlarının dağılımı nasıldır?\n\n\nKullanılan paketleri listeleyelim:\n\nlibrary(psych)\nlibrary(GPArotation)\nlibrary(sirt)\nlibrary(ltm)\n\nTabi ki işe öncelikle verinin working directory’den yüklenmesi ve ön düzenleme süreçleri ile başladık:\n\ndata1&lt;- read.csv2(\"sampledata.csv\")\nstr(data1) #yapısını incelemek için.\n\n'data.frame':   531 obs. of  25 variables:\n $ SIRA    : int  1 2 3 4 5 6 7 8 9 10 ...\n $ Fakulte : chr  \"Diger\" \"Diger\" \"Diger\" \"Diger\" ...\n $ Sinif   : int  4 4 5 4 5 4 4 4 4 4 ...\n $ Cinsiyet: chr  \"Kad\\xfdn\" \"Kad\\xfdn\" \"Kad\\xfdn\" \"Kad\\xfdn\" ...\n $ Ortalama: chr  \"2.88\" \"2.93\" \"3.12\" \"3.28\" ...\n $ L1      : int  2 1 2 2 1 3 3 1 1 2 ...\n $ L2      : int  1 1 4 2 1 4 4 3 4 1 ...\n $ L3      : int  2 4 4 3 1 4 3 1 4 4 ...\n $ L4      : int  1 2 4 3 2 4 4 1 2 3 ...\n $ L5      : int  2 4 4 4 2 5 4 1 3 4 ...\n $ L6      : int  1 5 4 3 4 3 3 5 4 2 ...\n $ L7      : int  2 1 4 3 5 4 4 1 2 3 ...\n $ L8      : int  1 3 3 3 1 4 4 1 2 1 ...\n $ L9      : int  2 1 4 4 1 3 5 1 3 4 ...\n $ L10     : int  1 1 2 3 2 4 4 1 4 2 ...\n $ L11     : int  2 1 4 3 3 4 5 4 3 2 ...\n $ L12     : int  1 3 4 3 5 4 5 2 3 2 ...\n $ L13     : int  2 1 2 2 1 2 3 1 1 4 ...\n $ L14     : int  1 1 1 2 1 3 4 1 2 5 ...\n $ L15     : int  3 3 4 3 2 3 4 1 2 4 ...\n $ L16     : int  1 1 1 2 1 2 2 2 2 5 ...\n $ L17     : int  2 2 2 2 1 3 4 1 2 4 ...\n $ L18     : int  2 2 2 3 4 3 2 1 2 3 ...\n $ L19     : int  2 2 2 3 2 4 3 1 4 2 ...\n $ L20     : int  1 1 2 3 1 3 4 1 3 4 ...\n\n\nVeri setinde ilk sütunun sıra sayıları olduğunu görünce aman tanrım dedik ve sildik.\n\ndata1&lt;-data1[,-1] # fazla kalabalığa gerek yok, aynı isimle devam.\n\nKayıp veri olup olmadığını anlamak için:\n\ndata1[data1 == 0] &lt;- NA\nsum(is.na(data1))\n\n[1] 44\n\n\nNeredeyse %8 oranında missing value var. Too much! Alayını atıyoruz. Artık adını da değiştirelim.\n\ndata2&lt;- na.omit(data1)\n\nMadem ki öylesine bir veri seti ile öylesine bir analiz yapıyoruz ve practical kaygılarımız yok, veri setimizi büyütelim. 1000 kişi olsun:\n\nset.seed(16611106)  # olur da sen de denemek istersen diye aynı veri setini üretmemizi sağlar.\ndata3 &lt;- data2[sample(1:495, 1000, replace = T), ] #adını da değiştirelim \n\nSon olarak, veri setinde işimize yaramayacak bir sürü demografik detay var. Atıyoruz:\n\nrow.names(data3)&lt;- NULL #önce adlarını silelim.\n\ndata4&lt;- data3[,5:24]  #5. sütundan sonrası çöp  \n\nÇalışmanın birinci araştırma sorusu kapsamında “data4” adlı veri seti kullanılmıştır. Bu veri seti 1000 gözlemden ve 20 değişkenden oluşmaktadır. Değişkenler, 1 ile 5 arasında bir tam sayı değeri almaktadır. Çalışmanın ikinci araştırma sorusu kapsamında ise her bir madde kendi madde ortalamasından kesilerek iki kategorili veriye dönüştürülmüştür. Bu aşamalar ilgili başlık altında raporlaştırılmıştır. Ön düzenlemelerin ardından araştırma sorularına cevap aramak için ileri analizlere devam edilmiştir."
  },
  {
    "objectID": "posts/Item Analysis with Item Response Theory (in Turkish)/index.html#giriş",
    "href": "posts/Item Analysis with Item Response Theory (in Turkish)/index.html#giriş",
    "title": "Item Analysis with Item Response Theory (in Turkish)",
    "section": "",
    "text": "Bu çalışmada çok kategorili puanlanan maddelerden elde edilen bir veri seti kullanılmıştır. Çalışmanın ilk kısmında çok kategorili maddelere yönelik MTK analizleri yürütülmüştür. Daha sonra aynı veri seti iki kategorili verilere dönüştürülmüştür. Yine MTK süreçleri bu sefer de iki kategorili maddeler için yürütülmüştür. İzlenen adımlar şu şekildedir:\n1. Çoklu puanlanan maddelere yönelik olarak;\n\nUygun MTK modeli nedir?\nBu modele göre madde ve test parametreleri nasıldır?\nİdeal ve sorunlu madde örnekleri nasıldır?\nTest bilgi fonksiyonunu nasıldır?\nBirey yetenek puanlarının dağılımı nasıldır?\n\n2. Her bir maddeyi, kendi madde ortalamasından keserek 1-0 verisine dönüştürünüz. Buna göre iki kategorili puanlanan maddelere yönelik olarak;\n\nUygun MTK modeli nedir?\nBu modele göre madde ve test parametreleri nasıldır?\nİdeal ve sorunlu madde örnekleri nasıldır?\nTest bilgi fonksiyonunu nasıldır?\nBirey yetenek puanlarının dağılımı nasıldır?\n\n\nKullanılan paketleri listeleyelim:\n\nlibrary(psych)\nlibrary(GPArotation)\nlibrary(sirt)\nlibrary(ltm)\n\nTabi ki işe öncelikle verinin working directory’den yüklenmesi ve ön düzenleme süreçleri ile başladık:\n\ndata1&lt;- read.csv2(\"sampledata.csv\")\nstr(data1) #yapısını incelemek için.\n\n'data.frame':   531 obs. of  25 variables:\n $ SIRA    : int  1 2 3 4 5 6 7 8 9 10 ...\n $ Fakulte : chr  \"Diger\" \"Diger\" \"Diger\" \"Diger\" ...\n $ Sinif   : int  4 4 5 4 5 4 4 4 4 4 ...\n $ Cinsiyet: chr  \"Kad\\xfdn\" \"Kad\\xfdn\" \"Kad\\xfdn\" \"Kad\\xfdn\" ...\n $ Ortalama: chr  \"2.88\" \"2.93\" \"3.12\" \"3.28\" ...\n $ L1      : int  2 1 2 2 1 3 3 1 1 2 ...\n $ L2      : int  1 1 4 2 1 4 4 3 4 1 ...\n $ L3      : int  2 4 4 3 1 4 3 1 4 4 ...\n $ L4      : int  1 2 4 3 2 4 4 1 2 3 ...\n $ L5      : int  2 4 4 4 2 5 4 1 3 4 ...\n $ L6      : int  1 5 4 3 4 3 3 5 4 2 ...\n $ L7      : int  2 1 4 3 5 4 4 1 2 3 ...\n $ L8      : int  1 3 3 3 1 4 4 1 2 1 ...\n $ L9      : int  2 1 4 4 1 3 5 1 3 4 ...\n $ L10     : int  1 1 2 3 2 4 4 1 4 2 ...\n $ L11     : int  2 1 4 3 3 4 5 4 3 2 ...\n $ L12     : int  1 3 4 3 5 4 5 2 3 2 ...\n $ L13     : int  2 1 2 2 1 2 3 1 1 4 ...\n $ L14     : int  1 1 1 2 1 3 4 1 2 5 ...\n $ L15     : int  3 3 4 3 2 3 4 1 2 4 ...\n $ L16     : int  1 1 1 2 1 2 2 2 2 5 ...\n $ L17     : int  2 2 2 2 1 3 4 1 2 4 ...\n $ L18     : int  2 2 2 3 4 3 2 1 2 3 ...\n $ L19     : int  2 2 2 3 2 4 3 1 4 2 ...\n $ L20     : int  1 1 2 3 1 3 4 1 3 4 ...\n\n\nVeri setinde ilk sütunun sıra sayıları olduğunu görünce aman tanrım dedik ve sildik.\n\ndata1&lt;-data1[,-1] # fazla kalabalığa gerek yok, aynı isimle devam.\n\nKayıp veri olup olmadığını anlamak için:\n\ndata1[data1 == 0] &lt;- NA\nsum(is.na(data1))\n\n[1] 44\n\n\nNeredeyse %8 oranında missing value var. Too much! Alayını atıyoruz. Artık adını da değiştirelim.\n\ndata2&lt;- na.omit(data1)\n\nMadem ki öylesine bir veri seti ile öylesine bir analiz yapıyoruz ve practical kaygılarımız yok, veri setimizi büyütelim. 1000 kişi olsun:\n\nset.seed(16611106)  # olur da sen de denemek istersen diye aynı veri setini üretmemizi sağlar.\ndata3 &lt;- data2[sample(1:495, 1000, replace = T), ] #adını da değiştirelim \n\nSon olarak, veri setinde işimize yaramayacak bir sürü demografik detay var. Atıyoruz:\n\nrow.names(data3)&lt;- NULL #önce adlarını silelim.\n\ndata4&lt;- data3[,5:24]  #5. sütundan sonrası çöp  \n\nÇalışmanın birinci araştırma sorusu kapsamında “data4” adlı veri seti kullanılmıştır. Bu veri seti 1000 gözlemden ve 20 değişkenden oluşmaktadır. Değişkenler, 1 ile 5 arasında bir tam sayı değeri almaktadır. Çalışmanın ikinci araştırma sorusu kapsamında ise her bir madde kendi madde ortalamasından kesilerek iki kategorili veriye dönüştürülmüştür. Bu aşamalar ilgili başlık altında raporlaştırılmıştır. Ön düzenlemelerin ardından araştırma sorularına cevap aramak için ileri analizlere devam edilmiştir."
  },
  {
    "objectID": "posts/Item Analysis with Item Response Theory (in Turkish)/index.html#çok-boyutlu-maddelere-yönelik-aşamalar",
    "href": "posts/Item Analysis with Item Response Theory (in Turkish)/index.html#çok-boyutlu-maddelere-yönelik-aşamalar",
    "title": "Item Analysis with Item Response Theory (in Turkish)",
    "section": "1. Çok Boyutlu Maddelere Yönelik Aşamalar",
    "text": "1. Çok Boyutlu Maddelere Yönelik Aşamalar\nÇalışmanın ilk araştırma sorusu kapsamında, yapılan analizler çok kategorili puanlanan maddeler üzerinden yürütülmüştür.\n1.a. Varsayımların kontrolü ve uygun MTK modelinin belirlenmesi\nÇok kategorili puanlanan maddelerden oluşan ölçeğin Madde Tepki Kuramı çerçevesinde incelenmesi sürecinde öncelikle uygun MTK modelinin belirlenebilmesi için varsayım kontrolleri yapılmıştır. Bu bağlamda, tek boyutluluk ve yerel bağımsızlık varsayımları ile model-veri uyumu kontrol edilmiştir. Tek boyutluluk varsayımı kontrolü için paralel analiz, yamaç birikinti grafiği ve faktör analizi kullanılmıştır. Bu aşamada kullanılan paketler: Psych ve GPArotation.\nParalel Analizden elde edilen yamaç birikinti grafiği şöyledir:\n\nfa.parallel(data4, n.obs = 1000, cor = \"poly\")\n\n\n\n\n\n\n\nParallel analysis suggests that the number of factors =  5  and the number of components =  2 \n\n\nYamaç birikinti grafiği iki bileşenli bir yapıyı göstermektedir.  Son olarak faktör analizi yardımı ile hem tek hem de iki bileşenli modeller oluşturulmuştur:\n\nfa_model1 &lt;- fa.poly(data4)\nfa_model2 &lt;- fa(data4, nfactors = 2, cor=\"poly\")\n\nBu iki modelin burada çıktılarını alsak baya uzun oluyor. Ama özetle; ben beğendiğim ve devam analizi için seçtiğim 2 faktörlü model ile devam ediyorum. Bu modelin çıktıları incelendiğinde, 1-12 numaralı maddelerin bir boyutta, 13-20 numaralı maddelerin ise diğer bir boyutta yüklendiği görülmektedir. Bu nedenle veri seti aşağıdaki adımlar izlenerek ikiye bölünmüş ve ileri analizler her iki faktör için de ayrı ayrı yürütülmüştür.\n\ndata4a&lt;-data4[1:12]\ndata4b&lt;-data4[13:20]\n\nYerel bağımsızlık varsayımının kontrolü için Yen’in Q analizi (Yen, 1984) her iki bileşen için de uygulanmıştır. Bu süreçte sirt paketinden (Robitzsch, 2020) yararlanılmış ve aşağıdaki adımlar izlenmiştir.  \nMod1 &lt;- TAM::tam.mml( resp=data4a )\nMod1.wle &lt;- TAM::tam.wle(Mod1)\nMod1.q3 &lt;- sirt::Q3( dat=data4a, theta=Mod1.wle$theta, b=Mod1$item_irt[[3]] )\nMod2 &lt;- TAM::tam.mml( resp=data4b )\nMod2.wle &lt;- TAM::tam.wle(Mod2)\nMod2.q3 &lt;- sirt::Q3( dat=data4b, theta=Mod2.wle$theta, b=Mod2$item_irt[[3]] )\nBu üstteki kodun çıktısı çooook uzun. Buraya koymuyorum. Tabi biz veriyi bootstrap ile çoğalttığımız için bu varsayım karşılanmadı ama gerçek veri ile çalışsaydık bu varsayımın karşılanmaması durumunda çöp olacaktı analiz. Yani örneklem yetersiz, daha çok örneklem lazım diyecektik. Ya da modeli veya madde sayılarını inceleyecektik vs. vs.\nBirinci araştırma sorusunun son aşamasında ise model veri uyumunun incelenmesi ve en uygun modelin seçilmesi yer almaktadır. Model-veri uyumu incelemesi ltm paketi (Rizopoulos, 2006) yardımı ile her iki bileşen için de ayrı ayrı GRM modeli ile yürütülmüştür. Her iki bileşende de model-1, ayırt edicilik düzeylerinin her madde için farklılaştığı modeli betimlemektedir. Model-2 ise ayırt edicilik düzeylerinin her madde için eşit tutulduğu modeldir.\n\nmodel1_d4a&lt;- grm(data4a)\nmodel2_d4a&lt;- grm(data4a, constrained = TRUE)\nmodel1_d4b&lt;- grm(data4b)\nmodel2_d4b&lt;- grm(data4b, constrained = TRUE)\nanova(model2_d4a, model1_d4a)\n\n\n Likelihood Ratio Table\n                AIC      BIC   log.Lik    LRT df p.value\nmodel2_d4a 31785.45 32025.93 -15843.72                  \nmodel1_d4a 31660.51 31954.98 -15770.26 146.94 11  &lt;0.001\n\nanova(model2_d4b, model1_d4b)\n\n\n Likelihood Ratio Table\n                AIC      BIC   log.Lik   LRT df p.value\nmodel2_d4b 21742.58 21904.54 -10838.29                 \nmodel1_d4b 21664.77 21861.08 -10792.38 91.81  7  &lt;0.001\n\n\nModeller arasında manidar farklılık anlamına gelen p değerlerine (&lt;.05) sahip olmasının yanı sıra, Akaike ve Bayesian bilgi kriter değerleri en düşük olan modellerin her iki bileşen için de model-1 olduğu görülmektedir. Bu nedenle model-1 ile daha iyi bir model-veri uyumu sağlanmaktadır. Devam analizleri her iki bileşen için de model-1 ile yürütülmüştür.\n1.b. Madde parametreleri\nModel-veri uyumu sınandıktan ve en uygun model belirlendikten sonra, madde parametrelerinin incelenmesi aşamasına geçilmiştir. Model-1 üzerinden aşağıdaki kod satırları kullanılarak elde edilen madde parametreleri görülebilir.\n\ncoef(model1_d4a)\n\n    Extrmt1 Extrmt2 Extrmt3 Extrmt4 Dscrmn\nL1   -1.259  -0.119   0.576   1.625  2.361\nL2   -0.994   0.224   1.002   2.265  1.383\nL3   -1.157  -0.363   0.254   1.498  2.401\nL4   -1.270  -0.276   0.610   1.718  2.355\nL5   -1.130  -0.312   0.337   1.382  2.248\nL6   -1.565  -0.657   0.322   1.309  1.895\nL7   -1.061  -0.257   0.460   1.453  2.129\nL8   -1.201  -0.401   0.314   1.367  2.635\nL9   -1.102  -0.322   0.363   1.246  2.296\nL10  -1.222  -0.295   0.476   1.473  1.815\nL11  -1.218  -0.457   0.345   1.320  2.123\nL12  -1.096  -0.249   0.737   1.819  1.661\n\ncoef(model1_d4b)\n\n    Extrmt1 Extrmt2 Extrmt3 Extrmt4 Dscrmn\nL13  -0.665   0.150   0.830   1.786  2.353\nL14  -0.791   0.148   0.917   1.802  2.290\nL15  -1.388  -0.736   0.198   0.920  2.308\nL16  -1.210   0.064   1.206   1.969  1.718\nL17  -0.998   0.150   1.077   1.832  1.898\nL18  -1.775  -0.720   0.598   1.664  1.338\nL19  -1.751  -0.514   0.694   1.600  1.505\nL20  -1.262  -0.351   0.505   1.277  2.007\n\n\nTablo 4 incelendiğinde, her madde için eşik parametrelerinin beklendik bir şekilde birinciden dördüncüye doğru arttığı görülmektedir. Bazı maddelerin birinci eşik parametresinin -1’den daha büyük bir değerle başladığı dikkat çekmektedir. Örneğin ikinci bileşene ait ilk madde olan 13. Maddenin ilk kategorisini seçen bir bireyin yetenek seviyesi %50 ihtimalle -.66’dan daha düşüktür. \n1.c. Örnek Madde Karakteristik Eğrileri\nÇalışmanın bu aşamasında madde karakteristik eğrisi ideal ve sorunlu olan birer madde incelenmiş ve yorumlanmıştır. Bu nedenle, öncelikle birinci bileşeni oluşturan tüm maddelerin madde karakteristik eğrileri plot() fonksiyonu ile oluşturulmuştur. İdeal bir madde karakteristik eğrisine sahip olduğu düşünülen sekizinci madde ve kısmi sorunlu olduğu düşünülen ikinci maddenin madde karakteristik eğrileri aşağıdaki kod satırları yürütülerek oluşturulmuştur.\n\nplot(model1_d4a, type=\"ICC\",item=6, xlab= \"YETENEK\", cex.main = 1, main = \"MADDE KARAKTERİSTİK EĞRİSİ- Madde: 6\", ylab = \"OLASILIK\" , lwd= 2, col.main= \"red\", font.axis= 3, font.lab=2)\n\n\n\n\n\n\nplot(model1_d4a, type=\"ICC\",xlab= \"YETENEK\", cex.main = 1, ylab = \"OLASILIK\" , lwd= 2, col.main= \"red\", font.axis= 3, font.lab=2, main = \"MADDE KARAKTERİSTİK EĞRİSİ- Madde: 2\", items = 2)\n\n\n\n\n\n\n\nGörülen ilk grafik diğer maddelere göre daha ideal dağılım gösteren bir maddeye aittir. Bu maddenin eğrileri tüm yetenek düzeylerini kapsayacak şekilde sivrilip dağılmaktadır. Örneğin, sıfır yetenek düzeyinde bir bireyin üçüncü kategoride yer alma olasılığı en yüksek düzeydedir. Benzer şekillerde diğer kategorilerin de yüksek olasılık ile temsil ettikleri yetenek düzeyleri belirgin bir şekilde görülmektedir. Bu durumda bu maddenin ayırt edicilik düzeyinin yüksek olması beklenir. Madde parametreleri çıktısında da görüleceği üzere, bu maddeye ait ayırt edicilik parametresi birinci bileşenin en yüksek ayırt edicilik düzeyidir.\nİkinci grafik ise kısmen problemli olduğu düşünülen bir grafiktir. Bu grafiğin daha iyi yorumlanabilmesi için, eksenlerinde yer alan değerler axis() fonksiyonu ile daha detaylı hale getirilmiştir. Ayrıca abline() fonksiyonu ile 0.6 yetenek düzeyine dikey bir çizgi eklenmiştir.\n\nplot(model1_d4a, type=\"ICC\",\n     xlab= \"YETENEK\", \n     cex.main = 1, \n     ylab = \"OLASILIK\" , \n     lwd= 2, col.main= \"red\", \n     font.axis= 3, \n     font.lab=2, \n     main = \"MADDE KARAKTERİSTİK EĞRİSİ- Madde: 2\", \n     items = 2)\naxis(2, at = seq(0, 1, by = .1))\naxis(1, at = seq(-4, 4, by = .1))\nabline(v=.6)\n\n\n\n\n\n\n\nMaddenin eğrileri incelendiğinde, 0.6 yetenek düzeyinde bir bireyin ikinci, üçüncü ve dördüncü kategorileri seçme olasılıklarının birbirlerine çok yakın olduğu görülmektedir. Bu durum, maddenin ayırt ediciliğini olumsuz olarak etkilemektedir. Daha önceki çıktılardan bilindiği üzere bu maddeye ait ayırt edicilik parametresi birinci bileşenin en düşük ayırt edicilik düzeyidir. Ayrıca, bu maddenin üçüncü kategorisinin sivrilmediği de dikkat çekmektedir. Bu durumda bu maddenin üçüncü kategorisinin çıkarılarak dört kategorili bir maddeye dönüştürülmesi düşünülebilir. Bir başka seçenek ise bu maddenin ölçekten çıkarılması ve analizlerin yeniden yapılması olabilir. \n1.d. Test Bilgi Fonksiyonu\nHer iki bileşen için de test bilgi fonksiyonları aşağıdaki kod satırları kullanılarak elde edilmiştir:\n\nplot(model1_d4a, type=\"IIC\", items = 0, xlab= \"YETENEK\", cex.main = 1, main = \"TEST   BİLGİ   FONKSİYONU\", ylab = \"BİLGİ\" , lwd= 2, col.main= \"red\", col=\"blue\", font.axis= 3, font.lab=2)\n\n\n\n\n\n\nplot(model1_d4b, type=\"IIC\", items = 0, xlab= \"YETENEK\", cex.main = 1, main = \"TEST   BİLGİ   FONKSİYONU\", ylab = \"BİLGİ\" , lwd= 2, col.main= \"red\", col=\"blue\", font.axis= 3, font.lab=2)\n\n\n\n\n\n\n\nTest bilgi fonksiyonlarının 0 yetenek düzeyinde en yüksek seviyede olduğu, -/+ 2 yetenek düzeylerinde sert bir şekilde düşmeye başladığı ve -/+ 4 yetenek düzeylerinde en düşük seviyesinde olduğu görülmektedir. Bu durumda her iki bileşenin de en çok 0 yetenek düzeyinde bilgi sağladığı ve -2 ile +2 aralığında yüksek düzeyde bilgi sağladığı söylenebilir. Ancak bu aralığın ötesinde sağlanan bilginin hızla azaldığı düşünülebilir. \nÇalışmanın bu aşamasında madde bilgi eğrileri de aşağıdaki kod satırları kullanılarak elde edilmiştir:\n\nplot(model1_d4a, type=\"IIC\",xlab= \"YETENEK\", cex.main = 1, ylab = \"BİLGİ\" , col.main= \"red\", font.axis= 3, font.lab=2, main = \"1. BİLEŞEN MADDE BİLGİ  EĞRİLERİ\")\n\n\n\n\n\n\nplot(model1_d4b, type=\"IIC\",xlab= \"YETENEK\", cex.main = 1, ylab = \"BİLGİ\" , col.main= \"red\", font.axis= 3, font.lab=2, main = \"2. BİLEŞEN MADDE BİLGİ  EĞRİLERİ\")\n\n\n\n\n\n\n\nBunlar incelendiğinde ise her iki bileşen için de maddelerin ölçülen özelliği geniş bir yetenek puanı ölçeğinde ölçtüğü görülmektedir. Birinci bileşen içerisinde en düşük bilgi sağlayan maddenin ikinci madde olduğu dikkat çekmektedir. Yine bilgi eğrisi en yüksek maddenin birinci bileşen için sekizinci madde olduğu da görülmektedir.  Bunlar, bir önceki bölümde kısmi problemli ve ideal dağılımlı olarak incelenen maddelerdir. Eğer ikinci bileşen için birer madde seçilecek olsaydı, o bileşene ait altı ve üç numaralı maddeler sırasıyla kısmi problemli ve ideal maddelere örnek olarak seçilebilirdi. \n1.e. Yetenek Puanları\nÇok kategorili puanlanan maddelere yönelik olarak yürütülen analizlerin son aşamasında bireylere ait yetenek puanları hesaplanmış ve bunların dağılımı incelenmiştir. Analizlerin bu aşamasında şu kod satırları kullanılarak birinci ve ikinci bileşen için yetenek puanları ayrı ayrı hesaplanmıştır. \n\nCode#BİLEŞEN-1\nscore_d4a&lt;- factor.scores(model1_d4a)\noruntu_d4a&lt;- score_d4a[[1]]\noruntu_d4a$toplam&lt;- rowSums((oruntu_d4a[,1:12]))\nscore_d4a&lt;- factor.scores(model1_d4a)\noruntu_d4a&lt;- score_d4a[[1]]\noruntu_d4a$toplam&lt;- rowSums((oruntu_d4a[,1:12]))\ntheta_d4a &lt;- numeric()\nfor (i in 1:419){ \n for (j in 1:1000){\n         if (sum (oruntu_d4a[i, 1: 12] == data4a[j, 1:12])==12)\n              theta_d4a[j] &lt;- oruntu_d4a[i, 15]      }}\ndata4a$theta&lt;-theta_d4a\ndata4a$toplam&lt;-rowSums(data4a[1:12])\n\n\n\n#BİLEŞEN-2\nscore_d4b&lt;- factor.scores(model1_d4b)\noruntu_d4b&lt;- score_d4b[[1]]\noruntu_d4b$toplam&lt;- rowSums((oruntu_d4b[,1:12]))\ntheta_d4b &lt;- numeric()\nfor (i in 1:404){\n  for (j in 1:1000){\n      if (sum (oruntu_d4b[i, 1: 8] == data4b[j, 1:8])==8)\n             theta_d4b[j] &lt;- oruntu_d4b[i, 11] }}\ndata4b$theta&lt;-theta_d4b\ndata4b$toplam&lt;-rowSums(data4b[1:8])\n\n\nBunun ardından describe() fonksiyonu ile birey yetenek puanlarının betimsel istatikleri elde edilmiştir. Ayrıca hist() fonksiyonu ile histogram grafikleri oluşturulmuştur. Şu kod satırları kullanılmıştır. \n\nCode#BİLEŞEN-1\ndescribe(data4a$theta)\n\n   vars    n mean   sd median trimmed  mad  min  max range skew kurtosis   se\nX1    1 1000 0.05 0.93   0.02    0.05 0.92 -2.3 2.59  4.88 0.03     -0.2 0.03\n\nCodehist(data4a$theta, xlab= \"YETENEK\", cex.main = 1, ylab = \"FREKANS\" , col.main= \"red\", font.axis= 3, font.lab=2, main = \"1. BİLEŞEN BİREY YETENEK PUANLARI\")\n\n\n\n\n\n\nCode#BİLEŞEN-2\ndescribe(data4b$theta)\n\n   vars    n  mean   sd median trimmed  mad   min max range  skew kurtosis   se\nX1    1 1000 -0.01 0.92   0.05       0 0.89 -2.19 2.5  4.69 -0.04    -0.14 0.03\n\nCodehist(data4b$theta, xlab= \"YETENEK\", cex.main = 1, ylab = \"FREKANS\" , col.main= \"red\", font.axis= 3, font.lab=2, main = \"2. BİLEŞEN BİREY YETENEK PUANLARI\")\n\n\n\n\n\n\n\nHer iki bileşene ait betimsel istatikler ve histogram grafikleri incelendiğinde, ortalamalarının sıfıra, standart sapmalarının da bire çok yakın olduğu görülmektedir. Bu durum her iki bileşenden elde edilen verinin de normal dağılım sergilediğini göstermektedir."
  },
  {
    "objectID": "posts/Item Analysis with Item Response Theory (in Turkish)/index.html#iki-kategorili-maddelere-yönelik-aşamalar",
    "href": "posts/Item Analysis with Item Response Theory (in Turkish)/index.html#iki-kategorili-maddelere-yönelik-aşamalar",
    "title": "Item Analysis with Item Response Theory (in Turkish)",
    "section": "2. İki Kategorili Maddelere Yönelik Aşamalar",
    "text": "2. İki Kategorili Maddelere Yönelik Aşamalar\nÇalışmanın ikinci araştırma sorusu kapsamında, çok kategorili maddelerden oluşan veri setinin iki kategoriye dönüştürülmesi gerekmektedir. Çok kategorili puanlanan maddelerin iki kategorili olarak kodlanmasında izlenen adımlar şunlardır:\n\ndata5&lt;- data4\nfor(i in 1:1000) {\n   for(j in 1:20) {\nif(data5[i,j] &lt;= mean(data5[,j])) {data5[i,j] &lt;- 0} \nelse data5[i,j] &lt;- 1\n }\n}\n\n2.a. Varsayımların Kontrolü ve Uygun MTK Modeli\nİki kategorili puanlanan maddelerden oluşan ölçeğin Madde Tepki Kuramı çerçevesinde incelenmesi sürecinde öncelikle uygun MTK modelinin belirlenebilmesi için varsayım kontrolleri yapılmıştır. Bu bağlamda, tek boyutluluk ve yerel bağımsızlık varsayımları ile model-veri uyumu kontrol edilmiştir. Tek boyutluluk varsayımı kontrolü için paralel analiz, yamaç birikinti grafiği ve faktör analizi kullanılmıştır. Bu amaçla, Psych paketinden (Revelle, 2020) faydalanılmıştır.\n\nfa.parallel(data5, main = \"PARALEL ANALİZ SAÇILIM GRAFİĞİ\", ylabel = \"Temel Bileşenler ve Faktör Analizi Özdeğerleri\")\n\n\n\n\n\n\n\nParallel analysis suggests that the number of factors =  4  and the number of components =  2 \n\n\nParalel analiz ve yamaç birikinti grafiği iki bileşenli bir yapıyı göstermektedir.  Bu nedenle hem iki hem de tek bileşenli modeller oluşturulmuştur. Aşağıda izlenen adımlar sonrası elde edilen modeller ait faktör yükleri görülebilir.\n\nmodel1 &lt;- fa(data5, cor = \"tet\")\nmodel2 &lt;- fa(data5, cor = \"tet\", nfactors = 2)\n\nFaktör yüklerinin her madde için her iki modelde de .60’nın üzerinde olduğu görülmektedir. Tek bileşenli yapının açıkladığı varyansın ise %61 olduğu anlaşılmaktadır. Bunun yanı sıra, iki bileşenli yapının birinci bileşenin %40, ikinci bileşenin %28 olmak üzere toplamda %68 oranında açıklanan varyansa sahip olduğu görülmektedir. Açıklanan varyans açısından modeller arasındaki farkın çok büyük olmadığı düşünüldüğünden, ileri analizlere tek bileşenli model ile devam edilmesine karar verilmiştir.  \nYerel bağımsızlık varsayımının kontrolü için Yen’in Q analizi (Yen, 1984)  uygulanmıştır. Bu süreçte sirt paketinden (Robitzsch, 2020) yararlanılmış ve aşağıdaki adımlar izlenmiştir. \n\nCodemodel &lt;- rasch.mml2(data5)\nmodel.wle &lt;-wle.rasch( dat=data5, b=model$item$b )\nyenq3 &lt;-Q3( dat=data5, theta=model.wle$theta, b=model$item$b)\n\n\nYerel bağımsızlık varsayımının karşılandığı görülmektedir. Bu nedenle model-veri uyumu incleme aşamasına geçilmiştir. Bu aşamada ltm paketinden (Rizopoulos, 2006) faydalanılmıştır. Aşağıdaki kod satırları kullanılarak elde edilen Benzerlik oranı tabloları görülebilir.\n\nmodel1 &lt;- rasch(data5)\nmodel2 &lt;- ltm(data5 ~ z1)  \nmodel3 &lt;- tpm(data5)\nanova(model1, model2)\n\n\n Likelihood Ratio Table\n            AIC      BIC  log.Lik    LRT df p.value\nmodel1 15975.79 16078.85 -7966.89                  \nmodel2 15913.21 16109.52 -7916.61 100.58 19  &lt;0.001\n\nanova(model1, model3)\n\n\n Likelihood Ratio Table\n            AIC      BIC  log.Lik    LRT df p.value\nmodel1 15975.79 16078.85 -7966.89                  \nmodel3 15914.14 16208.61 -7897.07 139.64 39  &lt;0.001\n\nanova(model2, model3)\n\n\n Likelihood Ratio Table\n            AIC      BIC  log.Lik   LRT df p.value\nmodel2 15913.21 16109.52 -7916.61                 \nmodel3 15914.14 16208.61 -7897.07 39.07 20   0.007\n\n\nAkaike ve Bayesian bilgi kriterleri incelendiğinde, üç model arasında ikinci modelin manidar farkla en uygun model olduğu görülmektedir. Bu nedenle, devam analizleri iki parametreli model ile yürütülmüştür.\n2.b. Madde Parametreleri\nModel-veri uyumu sınandıktan ve en uygun model belirlendikten sonra, madde parametrelerinin incelenmesi aşamasına geçilmiştir. Model-2 üzerinden coef() fonksiyonu kullanılarak elde edilen madde parametreleri aşağıdadır.\n\ncoef(model2)\n\n        Dffclt   Dscrmn\nL1  -0.8112296 2.832181\nL2  -0.7312085 1.933827\nL3  -0.8181936 2.659799\nL4  -0.8109842 3.100650\nL5  -0.8380050 2.394060\nL6  -1.0180088 2.070947\nL7  -0.8694781 2.103077\nL8  -0.8977956 2.683691\nL9  -0.7951705 2.798913\nL10 -0.8580558 2.207506\nL11 -0.8789145 2.322482\nL12 -0.8528908 1.705619\nL13 -0.6390827 2.246105\nL14 -0.6903395 2.247931\nL15 -0.8834606 2.517332\nL16 -0.7716486 2.330535\nL17 -0.7333232 2.334138\nL18 -1.1852158 1.497807\nL19 -1.0471877 1.896025\nL20 -0.8162163 2.864486\n\n\nÇıktılar incelendiğinde, güçlük parametresi en yüksek olan maddenin 13. madde olduğu göze çarpmaktadır. Yetenek düzeyi -.63’ten daha yüksek olan bireyler bu maddeyi %50’den daha yüksek bir ihtimalle doğru cevaplayacaklardır.\n2.c.  Örnek Madde Karakteristik Eğrileri\nÇalışmanın bu aşamasında, iki kategorili puanlanan maddelerin analizinde madde karakteristik eğrisi ideal ve sorunlu olan birer madde incelenmiş ve yorumlanmıştır. Bu nedenle, öncelikle birinci bileşeni oluşturan tüm maddelerin madde bilgi eğrileri plot() fonksiyonu ile oluşturulmuş, aşağıdaki kod satırları kullanılmıştır:\n\nplot(model2, type=\"IIC\",xlab= \"YETENEK\", cex.main = 1, ylab = \"BİLGİ\" , col.main= \"red\", font.axis= 3, font.lab=2, main = \"MADDE BİLGİ  EĞRİLERİ\")\n\n\n\n\n\n\n\nMadde bilgi eğrileri incelendiğinde, en çok bilgiyi dördüncü maddenin sağladığı görülmektedir. En düşük bilgi sağlayan maddelerden birisinin ise 12. madde olduğu görülmektedir. Bu nedenle madde karakteristik eğrilerinin incelenmesi sürecinde örnek olarak bu iki madde tercih edilmiştir. Bu maddelere ait karakteristik eğrileri aşağıdaki kod satırları ile elde edilmiştir.\n\nplot(model2, type=\"ICC\", items = c(4,12), labels = c(\"madde-4\", \"madde-12\"), legend = T, xlab= \"YETENEK\", cex.main = 1, main = \"MADDE KARAKTERİSTİK EĞRİSİ\", ylab = \"OLASILIK\" , lwd= 2)\npoints(-.81, .5, lwd= 3, pch= 3)\ntext(-.81, .5, lwd= 2, labels = \"b par.: -0.81\", pos = 4)\npoints(-.85, .5, lwd= 3, col= \"red\", pch=5)\ntext(-.85, .5, lwd= 3, labels = \"b par.: -0.85\", pos = 2, col= \"red\")\n\n\n\n\n\n\n\nDört ve 12 numaralı maddelerin güçlük parametreleri birbirlerine oldukça yakındır. Ancak dördüncü madde diğerine göre daha dik bir karakteristik eğrisine sahiptir. Bu durum iki maddenin ayırt edicilik parametrelerindeki farklılıktan kaynaklanmaktadır. Dördüncü madde yüksek bir ayırt edicilik parametresi ile ideal bir madde gibi görünürken, 12. madde düşük bir ayırt edicilik parametresi ile kısmi problemli bir madde görüntüsü sergilemektedir. Yine de 12 numaralı maddenin parametre değerlerinin kabul edilebilir olduğu da vurgulanmalıdır. \n2.d. Test Bilgi Fonksiyonu\nİki kategorili puanlanan maddelere yönelik test bilgi fonksiyonunu elde etmek için aşağıdaki kod satırları kullanılmıştır. locator() fonksiyonu aracılığı ile test bilgi fonksiyonunun tepe noktası tespit edilmiş ve ardından abline() fonksiyonu ile tepe noktasının koordinat çizgileri grafiğe eklenmiştir.\n\nplot(model2, type=\"IIC\", items = 0, xlab= \"YETENEK\", cex.main = 1, main = \"TEST   BİLGİ   FONKSİYONU\", ylab = \"BİLGİ\" , lwd= 2,col.main=\"red\",col=\"blue\", font.axis= 3, font.lab=2)\naxis(2, at = seq(0, 30, by =5))\naxis(1, at = seq(-4, 4, by = .1))\nabline(h=27.74, v=-.8451, lty=4)\n\n\n\n\n\n\n\nTest bilgi fonksiyonunun -0.84 yetenek düzeyinde en yüksek seviyede olduğu, -1.5 ile 0 yetenek düzeylerinde sert bir şekilde düşmeye başladığı ve -2.8 ve 1.1 yetenek düzeylerinde en düşük seviyesinde olduğu görülmektedir. Bu durumda en yüksek bilginin bu ölçekte -.8 yetenek düzeylerinde sağlandığı söylenebilir. \nBu durumda her iki bileşenin de en çok 0 yetenek düzeyinde bilgi sağladığı ve -2 ile +2 aralığında yüksek düzeyde bilgi sağladığı söylenebilir. Ancak bu aralığın ötesinde sağlanan bilginin hızla azaldığı düşünülebilir.\n2.e. Yetenek Puanları\nİki kategorili puanlanan maddelere yönelik olarak yürütülen analizlerin son aşamasında bireylere ait yetenek puanları hesaplanmış ve bunların dağılımı incelenmiştir. Şu kod satırları ile hesaplanan yetenek puanları veri setine eklenmiştir.\n\nscore_d5&lt;- factor.scores.ltm(model2)\noruntu_d5 &lt;- score_d5[[1]]\noruntu_d5$toplam &lt;- rowSums((oruntu_d5[,1:12]))\ntheta_d5 &lt;- numeric()\nfor (i in 1:454){\n     for (j in 1:1000){\n         if (sum (oruntu_d5[i, 1: 20] == data5[j, 1:20])==20)\n             theta_d5[j] &lt;- oruntu_d5[i, 23] }}\ndata5$theta&lt;-theta_d5\ndata5$toplam&lt;-rowSums(data5[1:20])\n\nBunun ardından describe() fonksiyonu ile birey yetenek puanlarının betimsel istatikleri elde edilmiştir. Ayrıca hist() fonksiyonu ile histogram grafikleri oluşturulmuştur. Bu süreçlerde kullanılan kod satırları şunlardır:\n\n#BİLEŞEN-1\ndescribe(data5$theta)\n\n   vars    n  mean   sd median trimmed  mad   min  max range  skew kurtosis\nX1    1 1000 -0.03 0.81   0.05    0.04 1.24 -2.17 0.89  3.06 -0.45     -0.8\n     se\nX1 0.03\n\nhist(data5$theta, xlab= \"YETENEK\", cex.main = 1, ylab = \"FREKANS\" , col.main= \"red\", font.axis= 3, font.lab=2, main = \"BİREY YETENEK PUANLARI\")\n\n\n\n\n\n\n\nİncelenen iki kategorili puanlanan veriye ait betimsel istatikleri incelendiğinde, ortalamanın -.03, standart sapmanın da .81 olduğu görülmektedir. Bu durum verinin normal dağıldığı şeklinde yorumlanabilir, ancak histogram grafiği incelendiğinde yetenek puanlarının -2 ile 1 arasında dağılım gösterdiği ve 1 yetenek puanında bir yığılma olduğu görülmektedir. Bu durum, verilerin çok kategorili puanlanan maddelerden ortalamaları doğrultusunda iki kategorili puanlanan maddelere çevrilmesiyle ilişkisi olduğu düşünülmektedir."
  },
  {
    "objectID": "posts/Item Analysis with Item Response Theory (in Turkish)/index.html#sonuç",
    "href": "posts/Item Analysis with Item Response Theory (in Turkish)/index.html#sonuç",
    "title": "Item Analysis with Item Response Theory (in Turkish)",
    "section": "SONUÇ",
    "text": "SONUÇ\nBu çalışma simüle edilmiş veri ile yürütülmüştür. Çıktılarında bu nedenle temel problemler görülebilmektedir. Yine de aşama aşama MTK ile madde analizinin R ile nasıl yapılacağına dair bana güzel bir referans olmaktadır. Bu çalışma, bir ders raporu olarak hazırlanmıştır."
  },
  {
    "objectID": "posts/efaWithLikertData-2/index.html",
    "href": "posts/efaWithLikertData-2/index.html",
    "title": "Multidimensional IRT for Factor Exploration",
    "section": "",
    "text": "Can we use multidimensional IRT (mIRT) for exploratory factor detection? What is the relation between EFA and mIRT? In fact, EFA is designed to work with continuous observed variables. And, in many cases, researchers use likert type scales to measure psychological constructs. So, that kind of discrete data may not be so suitable for EFA. However, Takane & Leeuw (1987) put forward that there is a relationship between IRT and EFA. That’s why the answer to the question is: Yes, IRT can be used for factor detection, both confirmatory and exploratory.\nToday, we will delve into the use of mIRT for exploratory factor analysis. The data we will use is the same as the one used in the previous post called “Exploratory Factor Analysis with Likert Scale Data”. So, you can check the preprocessing section of that post to see how we filtered the data.\nAlso, in the previous post, we have a detailed discussion about the number of the factors that can be extracted from this data. That’s why we are going to skip scree plots, K1, and parallel analysis in this post. I suggest seeing the previous post for those issues."
  },
  {
    "objectID": "posts/efaWithLikertData-2/index.html#introduction",
    "href": "posts/efaWithLikertData-2/index.html#introduction",
    "title": "Multidimensional IRT for Factor Exploration",
    "section": "",
    "text": "Can we use multidimensional IRT (mIRT) for exploratory factor detection? What is the relation between EFA and mIRT? In fact, EFA is designed to work with continuous observed variables. And, in many cases, researchers use likert type scales to measure psychological constructs. So, that kind of discrete data may not be so suitable for EFA. However, Takane & Leeuw (1987) put forward that there is a relationship between IRT and EFA. That’s why the answer to the question is: Yes, IRT can be used for factor detection, both confirmatory and exploratory.\nToday, we will delve into the use of mIRT for exploratory factor analysis. The data we will use is the same as the one used in the previous post called “Exploratory Factor Analysis with Likert Scale Data”. So, you can check the preprocessing section of that post to see how we filtered the data.\nAlso, in the previous post, we have a detailed discussion about the number of the factors that can be extracted from this data. That’s why we are going to skip scree plots, K1, and parallel analysis in this post. I suggest seeing the previous post for those issues."
  },
  {
    "objectID": "posts/efaWithLikertData-2/index.html#understand-the-data",
    "href": "posts/efaWithLikertData-2/index.html#understand-the-data",
    "title": "Multidimensional IRT for Factor Exploration",
    "section": "Understand the data",
    "text": "Understand the data\nThe following code is also provided in the previous post. It is used to load the data and filter the participants based on their demographics. Let’s just run the same code to get the data ready for the analysis. If you are coming from the previous post, this code is already run. So, you can skip this part.\n\nCodelibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nCodedata &lt;- read.delim(\"data.csv\")\n\ndf_with_demographics &lt;- data %&gt;% \n  filter(engnat == 1) %&gt;% \n  filter(hand == 1) %&gt;% \n  filter(age &gt;= 18 & age &lt;= 30) %&gt;% \n  filter(source == 1)\n\ndf &lt;- df_with_demographics %&gt;% select(1:44)\ndf &lt;- df %&gt;% select(-Q21, -Q43)\n\n\nJust to remember how the data is distributed, let’s see the summary and structure of the data. Remember that we have already discarded items 21 and 43 to avoid multicollinearity because they have high correlation with items 8 and 27 consecutively.\n\nsummary(df[ ,1:5]) # run summary(df) to see all items.\n\n       Q1              Q2             Q3          Q4              Q5       \n Min.   :0.000   Min.   :0.00   Min.   :0   Min.   :0.000   Min.   :0.000  \n 1st Qu.:1.000   1st Qu.:2.00   1st Qu.:1   1st Qu.:2.000   1st Qu.:1.000  \n Median :1.000   Median :5.00   Median :3   Median :4.000   Median :3.000  \n Mean   :2.023   Mean   :3.76   Mean   :3   Mean   :3.281   Mean   :2.972  \n 3rd Qu.:3.000   3rd Qu.:5.00   3rd Qu.:5   3rd Qu.:5.000   3rd Qu.:4.000  \n Max.   :5.000   Max.   :5.00   Max.   :5   Max.   :5.000   Max.   :5.000  \n\nstr(df)\n\n'data.frame':   26334 obs. of  42 variables:\n $ Q1 : int  1 1 1 1 1 1 1 1 1 1 ...\n $ Q2 : int  4 1 4 1 5 1 5 5 3 5 ...\n $ Q3 : int  1 4 5 5 1 1 5 1 5 5 ...\n $ Q4 : int  5 4 1 4 5 5 5 5 5 5 ...\n $ Q5 : int  2 5 4 5 1 5 5 1 2 4 ...\n $ Q6 : int  5 4 4 1 5 5 4 5 5 5 ...\n $ Q7 : int  1 5 5 5 1 3 1 3 1 2 ...\n $ Q8 : int  5 5 4 2 5 4 1 4 3 4 ...\n $ Q9 : int  1 4 5 5 1 5 3 1 2 3 ...\n $ Q10: int  4 2 1 3 5 4 2 3 2 1 ...\n $ Q11: int  1 1 3 2 1 5 2 3 1 1 ...\n $ Q12: int  5 4 1 1 5 2 1 4 1 5 ...\n $ Q13: int  5 4 2 1 1 3 1 1 1 1 ...\n $ Q14: int  5 2 5 2 5 2 5 2 1 4 ...\n $ Q15: int  1 4 5 5 1 5 5 3 2 5 ...\n $ Q16: int  1 2 1 1 5 1 5 1 5 5 ...\n $ Q17: int  1 5 1 5 1 2 2 1 1 4 ...\n $ Q18: int  5 2 4 3 1 3 5 5 3 3 ...\n $ Q19: int  1 2 1 3 1 1 5 1 4 2 ...\n $ Q20: int  5 3 4 4 5 1 4 5 1 5 ...\n $ Q22: int  5 4 1 4 5 2 3 4 5 5 ...\n $ Q23: int  4 2 5 2 1 3 4 3 4 3 ...\n $ Q24: int  5 4 1 1 5 1 3 5 2 2 ...\n $ Q25: int  1 5 4 2 1 3 4 1 1 1 ...\n $ Q26: int  5 2 3 2 5 1 5 1 4 5 ...\n $ Q27: int  1 4 4 5 1 5 5 1 1 5 ...\n $ Q28: int  5 4 1 3 5 1 4 4 1 4 ...\n $ Q29: int  1 1 1 1 1 3 5 1 5 1 ...\n $ Q30: int  5 1 2 2 5 1 3 5 5 4 ...\n $ Q31: int  1 5 1 4 1 3 1 1 5 4 ...\n $ Q32: int  5 4 1 5 5 1 1 5 1 4 ...\n $ Q33: int  1 5 3 3 1 2 4 1 2 3 ...\n $ Q34: int  5 5 4 4 5 1 3 1 5 5 ...\n $ Q35: int  1 4 5 2 1 5 5 1 5 5 ...\n $ Q36: int  3 2 3 1 5 1 1 4 3 4 ...\n $ Q37: int  1 1 5 5 1 4 4 1 5 3 ...\n $ Q38: int  5 4 4 1 5 1 1 3 5 5 ...\n $ Q39: int  1 2 2 1 1 5 5 1 5 5 ...\n $ Q40: int  4 3 4 3 5 1 5 2 3 5 ...\n $ Q41: int  1 4 4 5 1 4 5 1 5 4 ...\n $ Q42: int  5 1 5 2 5 1 2 4 1 2 ...\n $ Q44: int  5 3 4 1 5 1 5 5 5 4 ..."
  },
  {
    "objectID": "posts/efaWithLikertData-2/index.html#multidimensional-irt-model-building",
    "href": "posts/efaWithLikertData-2/index.html#multidimensional-irt-model-building",
    "title": "Multidimensional IRT for Factor Exploration",
    "section": "Multidimensional IRT Model Building",
    "text": "Multidimensional IRT Model Building\nInitial Model\nIn this post, we are going to build 2-factor model as all the previous analyses suggest that 2-factor model is the best fit for this data. The factors are defined as Masculinity and Femininity by the origibal authors of the scale.\n\n\n\n\n\n\nNote\n\n\n\nNote on Item Selection\nThe item selection processes for both mIRT and EFA are described in the literature strictly, although subjectivity within the boundaries of these definitions may still exist. In other words, although most decisions will be the same, there may be some differences among psychometricians in terms of selecting/discarding an item. One psychometrician might select an item while another might not. And, both might have their logical reasons. The reasoning behind selecting/discarding items might depend on both statistics and knowledge of construct that is being assessed. Thus, although I am not an expert in gender roles, I wanted this post to be a personal experiment to check if I can extract the same factors and items as the original authors. So, I avoided checking the items in the original scale developed by the authors. See the documentation of the scale for the original items here.\n\n\nLet’s build the model with the mirt package.\n\nlibrary(mirt)\nmodel_1 &lt;- mirt(df, 2, itemtype = 'graded', method = 'EM', verbose = FALSE)\nmodel_1\n\n\nCall:\nmirt(data = df, model = 2, itemtype = \"graded\", method = \"EM\", \n    verbose = FALSE)\n\nFull-information item factor analysis with 2 factor(s).\nConverged within 1e-04 tolerance after 236 EM iterations.\nmirt version: 1.42 \nM-step optimizer: BFGS \nEM acceleration: Ramsay \nNumber of rectangular quadrature: 31\nLatent density type: Gaussian \n\nLog-likelihood = -1590607\nEstimated parameters: 293 \nAIC = 3181800\nBIC = 3184196; SABIC = 3183265\nG2 (1e+10) = 2645146, p = 1\nRMSEA = 0, CFI = NaN, TLI = NaN\n\n\nThe `mirt() is a function built in the mirt package to build multidimensional IRT models. It takes a dataframe, number of factors, IRT model, method etc. In our model, we use the prominent Graded Response Model (GRM) that is suitable for Likert scale ordinal data. “EM”, which stands for “Expectation-Maximization”, is the default estimation method and suggested for models with less than 3 factors. The verbose = FALSE argument is used to suppress the output of the function after each iteration cycle.\nWith the coef() function of mirt package, we can get the item parameters such as discrimination and difficulty.\n\nitem_params &lt;- coef(model_1, simplify = TRUE)$items\nprint(head(item_params)) # run print(item_params) to see all items.\n\n           a1          a2       d1         d2         d3          d4\nQ1  0.5428696 -0.87427291 7.469361 -0.4101088 -0.9383750 -1.53030916\nQ2 -1.3926877  0.10494528 7.617294  1.9428222  1.4265050  0.99922695\nQ3  0.3767874 -1.84457406 8.098128  1.1101457  0.4213897 -0.18057288\nQ4 -1.2437652 -0.22953251 6.872875  1.8390821  0.9332445  0.06892971\nQ5  0.2059426 -0.98529256 6.896770  1.1428991  0.4112459 -0.30738685\nQ6 -0.3865773  0.04663095 6.864864  2.3255300  1.4584706  0.64351628\n            d5\nQ1 -2.80068782\nQ2  0.08154265\nQ3 -1.33340228\nQ4 -1.21175535\nQ5 -1.40389522\nQ6 -0.52337385\n\n\nIn the case of factor analysis with MIRT, discrimination parameters are referred as the slope of the item characteristic surface (ICS). a1 is the slope of the item on Factor 1, while a2 is the slope of the item on Factor 2. Positive or negative values indicate the direction and strength of the relationship between the item and the respective factor.\nd1, d2, d3, d4 and d5 are the difficulty parameters of the items. They are also called as the threshold parameters. They represent the intercepts of the item for each category in the likert scale. Thresholds should increase monotonically (d1 &gt; d2 &gt; d3 &gt; …) for well-functioning items. Non-monotonic thresholds suggest problems with item performance. To detect these items, we can use the following function. It is expected to print the index of problematic items.\n\ncheck_monotonic &lt;- function(data) {\n  d_columns &lt;- data[, grepl(\"^d\\\\d+$\", names(data))]\n  \n  non_monotonic &lt;- logical(nrow(d_columns))\n  \n  for (i in seq_len(nrow(d_columns))) {\n    non_monotonic[i] &lt;- any(diff(as.numeric(d_columns[i, ])) &lt; 0)\n  }\n  \n  which(non_monotonic)\n}\ncheck_monotonic(item_params)\n\ninteger(0)\n\n\nThe output is null, suggesting all the items have monotonic thresholds.\nThe summary() function provides us with the factor loadings and the communalities (h2) of the variables. The default rotation method is oblimin. Therfore, we provide rotate = \"none\" in the function. Below the factor loadings, we can see the explained variance by each factor. The first factor explains %17.6 of the variance, while the second factor explains %10.8. Also, the correlation between the factors seems to be zero. Oblique rotations (e.g., oblimin) allow factors to be correlated, providing these estimates directly. As we set the rotation to none, the correlation between the factors is not calculated. Yet, we will get the correlation scores after we apply a rotation.\n\nsummary(model_1, rotate = \"none\")\n\n\nUnrotated factor loadings: \n\n         F1        F2     h2\nQ1   0.2729 -0.439568 0.2677\nQ2  -0.6326  0.047666 0.4024\nQ3   0.1485 -0.726795 0.5503\nQ4  -0.5865 -0.108245 0.3558\nQ5   0.1041 -0.498283 0.2591\nQ6  -0.2214  0.026708 0.0497\nQ7   0.2366 -0.400577 0.2164\nQ8  -0.4228  0.126203 0.1947\nQ9   0.3049 -0.576922 0.4258\nQ10 -0.2790  0.051659 0.0805\nQ11  0.1865 -0.215908 0.0814\nQ12 -0.5260  0.171620 0.3061\nQ13  0.4317 -0.091842 0.1948\nQ14 -0.5978 -0.035233 0.3586\nQ15  0.0794 -0.610298 0.3788\nQ16 -0.6210  0.050615 0.3882\nQ17  0.1866 -0.542158 0.3288\nQ18 -0.3000 -0.082622 0.0968\nQ19  0.4003 -0.276375 0.2366\nQ20 -0.6201  0.163312 0.4112\nQ22 -0.5505  0.050788 0.3056\nQ23  0.0277 -0.341561 0.1174\nQ24 -0.4089  0.102879 0.1778\nQ25  0.3163 -0.396559 0.2573\nQ26 -0.5752 -0.007970 0.3310\nQ27  0.1348 -0.523084 0.2918\nQ28 -0.5349  0.021320 0.2866\nQ29  0.1100 -0.583130 0.3521\nQ30 -0.5607  0.042580 0.3162\nQ31  0.4048 -0.284227 0.2447\nQ32 -0.3573  0.095665 0.1368\nQ33  0.4155 -0.185081 0.2069\nQ34 -0.5225  0.054848 0.2760\nQ35  0.1540 -0.588756 0.3704\nQ36 -0.5778  0.047455 0.3361\nQ37  0.2797 -0.314347 0.1771\nQ38 -0.6441 -0.015461 0.4151\nQ39  0.1609 -0.744320 0.5799\nQ40 -0.5856 -0.025633 0.3436\nQ41  0.1618 -0.362005 0.1572\nQ42 -0.5156 -0.000675 0.2659\nQ44 -0.6280  0.000000 0.3943\n\nSS loadings:  7.399 4.525 \nProportion Var:  0.176 0.108 \n\nFactor correlations: \n\n   F1 F2\nF1  1   \nF2  0  1\n\n\nBefore we apply a rotation, we need to check communalities to be over 0.30 just as it is in EFA. Communalities do not change after rotation, so we can discard the items with communalities below 0.30 at this stage. Here is the code to print the communalities below 0.3:\n\ncommunalities &lt;- data.frame(value = model_1@Fit[[\"h2\"]])\nlow_communalities &lt;- communalities[communalities$value &lt; 0.3, , drop = FALSE]\n\n# print ordered by communality value:\nprint(low_communalities[order(low_communalities$value), , drop = FALSE])\n\n         value\nQ6  0.04973608\nQ10 0.08049372\nQ11 0.08140447\nQ18 0.09683406\nQ23 0.11743292\nQ32 0.13679041\nQ41 0.15723452\nQ37 0.17705607\nQ24 0.17778655\nQ8  0.19465828\nQ13 0.19482647\nQ33 0.20685832\nQ7  0.21643984\nQ19 0.23662033\nQ31 0.24466433\nQ25 0.25733267\nQ5  0.25913279\nQ42 0.26588832\nQ1  0.26771912\nQ34 0.27603944\nQ28 0.28656914\nQ27 0.29178386\n\n\nFinal Model\nItems with low communality values can be discarded from the dataset. To do that, run the following code:\n\ndiscard_these_items &lt;- rownames(low_communalities)\nprint(discard_these_items)\n\n [1] \"Q1\"  \"Q5\"  \"Q6\"  \"Q7\"  \"Q8\"  \"Q10\" \"Q11\" \"Q13\" \"Q18\" \"Q19\" \"Q23\" \"Q24\"\n[13] \"Q25\" \"Q27\" \"Q28\" \"Q31\" \"Q32\" \"Q33\" \"Q34\" \"Q37\" \"Q41\" \"Q42\"\n\ndf_final &lt;- df %&gt;% select(-discard_these_items)\n\nNow, using the df_final, we can build our final solution:\n\nmodel_2 &lt;- mirt(df_final, 2, itemtype = 'graded', method = 'EM', verbose = FALSE)\nmodel_2\n\n\nCall:\nmirt(data = df_final, model = 2, itemtype = \"graded\", method = \"EM\", \n    verbose = FALSE)\n\nFull-information item factor analysis with 2 factor(s).\nConverged within 1e-04 tolerance after 184 EM iterations.\nmirt version: 1.42 \nM-step optimizer: BFGS \nEM acceleration: Ramsay \nNumber of rectangular quadrature: 31\nLatent density type: Gaussian \n\nLog-likelihood = -752201.4\nEstimated parameters: 139 \nAIC = 1504681\nBIC = 1505818; SABIC = 1505376\nG2 (1e+10) = 968649.2, p = 1\nRMSEA = 0, CFI = NaN, TLI = NaN\n\n\nLet’s see the factor loadings for the final solution. If we need to, we can apply a rotation later on:\n\nsummary(model_2, rotate = \"none\")\n\n\nUnrotated factor loadings: \n\n        F1      F2    h2\nQ2   0.605  0.0369 0.368\nQ3  -0.148 -0.7520 0.587\nQ4   0.575 -0.1099 0.343\nQ9  -0.297 -0.5812 0.426\nQ12  0.511  0.1587 0.286\nQ14  0.634 -0.0172 0.402\nQ15 -0.078 -0.5531 0.312\nQ16  0.624  0.0593 0.393\nQ17 -0.183 -0.5115 0.295\nQ20  0.596  0.1708 0.385\nQ22  0.541  0.0636 0.297\nQ26  0.593  0.0153 0.351\nQ29 -0.109 -0.6068 0.380\nQ30  0.542  0.0504 0.296\nQ35 -0.145 -0.5846 0.363\nQ36  0.577  0.0392 0.334\nQ38  0.651 -0.0059 0.424\nQ39 -0.159 -0.7813 0.636\nQ40  0.630 -0.0200 0.397\nQ44  0.611  0.0000 0.374\n\nSS loadings:  4.777 2.872 \nProportion Var:  0.239 0.144 \n\nFactor correlations: \n\n   F1 F2\nF1  1   \nF2  0  1\n\n\nThe factor loadings and communalities of the final solution looks fine. The explained variance by each factor is %23.9 and %14.4, respectively. This suggests an increase in the explained variance by the factors when compared to the initial model. Now, we can apply a rotation to see the correlation of the factors.\n\nsummary(model_2, rotate = \"oblimin\")\n\n\nRotation:  oblimin \n\nRotated factor loadings: \n\n         F1       F2    h2\nQ2   0.6020 -0.01689 0.368\nQ3   0.0252  0.77249 0.587\nQ4   0.6058  0.13388 0.343\nQ9  -0.1646  0.59058 0.426\nQ12  0.4781 -0.14616 0.286\nQ14  0.6431  0.04007 0.402\nQ15  0.0496  0.56925 0.312\nQ16  0.6158 -0.03944 0.393\nQ17 -0.0663  0.52252 0.295\nQ20  0.5618 -0.15568 0.385\nQ22  0.5310 -0.04682 0.297\nQ26  0.5941  0.00498 0.351\nQ29  0.0308  0.62368 0.380\nQ30  0.5345 -0.03308 0.296\nQ35 -0.0107  0.59951 0.363\nQ36  0.5724 -0.02029 0.334\nQ38  0.6576  0.02896 0.424\nQ39  0.0204  0.80238 0.636\nQ40  0.6397  0.04283 0.397\nQ44  0.6164  0.02147 0.374\n\nRotated SS loadings:  4.572 3.009 \n\nFactor correlations: \n\n       F1 F2\nF1  1.000   \nF2 -0.257  1\n\n\nThe correlation between the factors is -0.26 after the rotation. This is a good result as it is not too high to suggest a single factor solution, nor too low to suggest a completely independent factor solution.\nModel Fit\nTo see model’s fit indices, we can use M2() of mirt:\n\nM2(model_2)\n\n            M2 df p      RMSEA    RMSEA_5   RMSEA_95      SRMSR      TLI\nstats 5090.389 71 0 0.05181388 0.05061086 0.05302509 0.03886059 0.914146\n            CFI\nstats 0.9445851\n\n\nThe model fit indices are suggesting a good model fit. RMSEA and SRMSR values are below 0.08 and TLI and CFI are above 0.90. These values are considered as good fit indices for a model.\nVisualization\nThe MIRT package provides with 3D plotting options. We can use the plot() and itemplot() functions to see the 3D plots of the model. We can also implement rotation to see the rotated model’s plot.\nExpected Total Score Plot\n\nplot(model_2)\n\n\n\n\n\n\nplot(model_2, rotate = \"oblimin\")\n\n\n\n\n\n\n\nItem Trace Plots\nWe can also check for item trace plots. Let’s see the first item’s trace plot for final model without rotation and with rotation:\n\nitemplot(model_2, 1)\n\n\n\n\n\n\nitemplot(model_2, 1, rotate = \"oblimin\")\n\n\n\n\n\n\n\nTest Information\nWe can also draw a plot to see the test information. Test information is a measure of the precision of the test at different levels of the latent traits. In multidimensional models we need to produce a contour plot.\nWe can plot the test information with the following code:\n\nplot(model_2, rotate = \"oblimin\" ,type=\"infocontour\") \n\n\n\n\n\n\n\nThe test is most effective (provides the highest precision) in the central regions where test information is high (contour values like 6 or 7). Meaning the test is most effective for around 0 thetas of both factors. At the edges of the plot, the test is less precise (values like 1 or 2), meaning the test is less effective at distinguishing individuals with very high or very low abilities in those dimensions.\nSelected Items and Defining Factors\nWe are solid that our data has 2 factors, with each variable loading to a single factor. These factors were named as Femininity and Masculinity in the original scale.\nThese are the factors and their variables.\n\n\n\n\n\n\nFemininity\nMasculinity\n\n\n\nQ2 I have thought about dying my hair.\nQ3 I have thrown knives, axes or other sharp things.\n\n\nQ4 I give people handmade gifts.\nQ9 I like guns.\n\n\nQ12 I use lotion on my hands.\nQ15 I have thought it would be exciting to be an outlaw.\n\n\nQ14 I dance when I am alone.\nQ17 I have considered joining the military.\n\n\nQ16 When I was a child, I put on fake concerts and plays with my friends.\nQ29 I have burned things up with a magnifying glass.\n\n\nQ20 I sometimes feel like crying when I get angry.\nQ35 I have taken apart machines just to see how they work.\n\n\nQ22 I save the letters I get.\nQ39 I have set fuels, aerosols or other chemicals on fire, just for fun.\n\n\nQ26 I jump up and down in excitement sometimes.\n\n\n\nQ30 I think horoscopes are fun.\n\n\n\nQ36 I take lots of pictures of my activities.\n\n\n\nQ38 I leave nice notes for people now and then.\n\n\n\nQ40 I really like dancing.\n\n\n\nQ44 I decorate my things (e.g. stickers on laptop).\n\n\n\nFactor scores of persons\nFinally, we can extract the factor scores of the persons. These scores can be used in further analyses such as regression, clustering, etc.\n\nhead(fscores(model_2)) ## run fscores(model_2) to see all scores.\n\n             F1         F2\n[1,]  1.9183653 -2.0882425\n[2,] -0.5932079  0.4430873\n[3,] -0.3261666  0.7950392\n[4,] -1.4358888  0.6969259\n[5,]  3.1571670 -2.2801150\n[6,] -2.3165436  1.3256648\n\n\nAs we did in EFA in the previous post, we can use these factor scores in a scree plot with genders of the participants. Let’s see the plot:\n\nlibrary(ggplot2)\nmirt_scores &lt;- fscores(model_2)\ncolnames(mirt_scores) &lt;- c(\"Femininity\", \"Masculinity\")\nscores &lt;- as_tibble(mirt_scores)\nscores &lt;- bind_cols(df_with_demographics |&gt; select(gender), scores) |&gt;\n  filter(gender %in% c(1, 2)) |&gt;  \n  mutate(gender = factor(gender, labels = c(\"Male\", \"Female\")))  \n\n# Plot the filtered data\nscores |&gt;\n  ggplot(aes(Femininity, Masculinity, color = gender)) +\n  geom_point() +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  labs(color = \"Gender\")\n\n\n\n\n\n\n\nIn my opinion this plot’s being parallelogram-like is interesting. The same plot in EFA was more like a rectangle, although both distributions are similar.\nComparison with EFA Model\nIt is also a good idea to check the correlation between the factor scores obtained from mIRT and EFA.\nLet’s build the EFA model again:\n\nlibrary(psych)\ndf_EFA &lt;- df %&gt;% select(\n  -Q6, -Q11, -Q10, -Q18, -Q23, -Q32, -Q37, -Q41, \n  -Q24, -Q8, -Q13, -Q33, -Q7, -Q19, -Q31, -Q42, \n  -Q25, -Q34, -Q28, -Q5, -Q1, -Q12, -Q27, -Q30, -Q22\n  )\n\n\npoly_matrix &lt;- polychoric(df_EFA)$rho\n\ntwo_fm &lt;- fa(poly_matrix, nfactors = 2, fm = \"pa\", rotate = \"oblimin\", cor = \"poly\", SMC=FALSE)\n\nRemember that although the items selected by EFA and IRT are very similar, they are not identical. We can compare the items selected by both methods:\n\nmirt_items&lt;-colnames(df_final)\nefa_items&lt;-colnames(df_EFA)\nall_items &lt;- unique(c(mirt_items, efa_items)) \n\ncomparison &lt;- data.frame(\n  mirt_items = ifelse(all_items %in% mirt_items, all_items, \"\"),\n  efa_items = ifelse(all_items %in% efa_items, all_items, \"\")\n)\n\nprint(comparison)\n\n   mirt_items efa_items\n1          Q2        Q2\n2          Q3        Q3\n3          Q4        Q4\n4          Q9        Q9\n5         Q12          \n6         Q14       Q14\n7         Q15       Q15\n8         Q16       Q16\n9         Q17       Q17\n10        Q20       Q20\n11        Q22          \n12        Q26       Q26\n13        Q29       Q29\n14        Q30          \n15        Q35       Q35\n16        Q36       Q36\n17        Q38       Q38\n18        Q39       Q39\n19        Q40       Q40\n20        Q44       Q44\n\n\nSo items 12, 22, and 30 are selected by IRT model but they are not selected by EFA model.\nNow let’s have a look at the correlation between factor scores obtained from mIRT and EFA models:\n\nefa_scores  &lt;- factor.scores(df_EFA, two_fm)$scores\ncolnames(efa_scores) &lt;- c(\"Femininity_efa\", \"Masculinity_efa\")\ncor(mirt_scores, efa_scores)\n\n            Femininity_efa Masculinity_efa\nFemininity       0.9741479      -0.3107335\nMasculinity     -0.3301980       0.9873525\n\n\nThe participants’ scores obtained from MIRT and EFA models are highly correlated. The correlation between the femininity scores is 0.98, while the correlation between the masculinity scores is 0.99. This suggests that the factor scores obtained from both models are very similar."
  },
  {
    "objectID": "posts/efaWithLikertData-2/index.html#conclusion",
    "href": "posts/efaWithLikertData-2/index.html#conclusion",
    "title": "Multidimensional IRT for Factor Exploration",
    "section": "Conclusion",
    "text": "Conclusion\nIn this post, we worked on how to run multidimensional Item Response Theory based exploratory factor analysis on likert scale data. We used the mirt package in R to build the model. We also compared the factor scores obtained from the mIRT model with the factor scores obtained from the EFA model, which was discussed in my previous post. The results suggest that the factor scores from both models are highly correlated. Also, the mIRT model has shown great fit, even better than EFA model. These findings supports the opinion that as EFA is designed to work on continuous data, mIRT is a good alternative for factor detection in likert scale data.\nComparison with the Original Scale\nBoth my EFA and mIRT models have similar items to the original scale. However, the scales are not identical. The original scale contains 10 items for each factor. The difference occurs because of the variation in our approaches in two ways:\n\nI discarded two items because they are highly correlated with other two items, pointing to multicollinearity issues. It wasn’t done on the original scale. This might be because the authors might not have this issue in their original data. This difference probably resulted in the alternation in the item selection processes.\nThe preferred rotation method is different in the original scale. Although this shouldn’t make an effect on item selection for the factors, the participants’ factor scores might slightly alter. I used oblimin rotation while the original authors used varimax. The varimax is usually preferred when factors are independent of each other (no correlation between factors). The aim of varimax rotation is to maximize the variance of the squared loadings. Thus, it creates a simpler structure and encourages each variable to load strongly on one factor and weakly on others. Assuming the factors (Femininity and Masculinity) are independent of each other, the original authors might have used varimax rotation. In my personal opinion, these two factors could be considered as correlated negatively. That’s why I used oblimin rotation. Yet, as I mentioned before, I am not an expert on gender roles. So, I might be wrong in this assumption. In a real case study, I would definitely consult with an expert in the field to decide on the rotation method."
  },
  {
    "objectID": "posts/efaWithLikertData-2/index.html#further-remarks",
    "href": "posts/efaWithLikertData-2/index.html#further-remarks",
    "title": "Multidimensional IRT for Factor Exploration",
    "section": "Further Remarks",
    "text": "Further Remarks\n\nRunning Confirmatory Factor Analysis (CFA) and mIRT for confirmatory factor detection is planned for a future post."
  },
  {
    "objectID": "posts/Automated Essay Scoring (AES) with R/index.html",
    "href": "posts/Automated Essay Scoring (AES) with R/index.html",
    "title": "Automated Essay Scoring (AES) with R",
    "section": "",
    "text": "In the realm of education and standardized testing, technological advancements have brought forth a significant transformation. Among the noteworthy innovations in this field is the adoption of Automated Essay Scoring (AES), an approach that leverages the capabilities of natural language processing (NLP) and machine learning. AES holds the potential to redefine the essay evaluation and grading process, offering efficiency, consistency, and accessibility in a way previously unattainable.\nAt its essence, AES relies on sophisticated algorithms to examine written text, subjecting it to a predefined set of criteria. These criteria encompass various aspects, including grammar, sentence structure, vocabulary, coherence, and the quality of argumentation. In this regard, AES algorithms function akin to meticulous digital assessors, diligently seeking out patterns and features within the text. They assess elements such as the presence of persuasive thesis statements, the adept use of supporting evidence, and the logical flow of ideas within the essay. The result of this is a numerical score that reflects the overall quality of the essay in question.\nToday we will develop a linear regression model to predict essay scores using the famous ASAP dataset. There are eight different essay types in this dataset to explore. Here we will use the essay set 2. Before you continue, please go and read the description and the details on Kaggle. As we will use only one essay set, I have preapared the data as a separate csv file and if you are already familiar with the ASAP data (or you have taken a look at the Kaggle page), please download the data for our use case from here. The packages and some of their functions that we will be using in this topic are:\n\n\n\nRequirements\n\nlibrary(\"rmarkdown\") \n# paged_table() \nlibrary(\"readxl\") \n# read_excel()\nlibrary(\"tidyverse\") \nlibrary(\"textstem\") \n# lemmatize_strings()\nlibrary(\"stopwords\") \n# stopwords()\nlibrary(\"stringr\")   \n# str_squish()\nlibrary(\"tm\")        \n# remove_words()\nlibrary(\"quanteda\")  \n# nsentence()"
  },
  {
    "objectID": "posts/Automated Essay Scoring (AES) with R/index.html#introduction",
    "href": "posts/Automated Essay Scoring (AES) with R/index.html#introduction",
    "title": "Automated Essay Scoring (AES) with R",
    "section": "",
    "text": "In the realm of education and standardized testing, technological advancements have brought forth a significant transformation. Among the noteworthy innovations in this field is the adoption of Automated Essay Scoring (AES), an approach that leverages the capabilities of natural language processing (NLP) and machine learning. AES holds the potential to redefine the essay evaluation and grading process, offering efficiency, consistency, and accessibility in a way previously unattainable.\nAt its essence, AES relies on sophisticated algorithms to examine written text, subjecting it to a predefined set of criteria. These criteria encompass various aspects, including grammar, sentence structure, vocabulary, coherence, and the quality of argumentation. In this regard, AES algorithms function akin to meticulous digital assessors, diligently seeking out patterns and features within the text. They assess elements such as the presence of persuasive thesis statements, the adept use of supporting evidence, and the logical flow of ideas within the essay. The result of this is a numerical score that reflects the overall quality of the essay in question.\nToday we will develop a linear regression model to predict essay scores using the famous ASAP dataset. There are eight different essay types in this dataset to explore. Here we will use the essay set 2. Before you continue, please go and read the description and the details on Kaggle. As we will use only one essay set, I have preapared the data as a separate csv file and if you are already familiar with the ASAP data (or you have taken a look at the Kaggle page), please download the data for our use case from here. The packages and some of their functions that we will be using in this topic are:\n\n\n\nRequirements\n\nlibrary(\"rmarkdown\") \n# paged_table() \nlibrary(\"readxl\") \n# read_excel()\nlibrary(\"tidyverse\") \nlibrary(\"textstem\") \n# lemmatize_strings()\nlibrary(\"stopwords\") \n# stopwords()\nlibrary(\"stringr\")   \n# str_squish()\nlibrary(\"tm\")        \n# remove_words()\nlibrary(\"quanteda\")  \n# nsentence()"
  },
  {
    "objectID": "posts/Automated Essay Scoring (AES) with R/index.html#preprocessing",
    "href": "posts/Automated Essay Scoring (AES) with R/index.html#preprocessing",
    "title": "Automated Essay Scoring (AES) with R",
    "section": "1. Preprocessing",
    "text": "1. Preprocessing\nNow that you have downloaded the data, move it to your working directory and follow along while reading. Let’s load the data and see the head of them:\n\n\n\n\n\nessay_set_2 &lt;- read_excel(\"essay_set_2.xlsx\") \npaged_table(head(essay_set_2, 2)) # paged_table() function is for a beautified table view on this page. You don't need to use it on your own trial.\n\n\n\n  \n\n\n\nThe nine columns are:\n\nessay_id: a unique identifier for the record.\nessay_set: That is set to 2 for all records in this dataset as we are working on essay set 2. We will get rid of this soon.\nessay: The text of the essay written by a real person.\nThere are also 6 more columns that include rater1_domain1, rater2_domain1, domain1_score, rater1_domain2, rater2_domain2, and domain2_score.\n\nLets see a sample essay which is randomly selected:\n\n\n\n\n\nset.seed(1234)\nrandom_essay &lt;- sample(1:length(essay_set_2), 1)\npaged_table(essay_set_2[random_essay,3])\n\n\n\n  \n\n\n\nWe will use the essay and rater1_domain1 columns for our analysis as we are just doing a research for demonstration purposes (of course, when you are working on a real life research case, there are many things that you have to take into consideration). The essay column is the text of the essay written by a real person and the rater1_domain1 column is the score given to the essay by the first rater. We will rename these columns as response and score respectively. We will also create a new column called doc_id which will be a unique identifier for each essay in our case study.\n\n\n\n\n\nset &lt;- essay_set_2 %&gt;% \n  select(essay, rater1_domain1) %&gt;% \n  rename(response = essay, score = rater1_domain1)\nset$doc_id &lt;- paste0(\"doc\", 1:nrow(set))\n\n\nWhen you scrutinize the essays, you will see that there are some @words in the text. These are the words that are replaced with private information about the students such as names, places or dates etc. Also, there are some non-alphabetic characters and capital letters. In order to make the essays ready to analysis, we will convert all the text to lowercase and remove the non-alphabetic characters. We will also remove the stopwords and lemmatize the text.\n\n\n\n\n\nset$processedResponse &lt;- gsub(\"@\\\\w+ *\", \"\", set$response) #remove @words\nset$processedResponse &lt;- gsub(\"[^a-zA-Z]\", \" \", set$processedResponse) #remove non-alphabetic characters\nset$processedResponse &lt;- tolower(set$processedResponse) #convert to lowercase\nset$processedResponse &lt;- lemmatize_strings(set$processedResponse) #lemmatize\nen_stopwords &lt;- stopwords::stopwords(\"en\", source = \"stopwords-iso\") #get stopwords\nset$processedResponse &lt;- removeWords(set$processedResponse, words = en_stopwords) #remove stopwords\nset$processedResponse &lt;- str_squish(set$processedResponse) #remove extra whitespaces\n\n\nHere how the new processed text looks like (the one we have seen above):\n\n\n\n\n\nsample_essay&lt;-merge(set[random_essay,4], set[random_essay,1])\npaged_table(sample_essay)"
  },
  {
    "objectID": "posts/Automated Essay Scoring (AES) with R/index.html#feature-extraction",
    "href": "posts/Automated Essay Scoring (AES) with R/index.html#feature-extraction",
    "title": "Automated Essay Scoring (AES) with R",
    "section": "2. Feature Extraction",
    "text": "2. Feature Extraction\nFeature extraction is a critical component of Automated Essay Scoring (AES), serving as the foundation upon which the system’s evaluation process is built. In AES, feature extraction procedures involve the identification and analysis of various linguistic and structural elements within the essay. These elements may include but are not limited to word frequency, sentence length, syntactic complexity, vocabulary richness, and the presence of specific content-related markers like thesis statements or evidence citations. The goal of feature extraction is to distill the complex nature of written language into a set of quantifiable, machine-readable attributes that can be used by the AES algorithms to assess the quality and coherence of the essay. Through a combination of natural language processing and statistical analysis, feature extraction empowers AES systems to objectively evaluate essays, providing educators and test administrators with consistent, efficient, and data-driven grading outcomes.\nFor the sake of simplicity of demonstration, we will extract the following features: number of sentences, number of paragraphs and number of contextual words.\n\n\n\n\n\nset$n_sentence &lt;- nsentence(set$response) \nset$n_paragraph &lt;- str_count(set$response, \"     \") + 1 \nset$n_contextWords &lt;- lengths(strsplit(set$processedResponse, ' '))\nhead(set)\n\n\n# A tibble: 6 × 7\n  response  score doc_id processedResponse n_sentence n_paragraph n_contextWords\n  &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;                  &lt;int&gt;       &lt;dbl&gt;          &lt;int&gt;\n1 Certain …     4 doc1   material remove …         20           4            126\n2 Write a …     1 doc2   write persuasive…          3           4             33\n3 Do you t…     2 doc3   library remove m…         15           3             59\n4 In @DATE…     4 doc4   offensive opinio…         31           5            111\n5 In life …     4 doc5   life offensive s…         35          11            131\n6 A lot of…     4 doc6   lot censor conte…         25           5             79\n\n\nAs stated before, there might be many other features that can be extracted from the essays. We will continue our study with these 3 features even if they are not enough for a real life research.\nBefore we start building a machine learning model, we need to divide our dataset as training and test data.\n\n\n\n\n\ndataset &lt;- set\ntrain_indices &lt;- sample(nrow(dataset), nrow(dataset) * 0.7)  # 70% for training\ntrain_data &lt;- dataset[train_indices, ]\ntest_data &lt;- dataset[-train_indices, ]"
  },
  {
    "objectID": "posts/Automated Essay Scoring (AES) with R/index.html#linear-regression-model-lr",
    "href": "posts/Automated Essay Scoring (AES) with R/index.html#linear-regression-model-lr",
    "title": "Automated Essay Scoring (AES) with R",
    "section": "3. Linear Regression Model (LR)",
    "text": "3. Linear Regression Model (LR)\nWhat is Linear Regression?\nLinear regression is a statistical model that examines the linear relationship between two (Simple Linear Regression ) or more (Multiple Linear Regression) variables — a dependent variable and independent variable(s). Linear relationship basically means that when one (or more) independent variables increases (or decreases), the dependent variable increases (or decreases) too. Linear regression models are used to show or predict the relationship between two variables or factors. The factor that is being predicted (the factor that the equation solves for) is called the dependent variable. The factors that are used to predict the value of the dependent variable are called the independent variables. The results of a LR model can be evaluated using statistical metrics such as Mean Squared Error (MSE), Root Mean Squared Error (RMSE) and R-squared.\nIn our case, the dependent variable is the score and the independent variables are n_sentence, n_paragraph and n_contextWords.\n\n\n\nLinear Model 1\n\nformula &lt;- as.formula(\"score ~ n_sentence + n_paragraph + n_contextWords\")\nmodel_1 &lt;- lm(formula, data = train_data)\n# Print the summary of the model\nsummary(model_1)\n\n\n\nCall:\nlm(formula = formula, data = train_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.52359 -0.36412  0.00356  0.40190  1.93435 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    2.1864591  0.0418320  52.268  &lt; 2e-16 ***\nn_sentence     0.0224310  0.0027105   8.276 3.23e-16 ***\nn_paragraph    0.0023062  0.0034157   0.675      0.5    \nn_contextWords 0.0076319  0.0005822  13.109  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5595 on 1256 degrees of freedom\nMultiple R-squared:  0.4701,    Adjusted R-squared:  0.4688 \nF-statistic: 371.4 on 3 and 1256 DF,  p-value: &lt; 2.2e-16\n\n\nPerformance of the Model 1\nLet’s investigate the output from the model one by one.\n\n\nResiduals: This section provides statistics about the residuals, which are the differences between the observed values and the predicted values by the model.\n\nMin: The minimum residual value is -2.51929.\n1Q: The first quartile (25th percentile) of the residuals is -0.34048.\nMedian: The median of the residuals is 0.00576.\n3Q: The third quartile (75th percentile) of the residuals is 0.37803.\nMax: The maximum residual value is 1.95369.\n\n\nCoefficients: This section provides information about the coefficients of the linear regression model. Each row corresponds to a predictor variable (independent variable) in the model.\nEstimate: This is the estimated coefficient for each predictor. Std. Error: It represents the standard error of the coefficient estimate. t value: The t-value is a measure of how many standard errors the coefficient estimate is away from zero. Pr(&gt;|t|): This is the p-value associated with the t-value, which tells you whether the coefficient is statistically significant. In our model, we have three predictor variables: n_sentence, n_paragraph, and n_contextWords. The Estimate column represents the estimated coefficients for each of these predictors. The *** symbols indicate that these coefficients are highly statistically significant. In other words, the number of paragraph is not significantly valuable for the model while the number of sentences and the number of contextual words are highly significant.\nMultiple R-squared: This is a measure of how well the model fits the data. It tells you the proportion of the variance in the dependent variable that is explained by the independent variables. In our case, R-squared is approximately 0.4737, which means that about 47.37% of the variance in the dependent variable is explained by our predictors.\nAdjusted R-squared: This is a version of R-squared that adjusts for the number of predictors in the model. It is a slightly more conservative measure of goodness of fit.\nF-statistic: This is a measure of the overall significance of the model. It tests whether at least one of the predictor variables is significantly related to the dependent variable. A high F-statistic and a very low p-value (which is the case here) indicate that the model is significant.\np-value: The p-value associated with the F-statistic. In this case, it’s extremely small, indicating that the overall model is highly significant.\nComparison of Model 1 and Model 2\nLet’s build another model without the feature n_paragraph and compare the results.\n\n\n\nLinear Model 2\n\nformula_2 &lt;- as.formula(\"score ~ n_sentence + n_contextWords\")\nmodel_2 &lt;- lm(formula_2, data = train_data)\n# Print the summary of the model\nsummary(model_2)\n\n\n\nCall:\nlm(formula = formula_2, data = train_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.51924 -0.36470  0.00476  0.39940  1.93587 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    2.195033   0.039849  55.083   &lt;2e-16 ***\nn_sentence     0.022599   0.002698   8.375   &lt;2e-16 ***\nn_contextWords 0.007637   0.000582  13.123   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5594 on 1257 degrees of freedom\nMultiple R-squared:  0.4699,    Adjusted R-squared:  0.4691 \nF-statistic: 557.1 on 2 and 1257 DF,  p-value: &lt; 2.2e-16\n\n\nResidual standard error: The residual standard error is very close in both models, indicating that they have similar predictive accuracy. Multiple R-squared: The R-squared values are also very close. Model 2 has a slightly lower R-squared, but the difference is minimal. F-statistic: Model 2 has a higher F-statistic compared to Model 1, indicating that the predictors collectively have more explanatory power in Model 2.\nIn summary, both models are quite similar in terms of their coefficient estimates and predictive accuracy. Model 2 has a slightly higher F-statistic. Personally I agree with the model. The number of paragraphs should not be a predictor while estimating the score. The number of sentences and the number of contextual words are much more important.\nEvaluation of Model 2 with the Test Dataset\nAs we would like to continue with the second model, we can use the predict() to make predictions on the test data. The predict() function takes the model and the new data set as arguments. It returns a vector of predictions, which we will save in a new column in the test data set to make comparisons in the next stage. Now, let’s use the mean() and sqrt() functions to calculate the MSE and RMSE of the model. We can also use the summary() function to get the R-squared value of the model.\n\n\n\n\n\n# Make predictions on the test data\npredictions &lt;- predict(model_2, newdata = test_data)\n# Evaluate the model\nmse &lt;- mean((test_data$score - predictions)^2)\nrmse &lt;- sqrt(mse)\nr_squared &lt;- summary(model_2)$r.squared\n# Printing evaluation metrics beautifully :D\ncat(paste0(\"Mean Squared Error (MSE): \", sprintf(\"%.2f\", mse), \"\\n\",\n            \"Root Mean Squared Error (RMSE): \", sprintf(\"%.2f\", rmse), \"\\n\",\n            \"R-squared (R²): \", sprintf(\"%.2f\", r_squared), \"\\n\"))\n\n\nMean Squared Error (MSE): 0.30\nRoot Mean Squared Error (RMSE): 0.54\nR-squared (R²): 0.47\n\n\n\nMSE (Mean Squared Error): MSE is a measure of the average squared difference between the observed (actual) values and the predicted values by the model. It’s a measure of the model’s accuracy, with lower values indicating a better fit.\nRMSE (Root Mean Squared Error): RMSE is the square root of the MSE and provides a measure of the average error in the same units as the dependent variable. It’s a commonly used metric to quantify the prediction error of the model.\nR-squared (R²): R-squared is a measure of how well the independent variables explain the variability in the dependent variable. It ranges from 0 to 1, with higher values indicating a better fit. In the output, the R-squared is approximately 0.4736732, which means that about 47.37% of the variance in the dependent variable is explained by the independent variables in the model.\nPredictions of the Test Dataset\nLet’s take a look at the predictions of the test data. We can use the head() function to print the first 10 predictions.\n\n\n\n\n\n#merge predictions to test data:\ntest_data$predictions &lt;- round(predictions)\nhead(test_data[,c(\"score\",\"predictions\")], 10)\n\n\n# A tibble: 10 × 2\n   score predictions\n   &lt;dbl&gt;       &lt;dbl&gt;\n 1     4           4\n 2     1           3\n 3     2           3\n 4     4           4\n 5     5           4\n 6     2           4\n 7     4           3\n 8     3           3\n 9     3           3\n10     3           2\n\n\nBut looking at the raw data would never be enough to get some insights. Let’s also print the confusion matrix to get a better overall picture. The confusion matrix is a table that is often used to describe the performance of a classification model on a set of test data for which the true values are known. It allows the visualization of the performance of an algorithm. The confusion matrix shows the ways in which the model is confused when it makes predictions. It gives us insight not only into the errors being made by a classifier but more importantly the types of errors that are being made.\n\n\n\n\n\n# Create the confusion matrix and print it\nconfusion_matrix &lt;- table(test_data$predictions, test_data$score)\nconfusion_matrix\n\n\n   \n      1   2   3   4   5   6\n  2   7  13   1   0   0   0\n  3   2  35 186  85   1   0\n  4   0   1  30 148  14   0\n  5   0   0   0   7   8   0\n  6   0   0   0   0   1   1\n\n\nIn this context, the confusion matrix helps us understand the model’s performance in grading essays. It indicates which score points are frequently confused with others, providing insights into where the model might need improvement. For instance, score points 3 and 4 appear to be frequently confused with each other. Yet, I believe that is not a big deal. Confusing a high score such as 6 with a lower score such as 2 would be a catastrophe in AES context, but here we don’t even have one such a case. Besides confusion matrix, one can calculate performance metrics such as accuracy, precision, recall, and F1-score to get a more comprehensive assessment of the model’s grading performance."
  },
  {
    "objectID": "posts/Automated Essay Scoring (AES) with R/index.html#conclusion",
    "href": "posts/Automated Essay Scoring (AES) with R/index.html#conclusion",
    "title": "Automated Essay Scoring (AES) with R",
    "section": "Conclusion",
    "text": "Conclusion\nOur exploration into AES has shed light on the impressive potential of this technology in revolutionizing the evaluation of written content. Our application of a Linear Regression model, even when considering a limited set of extracted features such as the number of words, sentences, and paragraphs, demonstrated the robustness of this approach. It is evident that by harnessing machine learning algorithms, we can achieve consistent and objective grading, streamlining the assessment process for educators and administrators.\nHowever, it’s important to note that Linear Regression is just one of the many models available for AES. The field of automated essay scoring continues to evolve, and researchers are exploring a range of models and techniques to enhance accuracy and broaden the scope of assessment. Some alternatives to LR include Support Vector Machines (SVM), Random Forests, Neural Networks, and Natural Language Processing models like Recurrent Neural Networks (RNNs) or Transformers, such as BERT and GPT-3. These models bring their unique strengths and capabilities to the table, offering a diverse array of tools for essay evaluation.\nAs AES advances, the synergy of these models with increasingly sophisticated feature extraction procedures promises to further elevate the quality and reliability of automated essay scoring. With this, we look to a future where technology and human expertise collaborate seamlessly to offer more efficient, accurate, and comprehensive assessment in the realm of written expression."
  },
  {
    "objectID": "about-me.html",
    "href": "about-me.html",
    "title": " ",
    "section": "",
    "text": "About"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Psychometrics",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nMultidimensional IRT for Factor Exploration\n\n\n20 min\n\n\n\ndplyr\n\n\nggplot2\n\n\nmirt\n\n\npsych\n\n\n\nThis post focuses on multidimensioanl IRT (mIRT) as an exploratory factor analysis method on likert scale data. It is a continuation of the previous post, which was about…\n\n\n\nNov 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploratory Factor Analysis with Likert Scale Data\n\n\n20 min\n\n\n\ndplyr\n\n\nggplot2\n\n\nmirt\n\n\npsych\n\n\n\nThis post focuses on exploratory factor analysis on likert scale data using the conventional principal axis factoring. It also includes the calculation of ordinal alpha…\n\n\n\nNov 18, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMulti-Facet Rasch Models with R\n\n\n10 min\n\n\n\nreadr\n\n\ntidyr\n\n\nggplot2\n\n\ndplyr\n\n\ntam\n\n\ncowplot\n\n\n\nIn this post, we’ll explore the Multi-Facet Rasch Model, understand how it works, and demonstrate how to fit this model using R. We’ll also visualize the results to better…\n\n\n\nOct 17, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeasurement Invariance\n\n\n9 min\n\n\n\nlavaan\n\n\nsemPlot\n\n\nggplot2\n\n\nsemTools\n\n\n\nMeasurement invariance (MI) is essential in psychometrics to ensure that a test measures the same construct across different sub-groups of the population. Therefore, it is…\n\n\n\nOct 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAutomated Essay Scoring (AES) with R\n\n\n14 min\n\n\n\nreadxl\n\n\ntextstem\n\n\nstopwords\n\n\nstringr\n\n\nquanteda\n\n\ntm\n\n\ntidyverse\n\n\n\nAt the core of Automated Essay Scoring (AES) is natural language processing (NLP) and machine learning. These systems are designed to analyze the text based on a set of…\n\n\n\nOct 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTop 5 Data Sources\n\n\n6 min\n\n\n\n\n\nThis blog post explains some of the sources that I personally prefer to obtain data. I became aware of most of these sources from Google Data Analytics Certificate program.…\n\n\n\nMar 27, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nItem difficulty & discrimination: Exploring the psychometric properties using R\n\n\n9 min\n\n\n\nShinyItemAnalysis\n\n\nmultilevel\n\n\n\nThis blog post explains the importance of examining item difficulty and discrimination in the development and validation of psychological measures and provides a…\n\n\n\nMar 4, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeb scraping with Rvest package\n\n\n7 min\n\n\n\nRvest\n\n\nWordcloud\n\n\ndplyr\n\n\ntm\n\n\nstopwords\n\n\nwordcloud2\n\n\n\nIn this post, we will delve into harvesting a web page, Ekşi Sözlük. This doesn’t include the automation of the process. Yet, a completed web application for harvesting Ekşi…\n\n\n\nJan 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualisation of My Personal Google Data (2): My exercise routine\n\n\n9 min\n\n\n\npsych\n\n\ndplyr\n\n\nplotly\n\n\n\nThis post is a part of a series that demonstrates how to gain insights from personal Google data. Its purpose is to show how to visualize personal movement data.\n\n\n\nDec 11, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualisation of My Personal Google Data (1): My Locations\n\n\n12 min\n\n\n\nrjson\n\n\ntidyr\n\n\ndplyr\n\n\npurrr\n\n\nlubridate\n\n\nsp\n\n\nleaflet\n\n\n\nThis post is a part of a series that demonstrates how to gain insights from personal Google data. Its purpose is to show how to visualize the locations I visited within a…\n\n\n\nNov 16, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStructural Equation Modelling (in Turkish)\n\n\n76 min\n\n\n\nhaven\n\n\ndplyr\n\n\nlavaan\n\n\n\nYEM: ‘Your View on Science’ ölçeğinden elde edilen 2015 PISA Turkiye örneklemi ile bir örnek çalışma. Bu çalışma Gazi üniversitesi’nde ‘Yapısal Eşitlik Modellemeleri’ dersi…\n\n\n\nOct 30, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTest Equating\n\n\n5 min\n\n\n\nequate\n\n\n\nEquating two test forms: This is a simple test equating study. The data used in this study is simulated from real data. We don’t use the real data for privacy purposes here.\n\n\n\nSep 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nItem Analysis with Item Response Theory (in Turkish)\n\n\n15 min\n\n\n\npsych\n\n\nGPArotation\n\n\nsirt\n\n\nltm\n\n\n\nMadde Tepki Kuramı temelli madde analizi. Hem çok kategorili hem de iki kategorili maddeler için MTK temelli madde analizi uygulama süreçleri…\n\n\n\nNov 20, 2021\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/efaWithLikertData-1/index.html",
    "href": "posts/efaWithLikertData-1/index.html",
    "title": "Exploratory Factor Analysis with Likert Scale Data",
    "section": "",
    "text": "EFA is used to identify a potentially lower number of unobservable factors or constructs that can explain the patterns of correlations within a set of observed variables. This technique is widely used in the development of tests and measurements in psychological, educational, and social sciences research to ensure that the test measures what it’s supposed to measure.\nIn this post, we will build an EFA model using a Likert scale data obtained from Open Psychometrics. Follow the link and find the zip file (OSRI44_dev_data.zip) containing data related to Open Sex Role Inventory. This is an interactive personality test measuring masculinity and femininity (or gendered personality traits) modeled on the Bem Sex-Role Inventory.\nLikert scale data has multiple categories. Thus, it requires running the EFA with polychoric corelations. We will also utilize principle axis factoring. For the reliability of the scale, we will use ordinal Alpha coefficient suggested by Zumbo et al. (2007). All will be run using R. Unfortunately, software like SPSS do not provide an EFA with polychoric correlations or ordinal alpha coefficient. That’s why we have to use languages like R. Another alternative to SPSS in this regard is JASP (which is also built with R in terms of statistics) but it still doesn’t provide a way to calculate ordinal alpha.\nOnce you download the data, you will see two files in the folder: codebook.txt and data.csv. The first gives us information about the data, and the later contains the data itself. Move the data.csv to your working directory and run the following code to see the structure of the data.\n\ndata &lt;- read.delim(\"data.csv\")\nstr(data[,40:57]) # run str(data) to see all items. we do not include the first 39 items here\n\n'data.frame':   318573 obs. of  18 variables:\n $ Q40        : int  4 3 4 3 1 3 2 3 2 4 ...\n $ Q41        : int  1 5 2 4 1 1 1 2 4 4 ...\n $ Q42        : int  5 5 1 1 3 5 4 4 5 5 ...\n $ Q43        : int  1 5 3 4 4 1 4 5 3 4 ...\n $ Q44        : int  5 3 3 3 3 5 1 5 4 4 ...\n $ introelapse: int  579 63 24 3 25 524 5 10 4 21 ...\n $ testelapse : int  211 242 265 231 274 189 213 485 236 263 ...\n $ country    : chr  \"US\" \"PE\" \"CH\" \"US\" ...\n $ IPC        : int  1 1 1 1 2 1 2 1 1 1 ...\n $ source     : int  1 0 0 1 1 0 1 0 1 1 ...\n $ engnat     : int  1 1 2 1 1 1 1 1 1 1 ...\n $ age        : int  18 19 18 19 16 20 16 17 32 22 ...\n $ education  : int  2 2 2 3 2 2 2 2 3 3 ...\n $ gender     : int  2 1 2 2 1 1 1 2 3 1 ...\n $ orientation: int  2 2 1 1 1 2 1 0 1 1 ...\n $ race       : int  6 6 1 6 6 6 6 1 1 6 ...\n $ religion   : int  1 1 1 2 2 1 2 2 1 1 ...\n $ hand       : int  1 2 1 1 1 1 1 1 1 1 ..."
  },
  {
    "objectID": "posts/efaWithLikertData-1/index.html#introduction",
    "href": "posts/efaWithLikertData-1/index.html#introduction",
    "title": "Exploratory Factor Analysis with Likert Scale Data",
    "section": "",
    "text": "EFA is used to identify a potentially lower number of unobservable factors or constructs that can explain the patterns of correlations within a set of observed variables. This technique is widely used in the development of tests and measurements in psychological, educational, and social sciences research to ensure that the test measures what it’s supposed to measure.\nIn this post, we will build an EFA model using a Likert scale data obtained from Open Psychometrics. Follow the link and find the zip file (OSRI44_dev_data.zip) containing data related to Open Sex Role Inventory. This is an interactive personality test measuring masculinity and femininity (or gendered personality traits) modeled on the Bem Sex-Role Inventory.\nLikert scale data has multiple categories. Thus, it requires running the EFA with polychoric corelations. We will also utilize principle axis factoring. For the reliability of the scale, we will use ordinal Alpha coefficient suggested by Zumbo et al. (2007). All will be run using R. Unfortunately, software like SPSS do not provide an EFA with polychoric correlations or ordinal alpha coefficient. That’s why we have to use languages like R. Another alternative to SPSS in this regard is JASP (which is also built with R in terms of statistics) but it still doesn’t provide a way to calculate ordinal alpha.\nOnce you download the data, you will see two files in the folder: codebook.txt and data.csv. The first gives us information about the data, and the later contains the data itself. Move the data.csv to your working directory and run the following code to see the structure of the data.\n\ndata &lt;- read.delim(\"data.csv\")\nstr(data[,40:57]) # run str(data) to see all items. we do not include the first 39 items here\n\n'data.frame':   318573 obs. of  18 variables:\n $ Q40        : int  4 3 4 3 1 3 2 3 2 4 ...\n $ Q41        : int  1 5 2 4 1 1 1 2 4 4 ...\n $ Q42        : int  5 5 1 1 3 5 4 4 5 5 ...\n $ Q43        : int  1 5 3 4 4 1 4 5 3 4 ...\n $ Q44        : int  5 3 3 3 3 5 1 5 4 4 ...\n $ introelapse: int  579 63 24 3 25 524 5 10 4 21 ...\n $ testelapse : int  211 242 265 231 274 189 213 485 236 263 ...\n $ country    : chr  \"US\" \"PE\" \"CH\" \"US\" ...\n $ IPC        : int  1 1 1 1 2 1 2 1 1 1 ...\n $ source     : int  1 0 0 1 1 0 1 0 1 1 ...\n $ engnat     : int  1 1 2 1 1 1 1 1 1 1 ...\n $ age        : int  18 19 18 19 16 20 16 17 32 22 ...\n $ education  : int  2 2 2 3 2 2 2 2 3 3 ...\n $ gender     : int  2 1 2 2 1 1 1 2 3 1 ...\n $ orientation: int  2 2 1 1 1 2 1 0 1 1 ...\n $ race       : int  6 6 1 6 6 6 6 1 1 6 ...\n $ religion   : int  1 1 1 2 2 1 2 2 1 1 ...\n $ hand       : int  1 2 1 1 1 1 1 1 1 1 ..."
  },
  {
    "objectID": "posts/efaWithLikertData-1/index.html#preprocessing",
    "href": "posts/efaWithLikertData-1/index.html#preprocessing",
    "title": "Exploratory Factor Analysis with Likert Scale Data",
    "section": "Preprocessing",
    "text": "Preprocessing\nThere are 57 variables and the first 44 variables are actual data collected from the questionaire while the rest contains information about the test taker.\nTo see the number of test takers, run the following:\n\nnrow(data)\n\n[1] 318573\n\n\nThe number of observations is 318573, which I find a little exhausting for a demo analysis. So I will filter it to reduce the number. These are some of the demographics of the participants:\n\nengnat ” Is English you native language?” 1=Yes, 2=No\nage “What is your age?”, entered as text (ages &lt; 13 not recorded)\neducation “How much education have you completed?” 1=Less than high school, 2=High school, 3=University degree, 4=Graduate degree gender 1=Male, 2=Female, 3=Other\norientation 1=Heterosexual, 2=Bisexual, 3=Homosexual, 4=Asexual, 5=Other\nrace 1=Mixed race, 2=Asian, 3=Black, 4=Native American, 5=Native Australian, 6=White, 7=Other\nreligion 1=Atheist/Agnostic, 2=Christian, 3=Muslim, 4=Jewish, 5=Hindu, 6=Buddhist, 7=Other\nhand “What hand do you use to write with?” 1=Right, 2=Left, 3=Both\nsource how the user found the test, based on HTTP Referer info. 1=google, 2=tumblr, 3=facebook/messenger, 4=reddit, 5=bing, 0=other or unknown.\n\nLet’s filter participants whose native language is English, who are right-handed, between ages 18 and 40, and came from Google. Also, only keep the questionaire data as we are not interested in the others from now on. Finally see the summary of the data:\n\nlibrary(dplyr)\n\ndf_with_demographics &lt;- data %&gt;% \n  filter(engnat == 1) %&gt;% \n  filter(hand == 1) %&gt;% \n  filter(age &gt;= 18 & age &lt;= 30) %&gt;% \n  filter(source == 1)\n\ndf &lt;- df_with_demographics %&gt;% select(1:44)\n\nsummary(df[,1:5]) # run summary(df) to see all items.\n\n       Q1              Q2             Q3          Q4              Q5       \n Min.   :0.000   Min.   :0.00   Min.   :0   Min.   :0.000   Min.   :0.000  \n 1st Qu.:1.000   1st Qu.:2.00   1st Qu.:1   1st Qu.:2.000   1st Qu.:1.000  \n Median :1.000   Median :5.00   Median :3   Median :4.000   Median :3.000  \n Mean   :2.023   Mean   :3.76   Mean   :3   Mean   :3.281   Mean   :2.972  \n 3rd Qu.:3.000   3rd Qu.:5.00   3rd Qu.:5   3rd Qu.:5.000   3rd Qu.:4.000  \n Max.   :5.000   Max.   :5.00   Max.   :5   Max.   :5.000   Max.   :5.000"
  },
  {
    "objectID": "posts/efaWithLikertData-1/index.html#model-assumptions",
    "href": "posts/efaWithLikertData-1/index.html#model-assumptions",
    "title": "Exploratory Factor Analysis with Likert Scale Data",
    "section": "Model Assumptions",
    "text": "Model Assumptions\nSome of EFA’s assumptions are:\n\n\nContinuous variables: In our case the data is in likert scale, thus descrete. For such data, IRT based factor reduction is much more appropriate, which we will discuss in a later post. Yet, in practice, researchers use EFA with likert-scale data too if the scale is large enough.\n\nNo outliers: Outlier detection is another topic for another post. We will disregard this for now.\n\nNo multicollinearity: This is an assumption we can check simply with a correlation matrix. Basically, EFA expects no observed variables with high correlation. If there are pairs with high correlation, there are suggestions such as dropping one of the pair or merging them into a single variable.\n\nSampling Adequecy: We check if the data is adequate for EFA using Kaiser-Meyer-Olkin (KMO) test for each observed variable as well as the whole model. KMO values above .60 is considered acceptable, while closer to 1 means much better adequecy.\n\nSphericity: An assumption that is usually tested with Bartlett’s Test of Sphericity. The assumption here is that the correlation matrix cannot be an identity matrix (also referred as unit matrix). An identity matrix is a diagonal matrix with all its diagonal elements equal to 1 , and zeroes everywhere else. Bartlett’s test tests the null hypothesis that the correlation matrix is an identity matrix. If the p-value is less than .05, we reject the null hypothesis and conclude that the correlation matrix is not an identity matrix, which means the data is suitable for EFA.\n\nIn this post, we will test multicollinearity, KMO and Bartlett’s Test of Sphericity.\nMulticollinearity\nLet’s see a correlation heatmap for multicollinearity.\n\nlibrary(psych)\nlibrary(plotly)\n\npoly_matrix &lt;- polychoric(df)$rho\n\n# Create a long format of the correlation matrix\npoly_long &lt;- as.data.frame(as.table(poly_matrix)) %&gt;%\n  filter(as.numeric(Var1) &lt;= as.numeric(Var2))\n\n# Create the Plotly heatmap\nplot_ly(\n  data = poly_long,\n  x = ~Var2,\n  y = ~Var1,\n  z = ~Freq,\n  text = ~round(Freq, 2), \n  type = \"heatmap\",\n  colorscale = list(\n    c(0, \"blue\"),  # Minimum correlation (-1)\n    c(0.5, \"white\"), \n    c(1, \"red\")  # Maximum correlation (1)\n  ),\n  zmin = -1,  # Set the minimum value of the color bar\n  zmax = 1,   # Set the maximum value of the color bar\n  showscale = TRUE\n) %&gt;%\n  layout(\n    xaxis = list(title = \"Variables\", tickangle = 45),\n    yaxis = list(title = \"Variables\"),\n    title = \"Correlation Matrix\",\n    colorbar = list(title = \"Correlation\", tickvals = c(-1, -0.5, 0, 0.5, 1))  # Customize legend ticks\n  )\n\n\n\n\n\nItems with high correlation are candidates for merging or dropping. There are two cells with colors dark red or dark blue in the heatmap. Q21 and Q8 have negative high correlation (.77) and Q43 and Q27 has positive high correlation (.97). We can consider dropping one variable from each pair. Let’s see the values of these cells:\n\npoly_matrix[8, 21]\n\n[1] -0.7707296\n\npoly_matrix[27, 43]\n\n[1] 0.9671933\n\n\nRemoving one of the variables from each pair is a common practice in EFA. We will drop Q21 and Q43. Then, create the polychoric matrix again.\n\ndf &lt;- df %&gt;% select(-Q21, -Q43)\npoly_matrix &lt;- polychoric(df)$rho\n\nSampling Adequacy with KMO\nNow, let’s check the sampling adequacy with KMO test. We expect each varianle’s KMO value to be above .60. Also, the overall KMO value should be be above .60 too.\n\nKMO(poly_matrix)\n\nKaiser-Meyer-Olkin factor adequacy\nCall: KMO(r = poly_matrix)\nOverall MSA =  0.93\nMSA for each item = \n  Q1   Q2   Q3   Q4   Q5   Q6   Q7   Q8   Q9  Q10  Q11  Q12  Q13  Q14  Q15  Q16 \n0.95 0.94 0.93 0.93 0.92 0.85 0.94 0.95 0.92 0.88 0.88 0.97 0.91 0.87 0.91 0.97 \n Q17  Q18  Q19  Q20  Q22  Q23  Q24  Q25  Q26  Q27  Q28  Q29  Q30  Q31  Q32  Q33 \n0.92 0.90 0.96 0.97 0.96 0.88 0.93 0.93 0.96 0.93 0.95 0.92 0.96 0.95 0.86 0.94 \n Q34  Q35  Q36  Q37  Q38  Q39  Q40  Q41  Q42  Q44 \n0.95 0.93 0.96 0.91 0.95 0.91 0.87 0.92 0.96 0.96 \n\n\nSphericity with Bartlett’s Test\nNow, let’s check the sphericity with Bartlett’s Test. We expect the p-value to be less than .05.\n\ncortest.bartlett(poly_matrix, n = nrow(df))\n\n$chisq\n[1] 319501.8\n\n$p.value\n[1] 0\n\n$df\n[1] 861\n\n\nAs, the assumptions we tested are fulfilled, we can continue with the"
  },
  {
    "objectID": "posts/efaWithLikertData-1/index.html#determine-the-number-of-factors",
    "href": "posts/efaWithLikertData-1/index.html#determine-the-number-of-factors",
    "title": "Exploratory Factor Analysis with Likert Scale Data",
    "section": "Determine the number of factors",
    "text": "Determine the number of factors\nWhile determining the number of factors to extract, there are many methods that can be used. See a summary of them in Zwick et.al. (1986). Some of the most common methods are:\n1. Kaiser Criterion (K1): we can use the Kaiser criterion, which suggests retaining factors with eigenvalues greater than 1.\n2. Paralell Analysis: This is a simulation-based method that compares the observed eigenvalues with the eigenvalues obtained from random data. The factors with eigenvalues greater than the random data are retained in this approach.\n3. Very Simple Structure (VSS): VSS criterion evaluates how well a simplified factor pattern reproduces the original correlation matrix. In this simplified pattern, only the highest loading for each item is kept, while all other loadings are set to zero. The VSS score, ranging from 0 to 1, measures the goodness-of-fit of the factor solution. This evaluation is conducted for solutions with varying numbers of factors, starting from one (k = 1) up to a user-defined maximum. The optimal number of factors is determined by the solution with the highest VSS score.\n4. Minimum Average Partial (MAP): This method is based on partial correlations matrix. It is very similar to Principal Component Analysis (PCA).\nIn order to implement Kaiser Criterion, we simply run the following code.\n\nscree(poly_matrix, pc = FALSE)\n\n\n\n\n\n\n\nThe produced plot suggests 2 or 3 factors. To be more certain, we can run the parallel analysis with the same scree plot with the following code:\n\nfa.parallel(\n  poly_matrix, \n  n.obs = nrow(df),\n  fa = \"fa\", \n  fm = \"pa\", \n  cor = \"poly\",\n  n.iter = 20,\n  show.legend = FALSE \n)\n\nParallel analysis suggests that the number of factors =  12  and the number of components =  NA \n\nabline(h = 1, col = \"green\", lty = 2) \n\nlegend(\"topright\", \n       legend = c(\"Actual Data\", \"K1 Criterion\", \"Parallel analysis\"), \n       col = c(\"blue\", \"green\", \"red\"), \n       lty = c(1, 2, 3), \n       pch = c(24, NA, NA), \n       bty = \"n\")\n\n\n\n\n\n\n\nHowever, parallel analysis suggests 12 factors which is not reasonable. Let’s try VSS and MAP too. These can be obtained from nfactor() function of psych package.\n\nvss_complexity &lt;- nfactors(poly_matrix, n.obs = nrow(df), fm = \"pa\", cor = \"poly\", SMC=FALSE)\n\n\n\n\n\n\n\nThe output actually speaks for itself. To better understand this, let’s have a look at the plots produced with the function. The plot on the upper left corner shows the VSS fit values with the number of factors. As can be seen 2 and 3 factors are very close and they are higher than 1 factor. This is in alignment with the K1 criterion.\nThe plot on the upper right corner shows the complexity of the construct with the number of factors. Complexity increases as the number of factors increase until 12 factors. We do not want to go with a high number of factors, so this plot also supports our decision to ignore parallel analysis too.\nThe plot on the lower left corner shows the Empirical BIC values with the number of factors. BIC value decreases highest from 1 to 2 factors. But from 2 to 3, the decrease is not so eye-catching.\nAs for the plot on the lower right corner, it shows the RMR values with the number of factors. RMRs below 0.10 is acceptable while below 0.08 is desirable. Any model that has more than 1 factor has RMR below 0.08 in the plot.\nIn conclusion, we can say that 2 or 3 factors are reasonable, while the first is a little more preferable. For our final decision, we can also check the variance explained by two models. If there is not much increase from 2-factor model to 3-factor model, we can go with 2 factors.\n\ntwo_fm &lt;- fa(poly_matrix, n.obs = nrow(df), nfactors = 2, fm = \"pa\", rotate = \"none\", cor = \"poly\", SMC=FALSE)\nthree_fm &lt;- fa(poly_matrix, n.obs = nrow(df), nfactors = 3, fm = \"pa\", rotate = \"none\", cor = \"poly\", SMC=FALSE)\n#Explained Variance by 2-Factor Model:\ntwo_fm$Vaccounted\n\n                            PA1       PA2\nSS loadings           8.1762800 3.2680913\nProportion Var        0.1946733 0.0778117\nCumulative Var        0.1946733 0.2724850\nProportion Explained  0.7144368 0.2855632\nCumulative Proportion 0.7144368 1.0000000\n\n#Explained Variance by 3-Factor Model:\nthree_fm$Vaccounted\n\n                            PA1       PA2        PA3\nSS loadings           8.1985977 3.2818970 1.08421360\nProportion Var        0.1952047 0.0781404 0.02581461\nCumulative Var        0.1952047 0.2733451 0.29915972\nProportion Explained  0.6525100 0.2611996 0.08629039\nCumulative Proportion 0.6525100 0.9137096 1.00000000\n\n\nThe variance explained cumulatively by the 2-factor model is 0.27 while the 3-factor model explains 0.30. The increase is not so high, so we can go with the 2-factor model."
  },
  {
    "objectID": "posts/efaWithLikertData-1/index.html#rotation",
    "href": "posts/efaWithLikertData-1/index.html#rotation",
    "title": "Exploratory Factor Analysis with Likert Scale Data",
    "section": "Rotation",
    "text": "Rotation\nTo see if we need to rotate, we can check the factor loadings. If the loadings are high and clear, we can go without rotation. Let’s see the loadings for the 2-factor model.\n\nfactor_loadings &lt;- two_fm$loadings\nprint(head(factor_loadings)) #use the following for all variables:  print(factor_loadings)\n\n          PA1        PA2\nQ1 -0.4532517 0.25125939\nQ2  0.5952802 0.25000562\nQ3 -0.4598388 0.55066466\nQ4  0.4588183 0.35760922\nQ5 -0.3225258 0.38951608\nQ6  0.2084623 0.08448999\n\n\nThe loadings are not so clear. Some of the observed variables load on both factors highly and equally such as Q3, Q4, Q5 etc. We can try to rotate the factors.\nRotation does not change the underlying solution but redistributes the variance to simplify the interpretation of factor loadings.\nIt is typically used when you expect that each variable primarily loads on one factor, and you want to make those relationships clearer.\nRotation is a technique used to simplify the factor structure. It is used to make the factors more interpretable. There are two types of rotation: orthogonal and oblique. Orthogonal rotation assumes that the factors are uncorrelated, while oblique rotation allows the factors to be correlated.\nIn contemporary research, oblique rotation is more commonly used, particularly in fields like psychology, sociology, and behavioral sciences, where correlated factors are expected. While orthogonal rotation is still used in cases where independence among factors is assumed or desired, the realistic nature of oblique rotations makes them the preferred choice for most exploratory factor analysis (EFA) applications.\nThere are many rotation methods such as varimax, promax, oblimin, and quartimin.\nLet’s rotate the factors with oblimin:\n\ntwo_fm &lt;- fa(poly_matrix, n.obs = nrow(df), nfactors = 2, fm = \"pa\", rotate = \"oblimin\", cor = \"poly\", SMC=FALSE)\n\n\nfactor_loadings_rotated &lt;- two_fm$loadings\nprint(head(factor_loadings_rotated))\n\n           PA1         PA2\nQ1 -0.16698240  0.44127564\nQ2  0.63696320 -0.02629649\nQ3  0.04356274  0.72980540\nQ4  0.60970090  0.13704242\nQ5  0.03292070  0.51501188\nQ6  0.22085587 -0.01212766\n\n\nThe loadings are now more clear. Also, we can see that some of the variables load neither of the variables. This is a common situation in EFA. This also explains why a 3-factor model is also strong. Yet, we prefer 2-factor model so We can drop these variables one by one and re-run the analysis. To determine with which item to start, we can check the communalities of the variables.\n\nfactor_1 &lt;- as.matrix(two_fm$loadings[,\"PA1\"])\nfactor_2 &lt;- as.matrix(two_fm$loadings[,\"PA2\"])\ncommunalities &lt;- two_fm$communality\n\n# merge as a table\nloadings &lt;- data.frame(\n  factor_1 = factor_1,\n  factor_2 = factor_2,\n  communalities = communalities\n)\nloadings &lt;- loadings[order(loadings$communalities, decreasing = FALSE),]\nprint(head(loadings, 10)) # use print(loadings) to see all variables\n\n       factor_1    factor_2 communalities\nQ6   0.22085587 -0.01212766    0.05059508\nQ11 -0.12298253  0.21681114    0.07876337\nQ10  0.27005620 -0.03300168    0.07957850\nQ18  0.32004233  0.10727368    0.09252015\nQ23  0.07002200  0.35815591    0.11753590\nQ32  0.33746578 -0.07831434    0.13650095\nQ41 -0.06846225  0.37184011    0.15883090\nQ37 -0.19601538  0.30838271    0.17122609\nQ24  0.38433571 -0.07601277    0.17171431\nQ8   0.38663998 -0.10503405    0.18585326\n\n\nFinal Solution\nIn EFA, we do not want communalities lower than .30. Also loadings lower than .30 are not considered as strong. Preferably, loadings above .40 are considered strong.\nDropping variables is an iterative practice. You need to drop one variable, re-run the analysis, check the loadings and communalities, and decide if you need to drop another variable. This process continues until all variables have strong loadings and communalities. Thus, I will report only the final solution here.\nFollowing line of code will drop all the unwanted variables, and then re-run the 2-factor model with oblimin rotation:\n\ndf &lt;- df %&gt;% select(\n  -Q6, -Q11, -Q10, -Q18, -Q23, -Q32, -Q37, -Q41, \n  -Q24, -Q8, -Q13, -Q33, -Q7, -Q19, -Q31, -Q42, \n  -Q25, -Q34, -Q28, -Q5, -Q1, -Q12, -Q27, -Q30, -Q22\n  )\n\n\npoly_matrix &lt;- polychoric(df)$rho\n\ntwo_fm &lt;- fa(poly_matrix, nfactors = 2, fm = \"pa\", rotate = \"oblimin\", cor = \"poly\", SMC=FALSE)\n#Factor Loadings:\nprint(two_fm$loadings, cutoff = 0.4)\n\n\nLoadings:\n    PA1    PA2   \nQ2   0.592       \nQ3          0.756\nQ4   0.585       \nQ9          0.587\nQ14  0.638       \nQ15         0.558\nQ16  0.610       \nQ17         0.523\nQ20  0.544       \nQ26  0.582       \nQ29         0.600\nQ35         0.589\nQ36  0.547       \nQ38  0.626       \nQ39         0.788\nQ40  0.637       \nQ44  0.589       \n\n                 PA1   PA2\nSS loadings    3.587 2.878\nProportion Var 0.211 0.169\nCumulative Var 0.211 0.380\n\n# Communalities:\nprint(two_fm$communality, cutoff = 0.3)\n\n       Q2        Q3        Q4        Q9       Q14       Q15       Q16       Q17 \n0.3615579 0.5625665 0.3221763 0.4227183 0.3981835 0.3002707 0.3874601 0.2982325 \n      Q20       Q26       Q29       Q35       Q36       Q38       Q39       Q40 \n0.3665253 0.3399021 0.3515497 0.3474944 0.3071188 0.3870429 0.6124040 0.3960493 \n      Q44 \n0.3446296 \n\n\nWith this, our explained cumulative variance has risen to 38%. Let’s see factor correlations and the reliability score of the scale:\n\n#Factor Correlations:\nround(two_fm$Phi, 2)\n\n      PA1   PA2\nPA1  1.00 -0.24\nPA2 -0.24  1.00\n\nreliability_coef &lt;- psych::alpha(poly_matrix, check.keys=TRUE)\n#Ordinal Alpha:\nprint(round(reliability_coef[[\"total\"]][[\"raw_alpha\"]], 2))\n\n[1] 0.84\n\n\nCorrelation between factors is -0.24, which indicates a very low and negative correlation. The ordinal alpha coefficient is 0.84 which is considered as good. The 2-factor model with oblimin rotation is our final solution. Finally let’s draw the path diagram of the model:\n\nfa.diagram(\n  two_fm, \n  digits = 2,\n  main = \"Final Solution for Open Sex Role Inventory\",\n  e.size=.07,\n  rsize=.1,\n  cex=0.9)\n\n\n\n\n\n\n\nThe final solution has 10 items in the first factor and 7 items in the second factor. Here are the items in each factor:\n\n\n\n\n\n\nFactor 1\nFactor 2\n\n\n\nQ2 I have thought about dying my hair.\nQ3 I have thrown knives, axes or other sharp things.\n\n\nQ4 I give people handmade gifts.\nQ9 I like guns.\n\n\nQ14 I dance when I am alone.\nQ15 I have thought it would be exciting to be an outlaw.\n\n\nQ16 When I was a child, I put on fake concerts and plays with my friends.\nQ17 I have considered joining the military.\n\n\nQ20 I sometimes feel like crying when I get angry.\nQ29 I have burned things up with a magnifying glass.\n\n\nQ26 I jump up and down in excitement sometimes.\nQ35 I have taken apart machines just to see how they work.\n\n\nQ36 I take lots of pictures of my activities.\nQ39 I have set fuels, aerosols or other chemicals on fire, just for fun.\n\n\nQ38 I leave nice notes for people now and then.\n\n\n\nQ40 I really like dancing.\n\n\n\nQ44 I decorate my things (e.g. stickers on laptop).\n\n\n\n\nIt is pretty easy now to interpret the factors given that this is a scale named as Open Sex Role Inventory. The first factor seems to be related to femininity while the second factor seems to be related to masculinity.\nAs a final touch, let’s see the scree plot of the genders of the participants along with their scores on the two factors; masculinity and femininity. We will use factor.scores() to calculate participants’ scores and the ggplot2 package for visualization:\n\nlibrary(ggplot2)\nOSRI_scores &lt;- factor.scores(df, two_fm)$scores\ncolnames(OSRI_scores) &lt;- c(\"Femininity\", \"Masculinity\")\nscores &lt;- as_tibble(OSRI_scores)\nscores &lt;- bind_cols(df_with_demographics |&gt; select(gender), scores) |&gt;\n  filter(gender %in% c(1, 2)) |&gt;  \n  mutate(gender = factor(gender, labels = c(\"Male\", \"Female\")))  \n\n# Plot the filtered data\nscores |&gt;\n  ggplot(aes(Femininity, Masculinity, color = gender)) +\n  geom_point() +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  labs(color = \"Gender\")"
  },
  {
    "objectID": "posts/efaWithLikertData-1/index.html#conclusion",
    "href": "posts/efaWithLikertData-1/index.html#conclusion",
    "title": "Exploratory Factor Analysis with Likert Scale Data",
    "section": "Conclusion",
    "text": "Conclusion\nIn this post, we have seen how to conduct an EFA with polychoric correlations and ordinal alpha coefficient. We have also seen how to check the assumptions of EFA, determine the number of factors, and rotate the factors. We have also seen how to drop variables iteratively to reach a final solution. Then, we have seen how to interpret the factors and draw the path diagram of the model. Finally, we have seen how to obtain and visualize the scores of the participants on the factors."
  },
  {
    "objectID": "posts/efaWithLikertData-1/index.html#further-remarks",
    "href": "posts/efaWithLikertData-1/index.html#further-remarks",
    "title": "Exploratory Factor Analysis with Likert Scale Data",
    "section": "Further Remarks",
    "text": "Further Remarks\n\nThe data used here is a likert scale data. Yet, EFA actually expects continuous variables. Therefore, it might be wiser to use IRT based factor reduction methods for likert scale data. We will discuss this in a later post.\nThe outliers should be checked and removed before running the EFA. Yet, we haven’t done it here. Outlier detection is another topic for another post."
  },
  {
    "objectID": "posts/Exploring the psychometric properties using R/index.html",
    "href": "posts/Exploring the psychometric properties using R/index.html",
    "title": "Item difficulty & discrimination: Exploring the psychometric properties using R",
    "section": "",
    "text": "Developing and validating psychological measures requires examining their psychometric properties, including their reliability and validity. One aspect of a measure’s validity is its item difficulty, which refers to how easy or difficult each individual item is for respondents to answer correctly. Another important aspect is item discrimination, which measures the extent to which each item distinguishes between participants who have high or low levels of the construct being measured.\nUnderstanding item difficulty and item discrimination is crucial for several reasons. First, items that are too easy or too difficult can limit the variability of responses, making it harder to discriminate between participants who have different levels of the construct being measured. Second, items with low discrimination may not effectively differentiate between participants with different levels of the construct, leading to decreased validity.\nIn this blog post, we’ll explore how to calculate item difficulty and item discrimination in R using an example dataset. We’ll explain what each of these psychometric properties are, why they’re important, and how to interpret the results. We’ll also discuss some limitations and considerations when examining these properties. So let’s get started!\nThe dataset is generated via this web application based on Item Response Theory 2-Parameter Logistic Model. The dataset consists of randomly generated 40 items with a sample size of 500. The a-parameters of the items vary between 0.8 and 1.3 while b-parameters vary between -3 and 3. The c and d parameters are fixed to 0 and 1 respectively for all items.\nHere are the package(s) we use in this post:\n\nlibrary(multilevel)\nlibrary(ShinyItemAnalysis)"
  },
  {
    "objectID": "posts/Exploring the psychometric properties using R/index.html#introduction",
    "href": "posts/Exploring the psychometric properties using R/index.html#introduction",
    "title": "Item difficulty & discrimination: Exploring the psychometric properties using R",
    "section": "",
    "text": "Developing and validating psychological measures requires examining their psychometric properties, including their reliability and validity. One aspect of a measure’s validity is its item difficulty, which refers to how easy or difficult each individual item is for respondents to answer correctly. Another important aspect is item discrimination, which measures the extent to which each item distinguishes between participants who have high or low levels of the construct being measured.\nUnderstanding item difficulty and item discrimination is crucial for several reasons. First, items that are too easy or too difficult can limit the variability of responses, making it harder to discriminate between participants who have different levels of the construct being measured. Second, items with low discrimination may not effectively differentiate between participants with different levels of the construct, leading to decreased validity.\nIn this blog post, we’ll explore how to calculate item difficulty and item discrimination in R using an example dataset. We’ll explain what each of these psychometric properties are, why they’re important, and how to interpret the results. We’ll also discuss some limitations and considerations when examining these properties. So let’s get started!\nThe dataset is generated via this web application based on Item Response Theory 2-Parameter Logistic Model. The dataset consists of randomly generated 40 items with a sample size of 500. The a-parameters of the items vary between 0.8 and 1.3 while b-parameters vary between -3 and 3. The c and d parameters are fixed to 0 and 1 respectively for all items.\nHere are the package(s) we use in this post:\n\nlibrary(multilevel)\nlibrary(ShinyItemAnalysis)"
  },
  {
    "objectID": "posts/Exploring the psychometric properties using R/index.html#item-difficulty",
    "href": "posts/Exploring the psychometric properties using R/index.html#item-difficulty",
    "title": "Item difficulty & discrimination: Exploring the psychometric properties using R",
    "section": "Item Difficulty",
    "text": "Item Difficulty\nItem difficulty is a psychometric property that measures how easy or difficult an item is for respondents to answer correctly. Examining item difficulty is important because it can help identify items that are too easy or too difficult, which can limit the variability of responses and make it harder to discriminate between participants who have different levels of the construct being measured.\nThe proportion of correct responses for each item is calculated and reported as the item difficulty value. This calculation can be done manually using spreadsheet software or programmatically using statistical software such as R or SPSS. R has many packages and functions out there that we can use to calculate item difficulties. Yet we will calculate them simply with colMeans() function.\nLet’s start by introducing the dataset to R environment.\n\nmy_data&lt;-read.csv(\"data for post about item difficulty and discrimination.csv\",sep=\";\", header = TRUE)\nhead(my_data) \n\n  i1 i2 i3 i4 i5 i6 i7 i8 i9 i10 i11 i12 i13 i14 i15 i16 i17 i18 i19 i20 i21\n1  0  1  1  1  0  0  1  0  0   1   1   0   1   1   1   1   0   0   0   0   0\n2  0  1  1  1  0  0  1  0  0   1   0   0   1   0   1   1   1   1   0   1   0\n3  0  1  1  1  0  0  1  1  1   1   1   0   1   1   1   1   1   1   1   1   0\n4  1  1  0  0  0  0  0  0  0   1   0   0   1   1   0   1   1   0   0   0   0\n5  0  1  1  1  0  0  0  0  1   1   1   1   1   1   1   1   1   1   0   0   0\n6  0  1  1  1  1  1  1  1  1   1   1   1   1   1   0   1   1   0   0   0   1\n  i22 i23 i24 i25 i26 i27 i28 i29 i30 i31 i32 i33 i34 i35 i36 i37 i38 i39 i40\n1   0   0   1   1   1   0   1   0   0   0   1   0   1   0   1   0   1   1   0\n2   0   0   1   1   1   0   0   0   0   0   0   1   1   0   1   0   0   1   1\n3   0   0   1   1   1   0   1   0   0   0   1   1   1   1   1   0   1   1   1\n4   0   0   0   1   1   0   1   0   0   0   0   0   1   0   0   0   0   1   1\n5   0   0   1   1   1   1   0   0   0   0   1   1   1   1   1   0   0   1   1\n6   0   0   1   1   1   0   1   1   0   0   1   1   1   1   1   1   1   1   1\n\n\nNow let’s get the item difficulties:\n\n# Calculate item difficulty\nitem_difficulty &lt;- colMeans(my_data)\nitem_difficulty\n\n   i1    i2    i3    i4    i5    i6    i7    i8    i9   i10   i11   i12   i13 \n0.212 0.918 0.910 0.782 0.112 0.394 0.742 0.400 0.862 0.870 0.774 0.386 0.882 \n  i14   i15   i16   i17   i18   i19   i20   i21   i22   i23   i24   i25   i26 \n0.682 0.594 0.928 0.722 0.384 0.174 0.346 0.512 0.176 0.100 0.682 0.900 0.882 \n  i27   i28   i29   i30   i31   i32   i33   i34   i35   i36   i37   i38   i39 \n0.264 0.536 0.148 0.044 0.046 0.918 0.638 0.946 0.610 0.800 0.146 0.566 0.928 \n  i40 \n0.952 \n\n\nInterpreting the results of item difficulty is straightforward. Items with higher difficulty values indicate that they were easier for participants to answer correctly, while items with lower difficulty values were more difficult. In our example dataset, the output shows that item 40 had the highest difficulty value of 0.952, meaning that 95.2% of participants answered this item correctly. Item 30, on the other hand, had the lowest difficulty value of 0.044, meaning that only 4.4% of participants answered this item correctly.\nIt’s important to note that each construct should be evaluated within its own concept while interpreting item difficulties. Yet, for achievement tests, a generic classification might be defined as “easy” if the index is 0.85 or above; “moderate” if it is between 0.41 and 0.84; and “hard” if it is 0.40 or below. Also, item difficulty is not the only factor to consider when evaluating the quality of a measure. Items that are too easy or too difficult may still be valid and reliable, depending on the construct being measured and the purpose of the measure. However, examining item difficulty can provide valuable insights into the psychometric properties of the measure and inform decisions about item selection and revision."
  },
  {
    "objectID": "posts/Exploring the psychometric properties using R/index.html#item-discrimination",
    "href": "posts/Exploring the psychometric properties using R/index.html#item-discrimination",
    "title": "Item difficulty & discrimination: Exploring the psychometric properties using R",
    "section": "Item Discrimination",
    "text": "Item Discrimination\nItem discrimination is another important psychometric property that measures the extent to which each item differentiates between participants who have high or low levels of the construct being measured. It indicates how well an item distinguishes between participants with different levels of the construct.\nIt’s important to note that (just like item difficulties) each construct should be evaluated within its own concept while interpreting item discriminations. Yet, for achievement tests, a generic classification might be defined as “good” if the index is above 0.30; “fair” if it is between 0.10 and 0.30; and “poor” if it is below 0.10.\nTo obtain a value for item discrimination, there are several statistical approaches that we can utilize. Here we will discuss (1) correlation between item and total score with the item, (2) correlation between item and total score without the item, and (3) upper-lower groups index.\n1. Correlation between item and total score with the item\nThis approach is based on calculating the point-biserial correlation coefficient (rpb) between each item and the total score of the measure. The total score is calculated by summing the scores of all items. The rpb ranges from -1 to 1, with values closer to 1 indicating higher discrimination.\nLet’s first calculate the total score for each participant in our example dataset. Then, use a for loop to calculate rpb coefficients for each item:\n\n#get the total score for each participant\ntotal_score &lt;- rowSums(my_data)\n\n#There are 40 items in the test:\nitem_discrimination1 &lt;- 40  \n\n#calculate rpb for each item:\nfor(i in 1:40){        \n  item_discrimination1[i] &lt;- cor(total_score, my_data[,i])  \n}\nround(item_discrimination1,4)\n\n [1] 0.3984 0.2162 0.2308 0.4485 0.3019 0.4847 0.4148 0.4548 0.2796 0.3335\n[11] 0.4498 0.5138 0.2735 0.4336 0.4423 0.1956 0.4288 0.4133 0.3297 0.4214\n[21] 0.4909 0.3174 0.3222 0.4098 0.3312 0.3464 0.3737 0.4876 0.2481 0.2257\n[31] 0.1783 0.3536 0.4094 0.2837 0.3893 0.3888 0.3962 0.5025 0.2866 0.1906\n\n\n2. Correlation between item and total score without the item\nThis approach is very similar to the first one. The only difference is that when we calculate the correlation between an item and the total score, we do not include the item. This type of an approach will result in slightly reduced index values when compared to the first approach. Therefore, it is usually a more-preferred approach by test developers (we would love to stay in the safe-zone).\nThe package multilevel has a specific function for this index. The item.total() function takes only the dataset as input and provides us with a dataframe with four columns: item name, discrimination index, a reliability index without the item and the sample size. Yet, we only need the discrimination index. Here how we get it:\n\nitem_discrimination2&lt;-multilevel::item.total(my_data)\nitem_discrimination2$Item.Total\n\n [1] 0.3365024 0.1703597 0.1832279 0.3887597 0.2511883 0.4160659 0.3493047\n [8] 0.3837360 0.2232991 0.2803945 0.3892913 0.4477891 0.2207879 0.3648121\n[15] 0.3701690 0.1521115 0.3624807 0.3399690 0.2694241 0.3503333 0.4211616\n[22] 0.2564267 0.2746220 0.3395723 0.2838892 0.2959223 0.3053936 0.4177549\n[29] 0.1892085 0.1916937 0.1429237 0.3110586 0.3368093 0.2471547 0.3143658\n[36] 0.3277659 0.3428951 0.4342855 0.2447900 0.1546114\n\n\n3. Upper-lower groups index\nPersonally, I feel that this approach is the most meaningfully-related approach in terms of “discrimination”. That’s because while calculating it, we divide the whole group into sub-groups (usually three groups) according to their total scores. Then, we calculate the discrimination index for an item by comparing these groups’ responses to that item. This definition feels more like a discrimination index to my illiterate ears.\nIn R environment, ShinyItemAnalysis package has a specific function for this index. The gDiscrim() function has several arguements such as Data(the data set), k (the number of sub groups and 3 is default), l and u (numeric values to define the lower and upper groups and the defaults are 1 and 3 consecutively). There are other arguments that should be checked on the manual before using the function.\nWe simply use the function as:\n\nitem_discrimination3&lt;-ShinyItemAnalysis::gDiscrim(my_data)\nitem_discrimination3\n\n        i1         i2         i3         i4         i5         i6         i7 \n0.35177700 0.11253051 0.12801251 0.42537370 0.20042709 0.57908786 0.40470561 \n        i8         i9        i10        i11        i12        i13        i14 \n0.56204240 0.19718578 0.26586333 0.43982611 0.56566504 0.21888347 0.50640635 \n       i15        i16        i17        i18        i19        i20        i21 \n0.49603417 0.11253051 0.45530811 0.44428768 0.26346095 0.46442190 0.56829622 \n       i22        i23        i24        i25        i26        i27        i28 \n0.26449054 0.20145668 0.43875839 0.20908328 0.25709274 0.36420836 0.59567572 \n       i29        i30        i31        i32        i33        i34        i35 \n0.17720409 0.08419768 0.07748627 0.19100824 0.45732916 0.12957596 0.41599298 \n       i36        i37        i38        i39        i40 \n0.37633466 0.29804759 0.54766626 0.13163514 0.09601891 \n\n\nNote that if you change the number of subgroups, you should be careful while interpreting the results.\nNo matter what statistical approach you use to estimate discrimination indexes, it’s also important to note that item discrimination can be influenced by factors such as the sample size, the range of scores, and the homogeneity of the sample. Therefore, it’s recommended to examine item discrimination in conjunction with other psychometric properties such as item difficulty and reliability."
  },
  {
    "objectID": "posts/Exploring the psychometric properties using R/index.html#a-package-of-personal-preferrance",
    "href": "posts/Exploring the psychometric properties using R/index.html#a-package-of-personal-preferrance",
    "title": "Item difficulty & discrimination: Exploring the psychometric properties using R",
    "section": "A Package of Personal Preferrance",
    "text": "A Package of Personal Preferrance\nWhile we can calculate both difficulty and discrimination indexes manually or by using different functions from different packages, my totally-subjective opinion is that ItemAnalysis() function from the ShinyItemAnalysis package gives a well-groomed output for many item statistics. The following code snippet simply provides us with many indexes including difficulty and three types of discrimination indexes:\n\n#round is for rounding the values in the results.\nitem_stats&lt;-round(ShinyItemAnalysis::ItemAnalysis(my_data),2)\n#to see all the output in a table:\nknitr::kable(item_stats)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDifficulty\nMean\nSD\nCut.score\nobs.min\nMin.score\nobs.max\nMax.score\nProp.max.score\nRIR\nRIT\nCorr.criterion\nULI\ngULI\nAlpha.drop\nIndex.rel\nIndex.val\nPerc.miss\nPerc.nr\n\n\n\ni1\n0.21\n0.21\n0.41\nNA\n0\n0\n1\n1\n0.21\n0.34\n0.40\nNA\n0.35\nNA\n0.83\n0.16\nNA\n0\n0\n\n\ni2\n0.92\n0.92\n0.27\nNA\n0\n0\n1\n1\n0.92\n0.17\n0.22\nNA\n0.11\nNA\n0.84\n0.06\nNA\n0\n0\n\n\ni3\n0.91\n0.91\n0.29\nNA\n0\n0\n1\n1\n0.91\n0.18\n0.23\nNA\n0.13\nNA\n0.84\n0.07\nNA\n0\n0\n\n\ni4\n0.78\n0.78\n0.41\nNA\n0\n0\n1\n1\n0.78\n0.39\n0.45\nNA\n0.43\nNA\n0.83\n0.19\nNA\n0\n0\n\n\ni5\n0.11\n0.11\n0.32\nNA\n0\n0\n1\n1\n0.11\n0.25\n0.30\nNA\n0.20\nNA\n0.84\n0.10\nNA\n0\n0\n\n\ni6\n0.39\n0.39\n0.49\nNA\n0\n0\n1\n1\n0.39\n0.42\n0.48\nNA\n0.58\nNA\n0.83\n0.24\nNA\n0\n0\n\n\ni7\n0.74\n0.74\n0.44\nNA\n0\n0\n1\n1\n0.74\n0.35\n0.41\nNA\n0.40\nNA\n0.83\n0.18\nNA\n0\n0\n\n\ni8\n0.40\n0.40\n0.49\nNA\n0\n0\n1\n1\n0.40\n0.38\n0.45\nNA\n0.56\nNA\n0.83\n0.22\nNA\n0\n0\n\n\ni9\n0.86\n0.86\n0.35\nNA\n0\n0\n1\n1\n0.86\n0.22\n0.28\nNA\n0.20\nNA\n0.84\n0.10\nNA\n0\n0\n\n\ni10\n0.87\n0.87\n0.34\nNA\n0\n0\n1\n1\n0.87\n0.28\n0.33\nNA\n0.27\nNA\n0.83\n0.11\nNA\n0\n0\n\n\ni11\n0.77\n0.77\n0.42\nNA\n0\n0\n1\n1\n0.77\n0.39\n0.45\nNA\n0.44\nNA\n0.83\n0.19\nNA\n0\n0\n\n\ni12\n0.39\n0.39\n0.49\nNA\n0\n0\n1\n1\n0.39\n0.45\n0.51\nNA\n0.57\nNA\n0.83\n0.25\nNA\n0\n0\n\n\ni13\n0.88\n0.88\n0.32\nNA\n0\n0\n1\n1\n0.88\n0.22\n0.27\nNA\n0.22\nNA\n0.84\n0.09\nNA\n0\n0\n\n\ni14\n0.68\n0.68\n0.47\nNA\n0\n0\n1\n1\n0.68\n0.36\n0.43\nNA\n0.51\nNA\n0.83\n0.20\nNA\n0\n0\n\n\ni15\n0.59\n0.59\n0.49\nNA\n0\n0\n1\n1\n0.59\n0.37\n0.44\nNA\n0.50\nNA\n0.83\n0.22\nNA\n0\n0\n\n\ni16\n0.93\n0.93\n0.26\nNA\n0\n0\n1\n1\n0.93\n0.15\n0.20\nNA\n0.11\nNA\n0.84\n0.05\nNA\n0\n0\n\n\ni17\n0.72\n0.72\n0.45\nNA\n0\n0\n1\n1\n0.72\n0.36\n0.43\nNA\n0.46\nNA\n0.83\n0.19\nNA\n0\n0\n\n\ni18\n0.38\n0.38\n0.49\nNA\n0\n0\n1\n1\n0.38\n0.34\n0.41\nNA\n0.44\nNA\n0.83\n0.20\nNA\n0\n0\n\n\ni19\n0.17\n0.17\n0.38\nNA\n0\n0\n1\n1\n0.17\n0.27\n0.33\nNA\n0.26\nNA\n0.84\n0.13\nNA\n0\n0\n\n\ni20\n0.35\n0.35\n0.48\nNA\n0\n0\n1\n1\n0.35\n0.35\n0.42\nNA\n0.46\nNA\n0.83\n0.20\nNA\n0\n0\n\n\ni21\n0.51\n0.51\n0.50\nNA\n0\n0\n1\n1\n0.51\n0.42\n0.49\nNA\n0.57\nNA\n0.83\n0.25\nNA\n0\n0\n\n\ni22\n0.18\n0.18\n0.38\nNA\n0\n0\n1\n1\n0.18\n0.26\n0.32\nNA\n0.26\nNA\n0.84\n0.12\nNA\n0\n0\n\n\ni23\n0.10\n0.10\n0.30\nNA\n0\n0\n1\n1\n0.10\n0.27\n0.32\nNA\n0.20\nNA\n0.84\n0.10\nNA\n0\n0\n\n\ni24\n0.68\n0.68\n0.47\nNA\n0\n0\n1\n1\n0.68\n0.34\n0.41\nNA\n0.44\nNA\n0.83\n0.19\nNA\n0\n0\n\n\ni25\n0.90\n0.90\n0.30\nNA\n0\n0\n1\n1\n0.90\n0.28\n0.33\nNA\n0.21\nNA\n0.83\n0.10\nNA\n0\n0\n\n\ni26\n0.88\n0.88\n0.32\nNA\n0\n0\n1\n1\n0.88\n0.30\n0.35\nNA\n0.26\nNA\n0.83\n0.11\nNA\n0\n0\n\n\ni27\n0.26\n0.26\n0.44\nNA\n0\n0\n1\n1\n0.26\n0.31\n0.37\nNA\n0.36\nNA\n0.83\n0.16\nNA\n0\n0\n\n\ni28\n0.54\n0.54\n0.50\nNA\n0\n0\n1\n1\n0.54\n0.42\n0.49\nNA\n0.60\nNA\n0.83\n0.24\nNA\n0\n0\n\n\ni29\n0.15\n0.15\n0.36\nNA\n0\n0\n1\n1\n0.15\n0.19\n0.25\nNA\n0.18\nNA\n0.84\n0.09\nNA\n0\n0\n\n\ni30\n0.04\n0.04\n0.21\nNA\n0\n0\n1\n1\n0.04\n0.19\n0.23\nNA\n0.08\nNA\n0.84\n0.05\nNA\n0\n0\n\n\ni31\n0.05\n0.05\n0.21\nNA\n0\n0\n1\n1\n0.05\n0.14\n0.18\nNA\n0.08\nNA\n0.84\n0.04\nNA\n0\n0\n\n\ni32\n0.92\n0.92\n0.27\nNA\n0\n0\n1\n1\n0.92\n0.31\n0.35\nNA\n0.19\nNA\n0.83\n0.10\nNA\n0\n0\n\n\ni33\n0.64\n0.64\n0.48\nNA\n0\n0\n1\n1\n0.64\n0.34\n0.41\nNA\n0.46\nNA\n0.83\n0.20\nNA\n0\n0\n\n\ni34\n0.95\n0.95\n0.23\nNA\n0\n0\n1\n1\n0.95\n0.25\n0.28\nNA\n0.13\nNA\n0.84\n0.06\nNA\n0\n0\n\n\ni35\n0.61\n0.61\n0.49\nNA\n0\n0\n1\n1\n0.61\n0.31\n0.39\nNA\n0.42\nNA\n0.83\n0.19\nNA\n0\n0\n\n\ni36\n0.80\n0.80\n0.40\nNA\n0\n0\n1\n1\n0.80\n0.33\n0.39\nNA\n0.38\nNA\n0.83\n0.16\nNA\n0\n0\n\n\ni37\n0.15\n0.15\n0.35\nNA\n0\n0\n1\n1\n0.15\n0.34\n0.40\nNA\n0.30\nNA\n0.83\n0.14\nNA\n0\n0\n\n\ni38\n0.57\n0.57\n0.50\nNA\n0\n0\n1\n1\n0.57\n0.43\n0.50\nNA\n0.55\nNA\n0.83\n0.25\nNA\n0\n0\n\n\ni39\n0.93\n0.93\n0.26\nNA\n0\n0\n1\n1\n0.93\n0.24\n0.29\nNA\n0.13\nNA\n0.84\n0.07\nNA\n0\n0\n\n\ni40\n0.95\n0.95\n0.21\nNA\n0\n0\n1\n1\n0.95\n0.15\n0.19\nNA\n0.10\nNA\n0.84\n0.04\nNA\n0\n0\n\n\n\n\n\nThe same package also provides us with a nice visualization function (DDplot) for item difficulty and discrimination of any approach stated above. It takes discrim argument that can be defined as RIT (the first approach), RIR (the second approach above) or ULI (the third approach). Also, you can define a threshold value to draw a line on the plot via the thr argument. Here is a sample usage for our case:\n\nDDplot(my_data, discrim = 'ULI', k = 3, l = 1, u = 3, thr=0.1)\n\n\n\n\n\n\n\nIt can be seen that items 30, 31 and 40 from our simulated dataset are below our 0.1 threshold value in terms of discrimination. Interestingly, items 30 and 31 are the most difficult items while item 40 is the easiest one. What a weirdo… :D"
  },
  {
    "objectID": "posts/Exploring the psychometric properties using R/index.html#conclusion",
    "href": "posts/Exploring the psychometric properties using R/index.html#conclusion",
    "title": "Item difficulty & discrimination: Exploring the psychometric properties using R",
    "section": "Conclusion",
    "text": "Conclusion\nExamining item difficulty and item discrimination are important aspects of evaluating the psychometric properties of a measure. Item difficulty measures how easy or difficult each individual item is for respondents to answer correctly, while item discrimination measures the extent to which each item differentiates between participants who have high or low levels of the construct being measured.\nIn this blog post, we explored how to calculate item difficulty and item discrimination in R using an example dataset. We explained what each of these psychometric properties are, why they’re important, and how to interpret the results. We also discussed some limitations and considerations when examining these properties.\nOverall, understanding and evaluating the psychometric properties of a measure can help ensure its reliability and validity, and inform decisions about item selection and revision."
  },
  {
    "objectID": "posts/Measuerement Invariance/index.html",
    "href": "posts/Measuerement Invariance/index.html",
    "title": "Measurement Invariance",
    "section": "",
    "text": "Measurement invariance is critical because if a test is not invariant across groups, differences in test scores might reflect biases in how questions are interpreted. For instance, a math test may appear to show that boys score higher than girls, but this could be because certain items function differently for each group.\nThere are different levels of MI:\nConfigural Invariance: Tests whether the overall factor structure (i.e., the number and pattern of factors) is the same across groups. It is the smallest restrictive form of invariance. It allows us to conclude that the groups conceptualize the construct in the same way.\nMetric (Weak) Invariance: Tests whether the factor loadings (the strength of the relationship between each item and the latent factor) are equal across groups. This ensures that the items are equally good indicators of the latent construct in all groups.\nScalar (Strong) Invariance: Tests whether item intercepts are equal across groups. Scalar invariance is necessary for comparing latent means between groups.\nStrict Invariance: Tests whether item residual variances are equal across groups. It’s the strongest form of invariance, implying that the amount of measurement error is consistent across groups.\nIn the context of R, we can use structural equation modeling (SEM) to assess MI. The lavaan package provides a dataset (HolzingerSwineford1939) for several SEM analysis including MI. Let’s load the lavaan and HolzingerSwineford1939 data along with the semTools package for visualization.\nCode# requirements\nlibrary(\"lavaan\")\nlibrary(\"semPlot\")# For additional tools related to SEM\nlibrary(\"ggplot2\")\nlibrary(\"semTools\")"
  },
  {
    "objectID": "posts/Measuerement Invariance/index.html#introduction",
    "href": "posts/Measuerement Invariance/index.html#introduction",
    "title": "Measurement Invariance",
    "section": "",
    "text": "Measurement invariance is critical because if a test is not invariant across groups, differences in test scores might reflect biases in how questions are interpreted. For instance, a math test may appear to show that boys score higher than girls, but this could be because certain items function differently for each group.\nThere are different levels of MI:\nConfigural Invariance: Tests whether the overall factor structure (i.e., the number and pattern of factors) is the same across groups. It is the smallest restrictive form of invariance. It allows us to conclude that the groups conceptualize the construct in the same way.\nMetric (Weak) Invariance: Tests whether the factor loadings (the strength of the relationship between each item and the latent factor) are equal across groups. This ensures that the items are equally good indicators of the latent construct in all groups.\nScalar (Strong) Invariance: Tests whether item intercepts are equal across groups. Scalar invariance is necessary for comparing latent means between groups.\nStrict Invariance: Tests whether item residual variances are equal across groups. It’s the strongest form of invariance, implying that the amount of measurement error is consistent across groups.\nIn the context of R, we can use structural equation modeling (SEM) to assess MI. The lavaan package provides a dataset (HolzingerSwineford1939) for several SEM analysis including MI. Let’s load the lavaan and HolzingerSwineford1939 data along with the semTools package for visualization.\nCode# requirements\nlibrary(\"lavaan\")\nlibrary(\"semPlot\")# For additional tools related to SEM\nlibrary(\"ggplot2\")\nlibrary(\"semTools\")"
  },
  {
    "objectID": "posts/Measuerement Invariance/index.html#understand-the-data",
    "href": "posts/Measuerement Invariance/index.html#understand-the-data",
    "title": "Measurement Invariance",
    "section": "1. Understand the data",
    "text": "1. Understand the data\nLet’s load the data and see the head of them:\n\n\nCode\n\n\ndata(\"HolzingerSwineford1939\")\nhead(HolzingerSwineford1939)\n\n\n\n  id sex ageyr agemo  school grade       x1   x2    x3       x4   x5        x6\n1  1   1    13     1 Pasteur     7 3.333333 7.75 0.375 2.333333 5.75 1.2857143\n2  2   2    13     7 Pasteur     7 5.333333 5.25 2.125 1.666667 3.00 1.2857143\n3  3   2    13     1 Pasteur     7 4.500000 5.25 1.875 1.000000 1.75 0.4285714\n4  4   1    13     2 Pasteur     7 5.333333 7.75 3.000 2.666667 4.50 2.4285714\n5  5   2    12     2 Pasteur     7 4.833333 4.75 0.875 2.666667 4.00 2.5714286\n6  6   2    14     1 Pasteur     7 5.333333 5.00 2.250 1.000000 3.00 0.8571429\n        x7   x8       x9\n1 3.391304 5.75 6.361111\n2 3.782609 6.25 7.916667\n3 3.260870 3.90 4.416667\n4 3.000000 5.30 4.861111\n5 3.695652 6.30 5.916667\n6 4.347826 6.65 7.500000\n\n\nThe Holzinger and Swineford dataset contains data on students’ cognitive abilities, including variables like sex, age, and grade. It also includes information about the school of students. It includes several cognitive test scores (variables x1 to x9) that measure different abilities, which can be used to examine latent traits. These variables and code for the model is provided in the package’s own paper. The dataset measures three latent factors:\n\nVisual (x1, x2, x3),\nTextual (x4, x5, x6),\nSpeed (x7, x8, x9)."
  },
  {
    "objectID": "posts/Measuerement Invariance/index.html#fit-the-base-model",
    "href": "posts/Measuerement Invariance/index.html#fit-the-base-model",
    "title": "Measurement Invariance",
    "section": "2. Fit the base model",
    "text": "2. Fit the base model\nLet’s specify the CFA model.\n\n\nCode\n\n\nHS.model &lt;- ' visual  =~ x1 + x2 + x3\n              textual =~ x4 + x5 + x6\n              speed   =~ x7 + x8 + x9 '\n\n\n\nWe fit the model using the entire dataset without considering groups. This step provides a baseline understanding of how well the model fits:\n\n\nCode\n\n\nfit &lt;- lavaan(HS.model, data = HolzingerSwineford1939, \n              auto.var = TRUE, auto.fix.first = TRUE,\n              auto.cov.lv.x = TRUE)\nsummary(fit, fit.measures = TRUE)\n\n\n\nlavaan 0.6-19 ended normally after 35 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        21\n\n  Number of observations                           301\n\nModel Test User Model:\n                                                      \n  Test statistic                                85.306\n  Degrees of freedom                                24\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                               918.852\n  Degrees of freedom                                36\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.931\n  Tucker-Lewis Index (TLI)                       0.896\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -3737.745\n  Loglikelihood unrestricted model (H1)      -3695.092\n                                                      \n  Akaike (AIC)                                7517.490\n  Bayesian (BIC)                              7595.339\n  Sample-size adjusted Bayesian (SABIC)       7528.739\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.092\n  90 Percent confidence interval - lower         0.071\n  90 Percent confidence interval - upper         0.114\n  P-value H_0: RMSEA &lt;= 0.050                    0.001\n  P-value H_0: RMSEA &gt;= 0.080                    0.840\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.065\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  visual =~                                           \n    x1                1.000                           \n    x2                0.554    0.100    5.554    0.000\n    x3                0.729    0.109    6.685    0.000\n  textual =~                                          \n    x4                1.000                           \n    x5                1.113    0.065   17.014    0.000\n    x6                0.926    0.055   16.703    0.000\n  speed =~                                            \n    x7                1.000                           \n    x8                1.180    0.165    7.152    0.000\n    x9                1.082    0.151    7.155    0.000\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  visual ~~                                           \n    textual           0.408    0.074    5.552    0.000\n    speed             0.262    0.056    4.660    0.000\n  textual ~~                                          \n    speed             0.173    0.049    3.518    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .x1                0.549    0.114    4.833    0.000\n   .x2                1.134    0.102   11.146    0.000\n   .x3                0.844    0.091    9.317    0.000\n   .x4                0.371    0.048    7.779    0.000\n   .x5                0.446    0.058    7.642    0.000\n   .x6                0.356    0.043    8.277    0.000\n   .x7                0.799    0.081    9.823    0.000\n   .x8                0.488    0.074    6.573    0.000\n   .x9                0.566    0.071    8.003    0.000\n    visual            0.809    0.145    5.564    0.000\n    textual           0.979    0.112    8.737    0.000\n    speed             0.384    0.086    4.451    0.000\n\n\n\nChi-Square Test (85.306, df = 24, p &lt; 0.001): The significant result indicates that the model doesn’t perfectly fit the data, but Chi-square is highly sensitive to sample size.\nCFI (0.931): This value suggests a reasonably good fit (values above 0.90 are typically considered acceptable).\nRMSEA (0.092): The RMSEA is below the threshold for a good fit (&lt;0.10).\nSRMR (0.065): This is below 0.10.\n\nAll latent variables (visual, textual, speed) show significant factor loadings for their respective observed variables (e.g., x1–x3 for visual), indicating that these items effectively measure their intended constructs.\nCovariances between latent variables (visual, textual, speed) are significant, indicating meaningful relationships between these cognitive abilities."
  },
  {
    "objectID": "posts/Measuerement Invariance/index.html#measurement-invariance",
    "href": "posts/Measuerement Invariance/index.html#measurement-invariance",
    "title": "Measurement Invariance",
    "section": "3. Measurement Invariance",
    "text": "3. Measurement Invariance\nNow we move on to measurement invariance.\n3.1. Configural Invariance\nThe first step is testing configural invariance, which checks whether the factor structure (i.e., the number of factors and their loadings) is the same across groups. We’ll use school as the grouping variable:\n\n\nCode\n\n\nfit_configural &lt;- cfa(HS.model, data = HolzingerSwineford1939, group = \"school\")\nsummary(fit_configural, fit.measures = TRUE)\n\n\n\nlavaan 0.6-19 ended normally after 57 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        60\n\n  Number of observations per group:                   \n    Pasteur                                        156\n    Grant-White                                    145\n\nModel Test User Model:\n                                                      \n  Test statistic                               115.851\n  Degrees of freedom                                48\n  P-value (Chi-square)                           0.000\n  Test statistic for each group:\n    Pasteur                                     64.309\n    Grant-White                                 51.542\n\nModel Test Baseline Model:\n\n  Test statistic                               957.769\n  Degrees of freedom                                72\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.923\n  Tucker-Lewis Index (TLI)                       0.885\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -3682.198\n  Loglikelihood unrestricted model (H1)      -3624.272\n                                                      \n  Akaike (AIC)                                7484.395\n  Bayesian (BIC)                              7706.822\n  Sample-size adjusted Bayesian (SABIC)       7516.536\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.097\n  90 Percent confidence interval - lower         0.075\n  90 Percent confidence interval - upper         0.120\n  P-value H_0: RMSEA &lt;= 0.050                    0.001\n  P-value H_0: RMSEA &gt;= 0.080                    0.897\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.068\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\n\nGroup 1 [Pasteur]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  visual =~                                           \n    x1                1.000                           \n    x2                0.394    0.122    3.220    0.001\n    x3                0.570    0.140    4.076    0.000\n  textual =~                                          \n    x4                1.000                           \n    x5                1.183    0.102   11.613    0.000\n    x6                0.875    0.077   11.421    0.000\n  speed =~                                            \n    x7                1.000                           \n    x8                1.125    0.277    4.057    0.000\n    x9                0.922    0.225    4.104    0.000\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  visual ~~                                           \n    textual           0.479    0.106    4.531    0.000\n    speed             0.185    0.077    2.397    0.017\n  textual ~~                                          \n    speed             0.182    0.069    2.628    0.009\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .x1                4.941    0.095   52.249    0.000\n   .x2                5.984    0.098   60.949    0.000\n   .x3                2.487    0.093   26.778    0.000\n   .x4                2.823    0.092   30.689    0.000\n   .x5                3.995    0.105   38.183    0.000\n   .x6                1.922    0.079   24.321    0.000\n   .x7                4.432    0.087   51.181    0.000\n   .x8                5.563    0.078   71.214    0.000\n   .x9                5.418    0.079   68.440    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .x1                0.298    0.232    1.286    0.198\n   .x2                1.334    0.158    8.464    0.000\n   .x3                0.989    0.136    7.271    0.000\n   .x4                0.425    0.069    6.138    0.000\n   .x5                0.456    0.086    5.292    0.000\n   .x6                0.290    0.050    5.780    0.000\n   .x7                0.820    0.125    6.580    0.000\n   .x8                0.510    0.116    4.406    0.000\n   .x9                0.680    0.104    6.516    0.000\n    visual            1.097    0.276    3.967    0.000\n    textual           0.894    0.150    5.963    0.000\n    speed             0.350    0.126    2.778    0.005\n\n\nGroup 2 [Grant-White]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  visual =~                                           \n    x1                1.000                           \n    x2                0.736    0.155    4.760    0.000\n    x3                0.925    0.166    5.583    0.000\n  textual =~                                          \n    x4                1.000                           \n    x5                0.990    0.087   11.418    0.000\n    x6                0.963    0.085   11.377    0.000\n  speed =~                                            \n    x7                1.000                           \n    x8                1.226    0.187    6.569    0.000\n    x9                1.058    0.165    6.429    0.000\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  visual ~~                                           \n    textual           0.408    0.098    4.153    0.000\n    speed             0.276    0.076    3.639    0.000\n  textual ~~                                          \n    speed             0.222    0.073    3.022    0.003\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .x1                4.930    0.095   51.696    0.000\n   .x2                6.200    0.092   67.416    0.000\n   .x3                1.996    0.086   23.195    0.000\n   .x4                3.317    0.093   35.625    0.000\n   .x5                4.712    0.096   48.986    0.000\n   .x6                2.469    0.094   26.277    0.000\n   .x7                3.921    0.086   45.819    0.000\n   .x8                5.488    0.087   63.174    0.000\n   .x9                5.327    0.085   62.571    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .x1                0.715    0.126    5.676    0.000\n   .x2                0.899    0.123    7.339    0.000\n   .x3                0.557    0.103    5.409    0.000\n   .x4                0.315    0.065    4.870    0.000\n   .x5                0.419    0.072    5.812    0.000\n   .x6                0.406    0.069    5.880    0.000\n   .x7                0.600    0.091    6.584    0.000\n   .x8                0.401    0.094    4.249    0.000\n   .x9                0.535    0.089    6.010    0.000\n    visual            0.604    0.160    3.762    0.000\n    textual           0.942    0.152    6.177    0.000\n    speed             0.461    0.118    3.910    0.000\n\n\nThe configural invariance model provides a baseline for comparing the factor structure across groups (schools: Pasteur and Grant-White).\n\n\nFit Indices:\n\nThe CFI of 0.923 and TLI of 0.885 suggest a reasonably good fit but not excellent.\nThe RMSEA of 0.097 is slightly below the recommended 0.10 threshold.\nThe SRMR of 0.068 is within an acceptable range, below 0.10, indicating a good fit.\n\n\n\nThese results show that the overall factor structure is consistent across the two schools, but there’s room for improvement in model fit.\n3.2. Metric Invariance\nNext, we would proceed to test metric invariance, where we constrain the factor loadings across groups to assess if the model behaves equivalently in both schools.\n\n\nCode\n\n\nfit_metric &lt;- cfa(HS.model, data = HolzingerSwineford1939, group = \"school\", group.equal = \"loadings\")\nsummary(fit_metric, fit.measures = TRUE)\n\n\n\nlavaan 0.6-19 ended normally after 42 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        60\n  Number of equality constraints                     6\n\n  Number of observations per group:                   \n    Pasteur                                        156\n    Grant-White                                    145\n\nModel Test User Model:\n                                                      \n  Test statistic                               124.044\n  Degrees of freedom                                54\n  P-value (Chi-square)                           0.000\n  Test statistic for each group:\n    Pasteur                                     68.825\n    Grant-White                                 55.219\n\nModel Test Baseline Model:\n\n  Test statistic                               957.769\n  Degrees of freedom                                72\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.921\n  Tucker-Lewis Index (TLI)                       0.895\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -3686.294\n  Loglikelihood unrestricted model (H1)      -3624.272\n                                                      \n  Akaike (AIC)                                7480.587\n  Bayesian (BIC)                              7680.771\n  Sample-size adjusted Bayesian (SABIC)       7509.514\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.093\n  90 Percent confidence interval - lower         0.071\n  90 Percent confidence interval - upper         0.114\n  P-value H_0: RMSEA &lt;= 0.050                    0.001\n  P-value H_0: RMSEA &gt;= 0.080                    0.845\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.072\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\n\nGroup 1 [Pasteur]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  visual =~                                           \n    x1                1.000                           \n    x2      (.p2.)    0.599    0.100    5.979    0.000\n    x3      (.p3.)    0.784    0.108    7.267    0.000\n  textual =~                                          \n    x4                1.000                           \n    x5      (.p5.)    1.083    0.067   16.049    0.000\n    x6      (.p6.)    0.912    0.058   15.785    0.000\n  speed =~                                            \n    x7                1.000                           \n    x8      (.p8.)    1.201    0.155    7.738    0.000\n    x9      (.p9.)    1.038    0.136    7.629    0.000\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  visual ~~                                           \n    textual           0.416    0.097    4.271    0.000\n    speed             0.169    0.064    2.643    0.008\n  textual ~~                                          \n    speed             0.176    0.061    2.882    0.004\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .x1                4.941    0.093   52.991    0.000\n   .x2                5.984    0.100   60.096    0.000\n   .x3                2.487    0.094   26.465    0.000\n   .x4                2.823    0.093   30.371    0.000\n   .x5                3.995    0.101   39.714    0.000\n   .x6                1.922    0.081   23.711    0.000\n   .x7                4.432    0.086   51.540    0.000\n   .x8                5.563    0.078   71.087    0.000\n   .x9                5.418    0.079   68.153    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .x1                0.551    0.137    4.010    0.000\n   .x2                1.258    0.155    8.117    0.000\n   .x3                0.882    0.128    6.884    0.000\n   .x4                0.434    0.070    6.238    0.000\n   .x5                0.508    0.082    6.229    0.000\n   .x6                0.266    0.050    5.294    0.000\n   .x7                0.849    0.114    7.468    0.000\n   .x8                0.515    0.095    5.409    0.000\n   .x9                0.658    0.096    6.865    0.000\n    visual            0.805    0.171    4.714    0.000\n    textual           0.913    0.137    6.651    0.000\n    speed             0.305    0.078    3.920    0.000\n\n\nGroup 2 [Grant-White]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  visual =~                                           \n    x1                1.000                           \n    x2      (.p2.)    0.599    0.100    5.979    0.000\n    x3      (.p3.)    0.784    0.108    7.267    0.000\n  textual =~                                          \n    x4                1.000                           \n    x5      (.p5.)    1.083    0.067   16.049    0.000\n    x6      (.p6.)    0.912    0.058   15.785    0.000\n  speed =~                                            \n    x7                1.000                           \n    x8      (.p8.)    1.201    0.155    7.738    0.000\n    x9      (.p9.)    1.038    0.136    7.629    0.000\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  visual ~~                                           \n    textual           0.437    0.099    4.423    0.000\n    speed             0.314    0.079    3.958    0.000\n  textual ~~                                          \n    speed             0.226    0.072    3.144    0.002\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .x1                4.930    0.097   50.763    0.000\n   .x2                6.200    0.091   68.379    0.000\n   .x3                1.996    0.085   23.455    0.000\n   .x4                3.317    0.092   35.950    0.000\n   .x5                4.712    0.100   47.173    0.000\n   .x6                2.469    0.091   27.248    0.000\n   .x7                3.921    0.086   45.555    0.000\n   .x8                5.488    0.087   63.257    0.000\n   .x9                5.327    0.085   62.786    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .x1                0.645    0.127    5.084    0.000\n   .x2                0.933    0.121    7.732    0.000\n   .x3                0.605    0.096    6.282    0.000\n   .x4                0.329    0.062    5.279    0.000\n   .x5                0.384    0.073    5.270    0.000\n   .x6                0.437    0.067    6.576    0.000\n   .x7                0.599    0.090    6.651    0.000\n   .x8                0.406    0.089    4.541    0.000\n   .x9                0.532    0.086    6.202    0.000\n    visual            0.722    0.161    4.490    0.000\n    textual           0.906    0.136    6.646    0.000\n    speed             0.475    0.109    4.347    0.000\n\n\n\nCFI (0.921) and TLI (0.895) are still relatively good, indicating that the constrained model is acceptable.\nRMSEA (0.093) suggests an acceptable fit.\nSRMR (0.072) is still below 0.08, indicating acceptable fit.\n\nThe comparison between the configural and metric models shows a slight decline in fit, but the invariance model still seems reasonably supported. This indicates that the factor loadings are equivalent across the two schools, allowing us to meaningfully compare relationships between items and latent factors.\n3.3. Scalar Invariance\nNext, we would proceed with testing scalar invariance.\n\n\nCode\n\n\nfit_scalar &lt;- cfa(HS.model, data = HolzingerSwineford1939, group = \"school\", group.equal = c(\"loadings\", \"intercepts\"))\nsummary(fit_scalar, fit.measures = TRUE)\n\n\n\nlavaan 0.6-19 ended normally after 60 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        63\n  Number of equality constraints                    15\n\n  Number of observations per group:                   \n    Pasteur                                        156\n    Grant-White                                    145\n\nModel Test User Model:\n                                                      \n  Test statistic                               164.103\n  Degrees of freedom                                60\n  P-value (Chi-square)                           0.000\n  Test statistic for each group:\n    Pasteur                                     90.210\n    Grant-White                                 73.892\n\nModel Test Baseline Model:\n\n  Test statistic                               957.769\n  Degrees of freedom                                72\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.882\n  Tucker-Lewis Index (TLI)                       0.859\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -3706.323\n  Loglikelihood unrestricted model (H1)      -3624.272\n                                                      \n  Akaike (AIC)                                7508.647\n  Bayesian (BIC)                              7686.588\n  Sample-size adjusted Bayesian (SABIC)       7534.359\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.107\n  90 Percent confidence interval - lower         0.088\n  90 Percent confidence interval - upper         0.127\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.989\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.082\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\n\nGroup 1 [Pasteur]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  visual =~                                           \n    x1                1.000                           \n    x2      (.p2.)    0.576    0.101    5.713    0.000\n    x3      (.p3.)    0.798    0.112    7.146    0.000\n  textual =~                                          \n    x4                1.000                           \n    x5      (.p5.)    1.120    0.066   16.965    0.000\n    x6      (.p6.)    0.932    0.056   16.608    0.000\n  speed =~                                            \n    x7                1.000                           \n    x8      (.p8.)    1.130    0.145    7.786    0.000\n    x9      (.p9.)    1.009    0.132    7.667    0.000\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  visual ~~                                           \n    textual           0.410    0.095    4.293    0.000\n    speed             0.178    0.066    2.687    0.007\n  textual ~~                                          \n    speed             0.180    0.062    2.900    0.004\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .x1      (.25.)    5.001    0.090   55.760    0.000\n   .x2      (.26.)    6.151    0.077   79.905    0.000\n   .x3      (.27.)    2.271    0.083   27.387    0.000\n   .x4      (.28.)    2.778    0.087   31.953    0.000\n   .x5      (.29.)    4.035    0.096   41.858    0.000\n   .x6      (.30.)    1.926    0.079   24.426    0.000\n   .x7      (.31.)    4.242    0.073   57.975    0.000\n   .x8      (.32.)    5.630    0.072   78.531    0.000\n   .x9      (.33.)    5.465    0.069   79.016    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .x1                0.555    0.139    3.983    0.000\n   .x2                1.296    0.158    8.186    0.000\n   .x3                0.944    0.136    6.929    0.000\n   .x4                0.445    0.069    6.430    0.000\n   .x5                0.502    0.082    6.136    0.000\n   .x6                0.263    0.050    5.264    0.000\n   .x7                0.888    0.120    7.416    0.000\n   .x8                0.541    0.095    5.706    0.000\n   .x9                0.654    0.096    6.805    0.000\n    visual            0.796    0.172    4.641    0.000\n    textual           0.879    0.131    6.694    0.000\n    speed             0.322    0.082    3.914    0.000\n\n\nGroup 2 [Grant-White]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  visual =~                                           \n    x1                1.000                           \n    x2      (.p2.)    0.576    0.101    5.713    0.000\n    x3      (.p3.)    0.798    0.112    7.146    0.000\n  textual =~                                          \n    x4                1.000                           \n    x5      (.p5.)    1.120    0.066   16.965    0.000\n    x6      (.p6.)    0.932    0.056   16.608    0.000\n  speed =~                                            \n    x7                1.000                           \n    x8      (.p8.)    1.130    0.145    7.786    0.000\n    x9      (.p9.)    1.009    0.132    7.667    0.000\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  visual ~~                                           \n    textual           0.427    0.097    4.417    0.000\n    speed             0.329    0.082    4.006    0.000\n  textual ~~                                          \n    speed             0.236    0.073    3.224    0.001\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .x1      (.25.)    5.001    0.090   55.760    0.000\n   .x2      (.26.)    6.151    0.077   79.905    0.000\n   .x3      (.27.)    2.271    0.083   27.387    0.000\n   .x4      (.28.)    2.778    0.087   31.953    0.000\n   .x5      (.29.)    4.035    0.096   41.858    0.000\n   .x6      (.30.)    1.926    0.079   24.426    0.000\n   .x7      (.31.)    4.242    0.073   57.975    0.000\n   .x8      (.32.)    5.630    0.072   78.531    0.000\n   .x9      (.33.)    5.465    0.069   79.016    0.000\n    visual           -0.148    0.122   -1.211    0.226\n    textual           0.576    0.117    4.918    0.000\n    speed            -0.177    0.090   -1.968    0.049\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .x1                0.654    0.128    5.094    0.000\n   .x2                0.964    0.123    7.812    0.000\n   .x3                0.641    0.101    6.316    0.000\n   .x4                0.343    0.062    5.534    0.000\n   .x5                0.376    0.073    5.133    0.000\n   .x6                0.437    0.067    6.559    0.000\n   .x7                0.625    0.095    6.574    0.000\n   .x8                0.434    0.088    4.914    0.000\n   .x9                0.522    0.086    6.102    0.000\n    visual            0.708    0.160    4.417    0.000\n    textual           0.870    0.131    6.659    0.000\n    speed             0.505    0.115    4.379    0.000\n\n\nThe scalar invariance model (which adds constraints on intercepts) shows the following:\n\nFit indices: The CFI has dropped to 0.882 and TLI to 0.859, indicating a lower fit compared to the metric model. RMSEA increased to 0.107, which exceeds the acceptable threshold (0.10), suggesting a less satisfactory fit.\nSRMR is now 0.082.\n\nOverall, the model fit deteriorates, indicating that the intercepts might not be fully invariant across the two groups.\n3.4. Strict Invariance\nFinally lets test the strict invariance.\n\n\nCode\n\n\nfit_strict &lt;- cfa(HS.model, data = HolzingerSwineford1939, group = \"school\", group.equal = c(\"loadings\", \"intercepts\", \"residuals\"))\nsummary(fit_strict, fit.measures = TRUE)\n\n\n\nlavaan 0.6-19 ended normally after 59 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        63\n  Number of equality constraints                    24\n\n  Number of observations per group:                   \n    Pasteur                                        156\n    Grant-White                                    145\n\nModel Test User Model:\n                                                      \n  Test statistic                               181.511\n  Degrees of freedom                                69\n  P-value (Chi-square)                           0.000\n  Test statistic for each group:\n    Pasteur                                     93.093\n    Grant-White                                 88.419\n\nModel Test Baseline Model:\n\n  Test statistic                               957.769\n  Degrees of freedom                                72\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.873\n  Tucker-Lewis Index (TLI)                       0.867\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -3715.028\n  Loglikelihood unrestricted model (H1)      -3624.272\n                                                      \n  Akaike (AIC)                                7508.055\n  Bayesian (BIC)                              7652.632\n  Sample-size adjusted Bayesian (SABIC)       7528.947\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.104\n  90 Percent confidence interval - lower         0.086\n  90 Percent confidence interval - upper         0.123\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.984\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.088\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\n\nGroup 1 [Pasteur]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  visual =~                                           \n    x1                1.000                           \n    x2      (.p2.)    0.591    0.104    5.691    0.000\n    x3      (.p3.)    0.837    0.116    7.182    0.000\n  textual =~                                          \n    x4                1.000                           \n    x5      (.p5.)    1.125    0.066   17.134    0.000\n    x6      (.p6.)    0.933    0.056   16.752    0.000\n  speed =~                                            \n    x7                1.000                           \n    x8      (.p8.)    1.121    0.151    7.424    0.000\n    x9      (.p9.)    1.028    0.140    7.356    0.000\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  visual ~~                                           \n    textual           0.367    0.094    3.915    0.000\n    speed             0.174    0.065    2.666    0.008\n  textual ~~                                          \n    speed             0.176    0.062    2.827    0.005\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .x1      (.25.)    5.012    0.090   55.461    0.000\n   .x2      (.26.)    6.133    0.077   79.814    0.000\n   .x3      (.27.)    2.314    0.083   28.037    0.000\n   .x4      (.28.)    2.784    0.086   32.193    0.000\n   .x5      (.29.)    4.029    0.096   41.812    0.000\n   .x6      (.30.)    1.927    0.081   23.747    0.000\n   .x7      (.31.)    4.271    0.073   58.428    0.000\n   .x8      (.32.)    5.622    0.072   78.502    0.000\n   .x9      (.33.)    5.461    0.070   78.438    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .x1      (.10.)    0.638    0.102    6.249    0.000\n   .x2      (.11.)    1.130    0.102   11.124    0.000\n   .x3      (.12.)    0.771    0.090    8.608    0.000\n   .x4      (.13.)    0.383    0.047    8.095    0.000\n   .x5      (.14.)    0.435    0.057    7.616    0.000\n   .x6      (.15.)    0.354    0.042    8.341    0.000\n   .x7      (.16.)    0.769    0.080    9.571    0.000\n   .x8      (.17.)    0.501    0.071    7.021    0.000\n   .x9      (.18.)    0.576    0.069    8.353    0.000\n    visual            0.767    0.164    4.686    0.000\n    textual           0.894    0.131    6.827    0.000\n    speed             0.340    0.085    4.016    0.000\n\n\nGroup 2 [Grant-White]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  visual =~                                           \n    x1                1.000                           \n    x2      (.p2.)    0.591    0.104    5.691    0.000\n    x3      (.p3.)    0.837    0.116    7.182    0.000\n  textual =~                                          \n    x4                1.000                           \n    x5      (.p5.)    1.125    0.066   17.134    0.000\n    x6      (.p6.)    0.933    0.056   16.752    0.000\n  speed =~                                            \n    x7                1.000                           \n    x8      (.p8.)    1.121    0.151    7.424    0.000\n    x9      (.p9.)    1.028    0.140    7.356    0.000\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  visual ~~                                           \n    textual           0.422    0.095    4.446    0.000\n    speed             0.331    0.081    4.069    0.000\n  textual ~~                                          \n    speed             0.236    0.074    3.194    0.001\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .x1      (.25.)    5.012    0.090   55.461    0.000\n   .x2      (.26.)    6.133    0.077   79.814    0.000\n   .x3      (.27.)    2.314    0.083   28.037    0.000\n   .x4      (.28.)    2.784    0.086   32.193    0.000\n   .x5      (.29.)    4.029    0.096   41.812    0.000\n   .x6      (.30.)    1.927    0.081   23.747    0.000\n   .x7      (.31.)    4.271    0.073   58.428    0.000\n   .x8      (.32.)    5.622    0.072   78.502    0.000\n   .x9      (.33.)    5.461    0.070   78.438    0.000\n    visual           -0.157    0.120   -1.316    0.188\n    textual           0.575    0.118    4.888    0.000\n    speed            -0.176    0.090   -1.958    0.050\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .x1      (.10.)    0.638    0.102    6.249    0.000\n   .x2      (.11.)    1.130    0.102   11.124    0.000\n   .x3      (.12.)    0.771    0.090    8.608    0.000\n   .x4      (.13.)    0.383    0.047    8.095    0.000\n   .x5      (.14.)    0.435    0.057    7.616    0.000\n   .x6      (.15.)    0.354    0.042    8.341    0.000\n   .x7      (.16.)    0.769    0.080    9.571    0.000\n   .x8      (.17.)    0.501    0.071    7.021    0.000\n   .x9      (.18.)    0.576    0.069    8.353    0.000\n    visual            0.657    0.150    4.379    0.000\n    textual           0.876    0.132    6.621    0.000\n    speed             0.478    0.116    4.138    0.000\n\n\nFit Statistics:\n\nChi-square (181.511, df = 69, p &lt; 0.001): Significant, indicating that strict invariance does not hold perfectly, but Chi-square is sensitive to large sample sizes.\nCFI (0.873) and TLI (0.867): Both are below 0.90, indicating a moderate fit. These indices suggest some misfit when imposing strict invariance.\nRMSEA (0.104): Exceeds the desired threshold of 0.10, suggesting that the model fit could be improved.\nSRMR (0.088): Slightly below the 0.10 threshold.\n\nStrict invariance constrains residuals to be equal across groups. Although the fit is not ideal, it is common for strict invariance to show worse fit compared to less restrictive models."
  },
  {
    "objectID": "posts/Measuerement Invariance/index.html#evaluation",
    "href": "posts/Measuerement Invariance/index.html#evaluation",
    "title": "Measurement Invariance",
    "section": "4. Evaluation",
    "text": "4. Evaluation\nWe can all four models with anova() function:\n\n\nCode\n\n\nanova(fit_configural, fit_metric, fit_scalar, fit_strict)\n\n\n\n\nChi-Squared Difference Test\n\n               Df    AIC    BIC  Chisq Chisq diff    RMSEA Df diff Pr(&gt;Chisq)\nfit_configural 48 7484.4 7706.8 115.85                                       \nfit_metric     54 7480.6 7680.8 124.04      8.192 0.049272       6    0.22436\nfit_scalar     60 7508.6 7686.6 164.10     40.059 0.194211       6  4.435e-07\nfit_strict     69 7508.1 7652.6 181.51     17.409 0.078790       9    0.04269\n                  \nfit_configural    \nfit_metric        \nfit_scalar     ***\nfit_strict     *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nConfigural Model: This is the baseline with good fit (Chisq = 115.85).\nMetric Model: No significant difference from the configural model (p = 0.224), suggesting factor loadings are invariant across groups.\nScalar Model: Significant difference (p &lt; 0.001), implying that intercepts are not invariant across groups, indicating a potential bias.\nStrict Model: Marginally significant difference (p = 0.043), suggesting that residual variances also vary, reducing strict invariance.\n\nOverall, the scalar and strict invariance do not hold as strongly as the other two.\nLet’s investigate all models in a chart and see how the model fit metrics deteriorates after each checkpoint.\n\n\nCode\n\n\nfitMeasures_df &lt;- data.frame(\n  Model = c(\"Configural\", \"Metric\", \"Scalar\", \"Strict\"),\n  CFI = c(0.923, 0.921, 0.882, 0.873),\n  RMSEA = c(0.097, 0.093, 0.107, 0.104),\n  SRMR = c(0.068, 0.072, 0.082, 0.088)\n)\n# Plot with thresholds and legends\nggplot(fitMeasures_df, aes(x = Model)) +\n  geom_point(aes(y = CFI, color = \"CFI\"), size = 4) +\n  geom_point(aes(y = RMSEA, color = \"RMSEA\"), size = 4) +\n  geom_point(aes(y = SRMR, color = \"SRMR\"), size = 4) +\n  geom_line(aes(y = CFI, group = 1, color = \"CFI\")) +\n  geom_line(aes(y = RMSEA, group = 1, color = \"RMSEA\")) +\n  geom_line(aes(y = SRMR, group = 1, color = \"SRMR\")) +\n  \n  # Add threshold lines\n  geom_hline(yintercept = 0.90, linetype = \"dashed\", color = \"blue\", size = 0.5) +  # CFI threshold\n  geom_hline(yintercept = 0.10, linetype = \"dashed\", color = \"tomato\", size = 0.5) +   # RMSEA & SRMR threshold\n  \n  # Manual legend for threshold lines\n  annotate(\"text\", x = 4.5, y = 0.91, label = \"0.90\", color = \"blue\", size = 3.5, hjust = 1) +\n  annotate(\"text\", x = 4.5, y = 0.09, label = \"0.10\", color = \"tomato\", size = 3.5, hjust = 1) +\n  \n  labs(y = \"Fit Measures\", x = \"Model Type\", color = \"Fit Index\",\n       title = \"Comparison of Fit Measures Across Invariance Models with Thresholds\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nAs you see, configural and metric invariance is fulfilled as their CFI levels are above 0.90, and RMSEA & SRMR are below 0.10. Yet, scalar and strict invariance is slightly above the thresholds for RMSEA and SRMR and below for CFI.\nThese findings suggest some modifications might be a good idea on scalar and strict invariance models. Yet, model modifications are another blog post’s issue."
  },
  {
    "objectID": "posts/Measuerement Invariance/index.html#conclusion",
    "href": "posts/Measuerement Invariance/index.html#conclusion",
    "title": "Measurement Invariance",
    "section": "5. Conclusion",
    "text": "5. Conclusion\nIn this blog post, we explored the concept of measurement invariance and its importance in ensuring that a test measures the same construct across different sub-groups of the population. Using the lavaan package in R, we conducted a series of tests to assess different levels of MI — configural, metric, scalar, and strict invariance — on the Holzinger and Swineford dataset."
  },
  {
    "objectID": "posts/Measuerement Invariance/index.html#further-analysis",
    "href": "posts/Measuerement Invariance/index.html#further-analysis",
    "title": "Measurement Invariance",
    "section": "6. Further Analysis",
    "text": "6. Further Analysis\nAs scalar and strict invariance is not completely fulfilled, modifications might be usefull on the model for better fit.\nWe have run an analysis on school variable. Measurement invariance can also be checked using the gender variable."
  },
  {
    "objectID": "posts/Structural Equation Modelling (in Turkish)/index.html",
    "href": "posts/Structural Equation Modelling (in Turkish)/index.html",
    "title": "Structural Equation Modelling (in Turkish)",
    "section": "",
    "text": "Bu çalışmada PISA 2015 “student questionnaire: paper based verison” içerisindeki en son alt ölçek olan ‘Your View on Science’ ölçeğinden elde edilen veriler kullanılmıştır. Türkiye örnekleminden faydalanılmıştır. Bu örneklem ile bahsi geçen ölçeğin yapısal eşitlik modelleri oluşturulmuş ve karşılaştırılmıştır. Adım adım veri setinin R ile analize hazır hale getirilmesi anlatılmaktadır. Doğrudan analiz ile ilgileniyorsanız aşağıda ‘Analiz Süreci’ başlığına ilerleyiniz."
  },
  {
    "objectID": "posts/Structural Equation Modelling (in Turkish)/index.html#pre-processing-süreci",
    "href": "posts/Structural Equation Modelling (in Turkish)/index.html#pre-processing-süreci",
    "title": "Structural Equation Modelling (in Turkish)",
    "section": "Pre-processing süreci",
    "text": "Pre-processing süreci\nÖncelikle OECD tarafından yayınlanan veri setini şuradan indiriyoruz. İndirilen bu dosya .sav uzantılı bbir dosya. Bu tür dosyaları açmak için haven paketi read_sav fonksiyonundan faydalanabiliriz. Veri setinin tamamına ihtiyacımız yok, zaten oldukça büyük bir veri. Ülke değilşkeni ve ilgili ölçeğin maddeleri yeterli olacaktır. İndirdiğimiz .sav uzantılı dosyayı working directory’mize taşıdıktan sonra çalıştıracağımız kod satırı şunlar:\n\nCodelibrary(haven) \ndata &lt;- na.omit(read_sav(\n  \"CY6_MS_CMB_STU_QQQ.sav\",\n  col_select = c(\n    \"CNT\",\n    \"ST092Q01TA\",\n    \"ST092Q02TA\",\n    \"ST092Q04TA\",\n    \"ST092Q05TA\",\n    \"ST092Q06NA\",\n    \"ST092Q08NA\",\n    \"ST092Q09NA\",\n    \n    \"ST094Q01NA\",\n    \"ST094Q02NA\",\n    \"ST094Q03NA\",\n    \"ST094Q04NA\",\n    \"ST094Q05NA\",\n    \n    \"ST113Q01TA\",\n    \"ST113Q02TA\",\n    \"ST113Q03TA\",\n    \"ST113Q04TA\",\n    \n    \"ST129Q01TA\",\n    \"ST129Q02TA\",\n    \"ST129Q03TA\",\n    \"ST129Q04TA\",\n    \"ST129Q05TA\",\n    \"ST129Q06TA\",\n    \"ST129Q07TA\",\n    \"ST129Q08TA\",\n    \n    \"ST131Q01NA\",\n    \"ST131Q03NA\",\n    \"ST131Q04NA\",\n    \"ST131Q06NA\",\n    \"ST131Q08NA\",\n    \"ST131Q11NA\"\n  )\n))\n\n\nÜlke değişkenine artık ihtiyacımız yok. O nedenle veri setimizi sadece ölçek maddelerini içerecek şekilde yeniden tanımlayalım:\n\nCodelibrary(dplyr)\nscience_data&lt;-filter(data, CNT==\"TUR\")[,-1]\n\n\nBu veri setini şu kod satırını yürüterek .csv uzantılı bir dosya olarak bilgisayarıma kaydediyorum. Böylece analiz aşamasında o halini de paylaşabileceğim:\n\nCodewrite.csv(science_data, \"science_data.csv\", row.names = FALSE)"
  },
  {
    "objectID": "posts/Structural Equation Modelling (in Turkish)/index.html#analiz-süreci",
    "href": "posts/Structural Equation Modelling (in Turkish)/index.html#analiz-süreci",
    "title": "Structural Equation Modelling (in Turkish)",
    "section": "Analiz Süreci",
    "text": "Analiz Süreci\nİlgili ölçekten elde edilen veriyi .csv uzantılı dosya olarak indirebilirsiniz. Bunu read.csv fonksiyonu ile R ortamına aktarabilirsiniz. Veri setimiz hazır. Analizimizde öncelikle doğrulayı faktör analizi kullanacağız. Daha sonra da bi-faktör modelleme yapacağız. Veri setimizi tekrar yükleyelim:\nDoğrulayıcı Faktör Analizi (DFA)\nÇalışmamıza konu olan ölçek beş faktörden oluşmaktadır. Bu yapı ile ölçek geliştirme sürecinde tanımlanmıştır. Her faktörde farklı sayılarda maddeler yer almaktadır. Veri setinde bunlar ST ön eki ve faktör numarası ile tanımlanmıştır. Faktörleri ve veri setindeki kodlamalarını şu şekilde listeleyelim:\n\nST092: How informed are you about the following environmental issues?\nST094: How much do you disagree or agree with the statements about yourself below?\nST113: How much do you agree with the statements below?\nST129: How easy do you think it would be for you to perform the following tasks on your own?\nST131: How much do you disagree or agree with the statements below?\n\nBu beş faktörün altında tanımlanan yapıya göre DFA uygulayacağız ve bir geçerlilik çalışması yürüteceğiz. Bunun için öncelikle modelimizi R’a tanıtalım:\n\nCodeScience.model &lt;- 'ST092 =~ 1* ST092Q01TA+\n                              ST092Q02TA+\n                              ST092Q04TA+\n                              ST092Q05TA+\n                              ST092Q06NA+\n                              ST092Q08NA+\n                              ST092Q09NA\n                              \n                  ST094 =~ 1* ST094Q01NA+\n                              ST094Q02NA+\n                              ST094Q03NA+\n                              ST094Q04NA+\n                              ST094Q05NA\n                              \n                  ST113 =~ 1* ST113Q01TA+\n                              ST113Q02TA+\n                              ST113Q03TA+\n                              ST113Q04TA\n                  \n                  ST129 =~ 1* ST129Q01TA+\n                              ST129Q02TA+\n                              ST129Q03TA+\n                              ST129Q04TA+\n                              ST129Q05TA+\n                              ST129Q06TA+\n                              ST129Q07TA+\n                              ST129Q08TA\n                              \n                  ST131 =~ 1* ST131Q01NA+\n                              ST131Q03NA+\n                              ST131Q04NA+\n                              ST131Q06NA+\n                              ST131Q08NA+\n                              ST131Q11NA            \n                    \n                           \n                            ST092 ~~ ST094\n                            ST092 ~~ ST113\n                            ST092 ~~ ST129\n                            ST092 ~~ ST131\n                            \n                            ST094 ~~ ST113\n                            ST094 ~~ ST129\n                            ST094 ~~ ST131\n                            \n                            ST113 ~~ ST129\n                            ST113 ~~ ST131\n                            \n                            ST129 ~~ ST131'\n\n\nŞimdi Türkiye örnekleminden elde edilen verimizin bu modele uyum sağlayıp sağlamadığına bakacağız. Bu aşamada çeşitli modelleme yaklaşımlarından çıktılar alarak bunları karşılaştıracağız. Bu modelleme yaklaşımları şunlardır:\n\nmaximum likelihood model (MLM)\nweighted least squares (WLS)\nrobust maximum likelihood model (RMLM)\ndiagonally weighted least squares (DWLS)\n\nYukarıdaki lsitede görüldüğü sıra ile modellerimizi lavaan paketiyle oluşturalım:\n\nCodelibrary(lavaan)\nmodel_mlm &lt;- cfa(Science.model, data = science_data)\nmodel_wls &lt;- cfa(Science.model, WLS.V = TRUE, data = science_data)\nmodel_rml &lt;- cfa(Science.model, estimator = \"MLM\", se = \"robust.mlm\", data = science_data)\nmodel_dwls&lt;-cfa(Science.model, data = science_data, estimator=\"DWLS\")\n\n\nHer biri için de ayrı ayrı analiz çıktılarını summary() fonksiyonu ile tanımlayalım:\n\nCodescience.mlm&lt;-summary(\n  model_mlm,\n  fit.measures = TRUE,\n  standardized = TRUE,\n  rsquare = TRUE,\n  modindices = TRUE\n)\nscience.wls&lt;-summary(\n  model_wls,\n  fit.measures = TRUE,\n  standardized = TRUE,\n  rsquare = TRUE,\n  modindices = TRUE\n)\nscience.rml&lt;-summary(\n  model_rml,\n  fit.measures = TRUE,\n  standardized = TRUE,\n  rsquare = TRUE,\n  modindices = TRUE\n)\nscience.dwls&lt;-summary(\n  model_dwls,\n  fit.measures = TRUE,\n  standardized = TRUE,\n  rsquare = TRUE,\n  modindices = TRUE\n)\n\n\nTabi ki hem literatürde en yaygın kullanılan hem de alan uzmanları tarafından en ok önerilen yöntem olması sebebiyle MLM yöntemi önceliğimiz. Bu yöntemde normallik varsayımı karşılandığı sürece güçlü analizler elde edilebilmektedir. Veri setimiz de büyük bir örneklemden elde edildiği için bu varsayımın karşılandığı düşünülmektedir. Grafik incelemelerinde de bu durum görülecektir. Dolayısıyla öncelikle MLM’ye ait uyum indekslerini görelim:\n\nCodescience.mlm[[\"fit\"]]\n\n\n\nCodelibrary(lavaan)\nScience.model &lt;- 'ST092 =~ 1* ST092Q01TA+\n                              ST092Q02TA+\n                              ST092Q04TA+\n                              ST092Q05TA+\n                              ST092Q06NA+\n                              ST092Q08NA+\n                              ST092Q09NA\n                              \n                  ST094 =~ 1* ST094Q01NA+\n                              ST094Q02NA+\n                              ST094Q03NA+\n                              ST094Q04NA+\n                              ST094Q05NA\n                              \n                  ST113 =~ 1* ST113Q01TA+\n                              ST113Q02TA+\n                              ST113Q03TA+\n                              ST113Q04TA\n                  \n                  ST129 =~ 1* ST129Q01TA+\n                              ST129Q02TA+\n                              ST129Q03TA+\n                              ST129Q04TA+\n                              ST129Q05TA+\n                              ST129Q06TA+\n                              ST129Q07TA+\n                              ST129Q08TA\n                              \n                  ST131 =~ 1* ST131Q01NA+\n                              ST131Q03NA+\n                              ST131Q04NA+\n                              ST131Q06NA+\n                              ST131Q08NA+\n                              ST131Q11NA            \n                    \n                           \n                            ST092 ~~ ST094\n                            ST092 ~~ ST113\n                            ST092 ~~ ST129\n                            ST092 ~~ ST131\n                            \n                            ST094 ~~ ST113\n                            ST094 ~~ ST129\n                            ST094 ~~ ST131\n                            \n                            ST113 ~~ ST129\n                            ST113 ~~ ST131\n                            \n                            ST129 ~~ ST131'\nmodel_mlm &lt;- cfa(Science.model, data = science_data)\nscience.mlm&lt;-summary(\n  model_mlm,\n  fit.measures = TRUE,\n  standardized = TRUE,\n  rsquare = TRUE,\n  modindices = TRUE\n)\nscience.mlm[[\"fit\"]]\n\n             npar              fmin             chisq                df \n           70.000             0.652          6055.193           395.000 \n           pvalue    baseline.chisq       baseline.df   baseline.pvalue \n            0.000         96221.017           435.000             0.000 \n              cfi               tli              logl unrestricted.logl \n            0.941             0.935       -135531.110       -132503.514 \n              aic               bic            ntotal              bic2 \n       271202.220        271653.284          4646.000        271430.850 \n            rmsea    rmsea.ci.lower    rmsea.ci.upper      rmsea.pvalue \n            0.056             0.054             0.057             0.000 \n             srmr \n            0.039 \n\n\nBu çıktılar incelendiğinde, 70 parametreli 395 serbestlik derecesinde bir model oluştuğu görülmektedir. CFI ve TLI uyum indekleri .90 eşik değerin üzerindeyken, RMSEA ve SRMR .06’nın altında yer almaktadır. Bu durumda modelimizin uyumlu olduğu düşünülebilir. Yine de diğer modeller ile de karşılaştırmak gerekir. Onların uyum indeklerini de görelim:\nWLS model:\n\nCodescience.wls[[\"fit\"]]\n\n\n\n\n             npar              fmin             chisq                df \n           70.000             0.652          6055.193           395.000 \n           pvalue    baseline.chisq       baseline.df   baseline.pvalue \n            0.000         96221.017           435.000             0.000 \n              cfi               tli              logl unrestricted.logl \n            0.941             0.935       -135531.110       -132503.514 \n              aic               bic            ntotal              bic2 \n       271202.220        271653.284          4646.000        271430.850 \n            rmsea    rmsea.ci.lower    rmsea.ci.upper      rmsea.pvalue \n            0.056             0.054             0.057             0.000 \n             srmr \n            0.039 \n\n\nRML model:\n\nCodescience.rml[[\"fit\"]]\n\n\n\n\n                         npar                          fmin \n                       70.000                         0.652 \n                        chisq                            df \n                     6055.193                       395.000 \n                       pvalue                  chisq.scaled \n                        0.000                      4484.352 \n                    df.scaled                 pvalue.scaled \n                      395.000                         0.000 \n         chisq.scaling.factor                baseline.chisq \n                        1.350                     96221.017 \n                  baseline.df               baseline.pvalue \n                      435.000                         0.000 \n        baseline.chisq.scaled            baseline.df.scaled \n                    75815.224                       435.000 \n       baseline.pvalue.scaled baseline.chisq.scaling.factor \n                        0.000                         1.269 \n                          cfi                           tli \n                        0.941                         0.935 \n                   cfi.scaled                    tli.scaled \n                        0.946                         0.940 \n                   cfi.robust                    tli.robust \n                        0.942                         0.936 \n                         logl             unrestricted.logl \n                  -135531.110                   -132503.514 \n                          aic                           bic \n                   271202.220                    271653.284 \n                       ntotal                          bic2 \n                     4646.000                    271430.850 \n                        rmsea                rmsea.ci.lower \n                        0.056                         0.054 \n               rmsea.ci.upper                  rmsea.pvalue \n                        0.057                         0.000 \n                 rmsea.scaled         rmsea.ci.lower.scaled \n                        0.047                         0.046 \n        rmsea.ci.upper.scaled           rmsea.pvalue.scaled \n                        0.048                         1.000 \n                 rmsea.robust         rmsea.ci.lower.robust \n                        0.055                         0.053 \n        rmsea.ci.upper.robust           rmsea.pvalue.robust \n                        0.056                            NA \n                         srmr \n                        0.039 \n\n\nDWLS model:\n\nCodescience.dwls[[\"fit\"]]\n\n\n\n\n           npar            fmin           chisq              df          pvalue \n         70.000           0.188        1744.348         395.000           0.000 \n baseline.chisq     baseline.df baseline.pvalue             cfi             tli \n     120867.135         435.000           0.000           0.989           0.988 \n          rmsea  rmsea.ci.lower  rmsea.ci.upper    rmsea.pvalue            srmr \n          0.027           0.026           0.028           1.000           0.034 \n\n\nTüm bu çıktılar incelendiğinde, en uygun modelin diagonally weighted least squares (DWLS) olduğu düşünülmektedir. Veri setinin farklı ölçek düzeyinde olması ve kategorik olması bu durumun sebebi olabilir. DWLS modeli, liteatürde yaygın bir şekilde bu tür veri setleri için önerilmektedir.\nBI-FACTOR MODELLER\nBi faktör modellemede yapıyı oluşturan faktörlerin tüm maddelerden oluşan genel faktör ile ilişkisi incelenerek karar verilir. Bu amaçla DFA örneğinde olduğu gibi model tanımlamamızı yapıyoruz:\n\nCodeScience.bifactormodel &lt;- 'general.factor =~\n                              ST092Q01TA+\n                              ST092Q02TA+\n                              ST092Q04TA+\n                              ST092Q05TA+\n                              ST092Q06NA+\n                              ST092Q08NA+\n                              ST092Q09NA+\n                              ST094Q01NA+\n                              ST094Q02NA+\n                              ST094Q03NA+\n                              ST094Q04NA+\n                              ST094Q05NA+\n                              ST113Q01TA+\n                              ST113Q02TA+\n                              ST113Q03TA+\n                              ST113Q04TA+\n                              ST129Q01TA+\n                              ST129Q02TA+\n                              ST129Q03TA+\n                              ST129Q04TA+\n                              ST129Q05TA+\n                              ST129Q06TA+\n                              ST129Q07TA+\n                              ST129Q08TA+\n                              ST131Q01NA+\n                              ST131Q03NA+\n                              ST131Q04NA+\n                              ST131Q06NA+\n                              ST131Q08NA+\n                              ST131Q11NA\n\n                  ST092 =~ 1* ST092Q01TA+\n                              ST092Q02TA+\n                              ST092Q04TA+\n                              ST092Q05TA+\n                              ST092Q06NA+\n                              ST092Q08NA+\n                              ST092Q09NA\n\n                  ST094 =~ 1* ST094Q01NA+\n                              ST094Q02NA+\n                              ST094Q03NA+\n                              ST094Q04NA+\n                              ST094Q05NA\n\n                  ST113 =~ 1* ST113Q01TA+\n                              ST113Q02TA+\n                              ST113Q03TA+\n                              ST113Q04TA\n\n                  ST129 =~ 1* ST129Q01TA+\n                              ST129Q02TA+\n                              ST129Q03TA+\n                              ST129Q04TA+\n                              ST129Q05TA+\n                              ST129Q06TA+\n                              ST129Q07TA+\n                              ST129Q08TA\n\n                  ST131 =~ 1* ST131Q01NA+\n                              ST131Q03NA+\n                              ST131Q04NA+\n                              ST131Q06NA+\n                              ST131Q08NA+\n                              ST131Q11NA\n                              \n\n                general.factor  ~~ 0*ST092\n                general.factor  ~~ 0*ST094\n                general.factor  ~~ 0*ST113\n                general.factor  ~~ 0*ST129\n                general.factor  ~~ 0*ST131\n                \n                            ST092 ~~ ST094\n                            ST092 ~~ ST113\n                            ST092 ~~ ST129\n                            ST092 ~~ ST131\n\n                            ST094 ~~ ST113\n                            ST094 ~~ ST129\n                            ST094 ~~ ST131\n\n                            ST113 ~~ ST129\n                            ST113 ~~ ST131\n\n                            ST129 ~~ ST131'\n\n\nArdından modellerimizi veri setimiz ile sınıyoruz.\n\nCodeScience.bifactormodel_mlm &lt;-\n  cfa(\n    Science.bifactormodel,\n    data = science_data,\n    std.lv = TRUE,\n    information = \"observed\"\n  )\nScience.bifactormodel_rml &lt;-\n  cfa(\n    Science.bifactormodel,\n    data = science_data,\n    estimator = \"MLM\",\n    se = \"robust.mlm\",\n    std.lv = TRUE,\n    information = \"observed\"\n  )\nbifactor_mlm &lt;- summary(Science.bifactormodel_mlm ,\n        fit.measures = TRUE,\n        standardized = TRUE)\nbifactor_rml &lt;- summary(Science.bifactormodel_rml ,\n        fit.measures = TRUE,\n        standardized = TRUE)\n\nbifactor_mlm[['fit']]\n\n             npar              fmin             chisq                df \n           95.000             0.634          5891.081           370.000 \n           pvalue    baseline.chisq       baseline.df   baseline.pvalue \n            0.000         96221.017           435.000             0.000 \n              cfi               tli              logl unrestricted.logl \n            0.942             0.932       -135449.054       -132503.514 \n              aic               bic            ntotal              bic2 \n       271088.109        271700.266          4646.000        271398.392 \n            rmsea    rmsea.ci.lower    rmsea.ci.upper      rmsea.pvalue \n            0.057             0.055             0.058             0.000 \n             srmr \n            0.328 \n\nCodebifactor_rml[['fit']]\n\n                         npar                          fmin \n                       95.000                         0.634 \n                        chisq                            df \n                     5891.081                       370.000 \n                       pvalue                  chisq.scaled \n                        0.000                      4166.363 \n                    df.scaled                 pvalue.scaled \n                      370.000                         0.000 \n         chisq.scaling.factor                baseline.chisq \n                        1.414                     96221.017 \n                  baseline.df               baseline.pvalue \n                      435.000                         0.000 \n        baseline.chisq.scaled            baseline.df.scaled \n                    19393.334                       435.000 \n       baseline.pvalue.scaled baseline.chisq.scaling.factor \n                        0.000                         4.962 \n                          cfi                           tli \n                        0.942                         0.932 \n                   cfi.scaled                    tli.scaled \n                        0.800                         0.765 \n                   cfi.robust                    tli.robust \n                        0.943                         0.933 \n                         logl             unrestricted.logl \n                  -135449.054                   -132503.514 \n                          aic                           bic \n                   271088.109                    271700.266 \n                       ntotal                          bic2 \n                     4646.000                    271398.392 \n                        rmsea                rmsea.ci.lower \n                        0.057                         0.055 \n               rmsea.ci.upper                  rmsea.pvalue \n                        0.058                         0.000 \n                 rmsea.scaled         rmsea.ci.lower.scaled \n                        0.047                         0.046 \n        rmsea.ci.upper.scaled           rmsea.pvalue.scaled \n                        0.048                         1.000 \n                 rmsea.robust         rmsea.ci.lower.robust \n                        0.056                         0.054 \n        rmsea.ci.upper.robust           rmsea.pvalue.robust \n                        0.057                            NA \n                         srmr \n                        0.328 \n\n\nBI-FAKTÖR ML model:\n\n\n             npar              fmin             chisq                df \n           95.000             0.634          5891.081           370.000 \n           pvalue    baseline.chisq       baseline.df   baseline.pvalue \n            0.000         96221.017           435.000             0.000 \n              cfi               tli              logl unrestricted.logl \n            0.942             0.932       -135449.054       -132503.514 \n              aic               bic            ntotal              bic2 \n       271088.109        271700.266          4646.000        271398.392 \n            rmsea    rmsea.ci.lower    rmsea.ci.upper      rmsea.pvalue \n            0.057             0.055             0.058             0.000 \n             srmr \n            0.328 \n\n\nBI-FAKTÖR ROBUST ML model:\n\n\n                         npar                          fmin \n                       95.000                         0.634 \n                        chisq                            df \n                     5891.081                       370.000 \n                       pvalue                  chisq.scaled \n                        0.000                      4166.363 \n                    df.scaled                 pvalue.scaled \n                      370.000                         0.000 \n         chisq.scaling.factor                baseline.chisq \n                        1.414                     96221.017 \n                  baseline.df               baseline.pvalue \n                      435.000                         0.000 \n        baseline.chisq.scaled            baseline.df.scaled \n                    19393.334                       435.000 \n       baseline.pvalue.scaled baseline.chisq.scaling.factor \n                        0.000                         4.962 \n                          cfi                           tli \n                        0.942                         0.932 \n                   cfi.scaled                    tli.scaled \n                        0.800                         0.765 \n                   cfi.robust                    tli.robust \n                        0.943                         0.933 \n                         logl             unrestricted.logl \n                  -135449.054                   -132503.514 \n                          aic                           bic \n                   271088.109                    271700.266 \n                       ntotal                          bic2 \n                     4646.000                    271398.392 \n                        rmsea                rmsea.ci.lower \n                        0.057                         0.055 \n               rmsea.ci.upper                  rmsea.pvalue \n                        0.058                         0.000 \n                 rmsea.scaled         rmsea.ci.lower.scaled \n                        0.047                         0.046 \n        rmsea.ci.upper.scaled           rmsea.pvalue.scaled \n                        0.048                         1.000 \n                 rmsea.robust         rmsea.ci.lower.robust \n                        0.056                         0.054 \n        rmsea.ci.upper.robust           rmsea.pvalue.robust \n                        0.057                            NA \n                         srmr \n                        0.328 \n\n\nBu çıktılar incelendiğinde de bi faktör modelin verimize DWLS model kadar uyumlu olmadığı görülmektedir. Bu nedenle DWLS modelin en uyumlu model olduğu düşünülmektedir. Bu modelin çıktılarını tablolaştıralım.\nEN UYUMLU MODEL:DWLS çıktıları\nNOT: Faktör varyansları sabitlenerek model oluşturulduğu için faktör varyansları tablosu raporlaştırılmamıştır.\nModel uyum indeksleri\n\nParametre Tahminleri\n\nFaktör Kovaryansları\n\nArtık (residual) Varyanslar\n\nModel gösterimi"
  },
  {
    "objectID": "posts/Top 5 Data Sources/index.html",
    "href": "posts/Top 5 Data Sources/index.html",
    "title": "Top 5 Data Sources",
    "section": "",
    "text": "In the world of data analysis, having access to quality data sources can make all the difference. Whether you’re a researcher, a business owner, or just someone who loves playing with numbers, finding the right data can help you uncover insights and make better decisions. With so many data sources available online, it can be hard to know where to start. In this blog post, I’ll be sharing my top 5 favorite data sources, each of which I’ve found to be reliable, user-friendly, and full of valuable information. From the Cloud Google Marketplace to the Bureau of Labor Statistics, these data sources offer a wealth of insights that I hope you’ll find as useful and inspiring as I have. So without further ado, let’s dive in!"
  },
  {
    "objectID": "posts/Top 5 Data Sources/index.html#introduction",
    "href": "posts/Top 5 Data Sources/index.html#introduction",
    "title": "Top 5 Data Sources",
    "section": "",
    "text": "In the world of data analysis, having access to quality data sources can make all the difference. Whether you’re a researcher, a business owner, or just someone who loves playing with numbers, finding the right data can help you uncover insights and make better decisions. With so many data sources available online, it can be hard to know where to start. In this blog post, I’ll be sharing my top 5 favorite data sources, each of which I’ve found to be reliable, user-friendly, and full of valuable information. From the Cloud Google Marketplace to the Bureau of Labor Statistics, these data sources offer a wealth of insights that I hope you’ll find as useful and inspiring as I have. So without further ado, let’s dive in!"
  },
  {
    "objectID": "posts/Top 5 Data Sources/index.html#cloud-google-marketplace",
    "href": "posts/Top 5 Data Sources/index.html#cloud-google-marketplace",
    "title": "Top 5 Data Sources",
    "section": "1. Cloud Google Marketplace",
    "text": "1. Cloud Google Marketplace\nThe Cloud Google Marketplace is a collection of datasets, APIs, and applications that are hosted on the Google Cloud Platform. The marketplace offers a wide range of data sources from different industries, including finance, healthcare, and transportation. Some of the popular datasets available on the Cloud Google Marketplace include:\n\nCOVID-19 Open Research Dataset (CORD-19)\nNew York City Taxi and Limousine Commission (TLC) Trip Data\nChicago Crime Data\nOpenFDA Drug Label API\nUS Census Bureau Data\n\nThe platform offers an intuitive interface that allows users to search for datasets, preview data, and access documentation. The marketplace also provides access to cloud-based tools for data exploration, visualization, and analysis."
  },
  {
    "objectID": "posts/Top 5 Data Sources/index.html#drought.gov",
    "href": "posts/Top 5 Data Sources/index.html#drought.gov",
    "title": "Top 5 Data Sources",
    "section": "2. Drought.gov",
    "text": "2. Drought.gov\nDrought.gov is a comprehensive resource for drought-related data, maps, and tools in the United States. The website provides a variety of data sources, including:\n\nDrought Monitor\nSoil Moisture Active Passive (SMAP) data\nEvaporative Demand Drought Index (EDDI)\nU.S. Drought Atlas\n\nThe website offers easy access to a wide range of data, including both historical and real-time data. Additionally, the site provides a number of interactive maps and visualizations that make it easy to explore the data and gain insights."
  },
  {
    "objectID": "posts/Top 5 Data Sources/index.html#unicef-statistical-tables",
    "href": "posts/Top 5 Data Sources/index.html#unicef-statistical-tables",
    "title": "Top 5 Data Sources",
    "section": "3. UNICEF Statistical Tables",
    "text": "3. UNICEF Statistical Tables\nUNICEF Statistical Tables is a data source that provides a wide range of data on child welfare and development across the globe. The data is collected from various sources, including national surveys and administrative records. Some of the data available on the UNICEF Statistical Tables includes:\n\nChild mortality rates\nNutrition indicators\nEducation indicators\nChild protection indicators\n\nThe platform offers a user-friendly interface that allows users to browse data by country, indicator, or year. For instance, policymakers looking to design effective policies to improve child welfare in their countries could use data from UNICEF Statistical Tables to identify areas where interventions are most needed, such as high child mortality rates or low school enrollment rates."
  },
  {
    "objectID": "posts/Top 5 Data Sources/index.html#openpolicing.stanford.edu",
    "href": "posts/Top 5 Data Sources/index.html#openpolicing.stanford.edu",
    "title": "Top 5 Data Sources",
    "section": "4. OpenPolicing.Stanford.edu",
    "text": "4. OpenPolicing.Stanford.edu\nOpenPolicing.Stanford.edu is a data source that provides access to millions of records of police stops and searches across the United States. The data is collected from various law enforcement agencies and compiled into a standardized format for analysis. The platform also provides tools for exploring the data, including interactive maps and dashboards. As this source of data contains pretty complicate information, you should definitely check the Read Me File before you delve into it.\nThe platform provides detailed data on police stops and searches, including the race and ethnicity of the individuals involved, the reason for the stop, and the outcome of the encounter.\nOne example of how this data could be used is to identify patterns of racial bias in policing. By analyzing the data on police stops and searches, researchers can identify whether certain racial or ethnic groups are more likely to be stopped or searched than others, and whether these disparities are driven by legitimate factors (such as crime rates) or by bias."
  },
  {
    "objectID": "posts/Top 5 Data Sources/index.html#bureau-of-labor-statistics-bls-tables",
    "href": "posts/Top 5 Data Sources/index.html#bureau-of-labor-statistics-bls-tables",
    "title": "Top 5 Data Sources",
    "section": "5. Bureau of Labor Statistics (BLS) Tables",
    "text": "5. Bureau of Labor Statistics (BLS) Tables\nThe Bureau of Labor Statistics (BLS) Tables is a data source that provides access to a wide range of data on the U.S. labor market. The platform offers a wealth of data, including:\n\nUnemployment rates\nEmployment by industry and occupation\nLabor force participation rates\nAverage wages and salaries\nJob openings and hires\n\nThe platform provides a variety of tools for data exploration and analysis, including charts, tables, and interactive visualizations. For example, if you’re a policymaker looking to design policies that promote job growth and reduce unemployment, you could use BLS Tables data to identify which industries are experiencing job growth, which occupations are in high demand, and which regions have the highest unemployment rates."
  },
  {
    "objectID": "posts/Top 5 Data Sources/index.html#an-easter-egg-openpsychometrics.org",
    "href": "posts/Top 5 Data Sources/index.html#an-easter-egg-openpsychometrics.org",
    "title": "Top 5 Data Sources",
    "section": "An Easter Egg: OpenPsychometrics.org",
    "text": "An Easter Egg: OpenPsychometrics.org\nAs a surprise addition to my list of favorite data sources, I would like to introduce OpenPsychometrics.org. I came across with this website during a Ph.D. course on Item Response Theory carried by Associate Professor Ergül Demir. This website provides access to a collection of free psychological tests and assessments, which can be used for a wide range of purposes, from self-reflection to academic research.\nSome of the psychological tests available on the site include:\n\nThe Big Five personality traits\nEmotional intelligence\nCognitive abilities\nMoral reasoning\n\nAs someone who is interested in both data science and psychology, I have found OpenPsychometrics.org to be an incredibly interesting and valuable resource. The website offers a wide range of psychological tests that can help individuals gain insights into their own personality traits, cognitive abilities, and emotional intelligence. Additionally, researchers can use the site to conduct studies on these topics and collect data for analysis.\nIn conclusion, OpenPsychometrics.org is a valuable resource for anyone interested in psychology, personal development, or academic research. The platform offers a variety of free psychological tests that can help individuals gain insights into themselves, as well as collect data for research purposes."
  },
  {
    "objectID": "posts/Top 5 Data Sources/index.html#conclusion",
    "href": "posts/Top 5 Data Sources/index.html#conclusion",
    "title": "Top 5 Data Sources",
    "section": "Conclusion",
    "text": "Conclusion\nThese are just a few of my favorite data sources that have proven to be invaluable for my work as a researcher. Each of these data sources provides a unique perspective on different aspects of the world, from labor markets to child welfare to police behavior. By combining and analyzing data from these sources, we can gain a more comprehensive understanding of the world around us and make better-informed decisions."
  },
  {
    "objectID": "posts/Visualisation of My Personal Google Data (2)/index.html",
    "href": "posts/Visualisation of My Personal Google Data (2)/index.html",
    "title": "Visualisation of My Personal Google Data (2): My exercise routine",
    "section": "",
    "text": "If you give permission to Google to store your location data, they will keep them in their databases forever. You can also allow them to store it for a while and then ask them delete it. They will directly do so.\nWhat makes this study a fun project is its being very personal. I decided to analyze my personal data in August, 2022. Therefore, I granted many new permissions to Google along with many previously granted permissions. They keep them in various formats including .csv, .json, .mbox etc. When you query for your personal data, they provide it within a couple of days depending on the size of the data you queried.\nUsually, I provide the readers with the data in my posts. However, in this series, the data are very personal and so I will not."
  },
  {
    "objectID": "posts/Visualisation of My Personal Google Data (2)/index.html#introduction-to-the-series-visualisation-of-my-personal-google-data",
    "href": "posts/Visualisation of My Personal Google Data (2)/index.html#introduction-to-the-series-visualisation-of-my-personal-google-data",
    "title": "Visualisation of My Personal Google Data (2): My exercise routine",
    "section": "",
    "text": "If you give permission to Google to store your location data, they will keep them in their databases forever. You can also allow them to store it for a while and then ask them delete it. They will directly do so.\nWhat makes this study a fun project is its being very personal. I decided to analyze my personal data in August, 2022. Therefore, I granted many new permissions to Google along with many previously granted permissions. They keep them in various formats including .csv, .json, .mbox etc. When you query for your personal data, they provide it within a couple of days depending on the size of the data you queried.\nUsually, I provide the readers with the data in my posts. However, in this series, the data are very personal and so I will not."
  },
  {
    "objectID": "posts/Visualisation of My Personal Google Data (2)/index.html#introduction-my-exercise-routine",
    "href": "posts/Visualisation of My Personal Google Data (2)/index.html#introduction-my-exercise-routine",
    "title": "Visualisation of My Personal Google Data (2): My exercise routine",
    "section": "Introduction: “My exercise routine”",
    "text": "Introduction: “My exercise routine”\nIn this part of the series, we will investigate my personal location data. We will visualize the spots I visited within a period of time. This way, I personally will gain insights about how boring my days are :)\nThe R packages that we use in this post are as follows:\n\nlibrary(\"psych\")\nlibrary(\"dplyr\")\nlibrary(\"plotly\")"
  },
  {
    "objectID": "posts/Visualisation of My Personal Google Data (2)/index.html#understand-the-data-pre-processing",
    "href": "posts/Visualisation of My Personal Google Data (2)/index.html#understand-the-data-pre-processing",
    "title": "Visualisation of My Personal Google Data (2): My exercise routine",
    "section": "Understand the Data & Pre-processing",
    "text": "Understand the Data & Pre-processing\nLet’s start the procedure by reading the data into R. The head() function provides us with the first six observations of the data frame. There are 16 variables some of which are filled with many many NAs. such as Biking.duration..ms. The reason for that is pretty simple; I do not ride around. The variables are mostly numeric, yet Dategives us information about the date of the observation. The variable Running.duration..ms. has many NAs too. But it also has some observations because I run only a few days a week. I will use this variable to filter my data later.\n\ndaily_metrics&lt;-read.csv(\"Daily activity metrics.csv\",sep=\",\", header = TRUE)\nhead(daily_metrics) \n\n        Date Move.Minutes.count Calories..kcal. Distance..m. Heart.Points\n1 2021-05-16                 77        687.6241     4090.742           40\n2 2021-05-17                 79       2016.4717     3978.947           13\n3 2021-05-18                 31       1752.2711     2267.061           21\n4 2021-05-19                140       2201.5012     9714.473           74\n5 2021-05-20                 50       1842.7984     2618.885           16\n6 2021-05-21                 63       1902.6128     4571.006           43\n  Heart.Minutes Average.speed..m.s. Max.speed..m.s. Min.speed..m.s. Step.count\n1            40           0.9532187        1.017549       0.8888889       7654\n2            13           0.3553377        1.328190       0.2391036       6161\n3            21           0.4802522        1.643308       0.2550294       3450\n4            74           0.5155224        1.711053       0.2550294      14000\n5            16           0.3804749        1.516558       0.2588491       4078\n6            43           0.5348663        1.492037       0.2656556       6768\n  Average.weight..kg. Max.weight..kg. Min.weight..kg. Biking.duration..ms.\n1                  80              80              80                   NA\n2                  NA              NA              NA                   NA\n3                  NA              NA              NA                   NA\n4                  NA              NA              NA                   NA\n5                  NA              NA              NA                   NA\n6                  NA              NA              NA                   NA\n  Walking.duration..ms. Running.duration..ms.\n1               4664026                    NA\n2               4692410                424601\n3               1610096                    NA\n4               8268890                 93935\n5               2994689                    NA\n6               3909532                    NA\n\n\nAs the data contains some variables with decimal numbers, I would love to round them to increase meaningfulness. The mutate_if function combined with is.numeric gives me the opportunity to pick only the numeric variables to round.\n\ndaily_metrics &lt;- daily_metrics%&gt;% mutate_if(is.numeric, round, digits=2)\n\nSo far so good. Now I want to select the variables that I want to visualize in my project and filter my data accordingly. Let’s see the variable names:\n\ncolnames(daily_metrics)\n\n [1] \"Date\"                  \"Move.Minutes.count\"    \"Calories..kcal.\"      \n [4] \"Distance..m.\"          \"Heart.Points\"          \"Heart.Minutes\"        \n [7] \"Average.speed..m.s.\"   \"Max.speed..m.s.\"       \"Min.speed..m.s.\"      \n[10] \"Step.count\"            \"Average.weight..kg.\"   \"Max.weight..kg.\"      \n[13] \"Min.weight..kg.\"       \"Biking.duration..ms.\"  \"Walking.duration..ms.\"\n[16] \"Running.duration..ms.\"\n\n\nIn this project, I will use following variables: date, Move.Minutes.count,Calories..kcal.,Distance..m.,Average.speed..m.s.,Biking.duration..ms.,Step.caoun and walking.duration.ms.. Let’s use subset function to get rid of the unnecessary part of the data.\n\ndaily_metrics &lt;- subset(daily_metrics, select = -c(5,6,8,9,14, 13,12,11) ) #idk why so randomly ordered numbers\nhead(daily_metrics)\n\n        Date Move.Minutes.count Calories..kcal. Distance..m.\n1 2021-05-16                 77          687.62      4090.74\n2 2021-05-17                 79         2016.47      3978.95\n3 2021-05-18                 31         1752.27      2267.06\n4 2021-05-19                140         2201.50      9714.47\n5 2021-05-20                 50         1842.80      2618.88\n6 2021-05-21                 63         1902.61      4571.01\n  Average.speed..m.s. Step.count Walking.duration..ms. Running.duration..ms.\n1                0.95       7654               4664026                    NA\n2                0.36       6161               4692410                424601\n3                0.48       3450               1610096                    NA\n4                0.52      14000               8268890                 93935\n5                0.38       4078               2994689                    NA\n6                0.53       6768               3909532                    NA\n\n\nAs mentioned before, I would like to work on the days that I run. That’s why I will also get rid of observations that do not contain any running data and then my data will be ready for the visualization.\nFinally, let’s get the scatter plots of whatever we have left to see a bigger picture. There are some outliers; you will easily recognize some bubles sitting alone in their plots. However, they don’t look like they need further investigation.\n\ndata_for_plotting&lt;-daily_metrics[!is.na(daily_metrics$Running.duration..ms.),]\nplot(data_for_plotting)"
  },
  {
    "objectID": "posts/Visualisation of My Personal Google Data (2)/index.html#data-visualization",
    "href": "posts/Visualisation of My Personal Google Data (2)/index.html#data-visualization",
    "title": "Visualisation of My Personal Google Data (2): My exercise routine",
    "section": "Data Visualization",
    "text": "Data Visualization\nplotly is an amazing alternative to ggplot2. Today we will work on this package with our data. Our first chart contains information about the number of steps I took each day that I ran from May 2021 to November 2022.\n\nfig&lt;-plotly::plot_ly(data=data_for_plotting, type =\"scatter\", mode=\"lines+markers\",  \n                     y=data_for_plotting$Step.count, x=~Date)\nfig\n\n\n\n\n\nWhen you hover over the chart you might see the date and the number of steps that I took. For instance on 12th and 28th of August, I took more than 16k steps.\nNow let’s add two dimensions to our chart: the duration of walking and running in a day using the addtrace function. Also layout function helps us name our plot.\n\n########################\n# walking VS runing duration bars\n#########################\nay&lt;- list(\n  tickfont =list(color =\"red\"),\n  overlaying = \"y\",\n  side= \"right\",\n  title= \"&lt;b&gt; secondary&lt;/b&gt; y axis\"\n)\n\n\nfig&lt;-plotly::plot_ly()\nfig&lt;- fig %&gt;%\n  add_trace(type =\"bar\",  \n            y =data_for_plotting$Walking.duration..ms.,\n            x=data_for_plotting$Date, \n            name=\"walking duration (ms)\"\n  )\nfig&lt;- fig %&gt;% add_trace (type =\"scatter\", mode=\"lines+markers\" , #yaxis=\"y2\",\n                         y=data_for_plotting$Running.duration..ms., \n                         x=data_for_plotting$Date , \n                         name=\"running duration (ms)\")\n\n\nfig &lt;- fig %&gt;% layout(\n  title=\"two axis\",\n  yaxis2 = ay,\n  xaxis = list( title= \"Days\"),\n  yaxis = list( title= \"Time I moved in miliseconds\")\n)\nfig\n\n\n\n\n\nThis chart brings another aspect of my exercise routine. The days I ran in the last year, I walked a lot more than I ran, obviously. One exception to that might be 25th of July when I almost walked and ran same amount of time.\nNow, besides time, let’s add another dimension of distance. This chart will have another y axis. We need to let plotly know that we will use another dimension. overlaying = \"y\" argument will do it. Also, we use another addtrace function to add the second y axis properties. yaxis=\"y2\" argument will link this trace to our new dimension. y2 here is defined by plotly as default setting.\n\nay&lt;- list(\n  tickfont =list(color =\"red\"),\n  overlaying = \"y\",\n  side= \"right\",\n  title= \"&lt;b&gt; Distance I walked in meters &lt;/b&gt;\"\n)\n\n\nfig&lt;-plotly::plot_ly()\n\nfig&lt;- fig %&gt;%\n  add_trace(y =data_for_plotting$Move.Minutes.count,\n            x=data_for_plotting$Date, \n            name=\"movemnt(min.)\", \n            type =\"bar\" )\n\n\nfig&lt;- fig %&gt;% add_trace (y=data_for_plotting$Distance..m., \n                         x=data_for_plotting$Date , \n                         name=\"distance(m.)\", yaxis=\"y2\",\n                         type =\"scatter\", mode=\"lines+markers\")\n\nfig &lt;- fig %&gt;% layout(\n  title=\"My movement VS distance  &lt;b&gt;in 2022&lt;/b&gt;\",\n  yaxis2 = ay,\n  xaxis = list( title= \"&lt;b&gt;Days&lt;/b&gt;\"),\n  yaxis = list( title= \"&lt;b&gt; Time I moved in minutes &lt;/b&gt; \")\n)\nfig\n\n\n\n\n\nAs the final result, you can see that on the first y axis there are the number of minutes (from 0 to 200) when I was in a moving state. On the other side, we have the distance I walked or run in meters (from 0 to 10k). For example; on sixth of Auust in 2022, I walked almost 7k meters in 150 minutes.\nAs a final step, let’s see the calories I burnt during that time. Another addtrace would help us draw the line for the calories that I burnt throughout these days:\n\nfig&lt;-plotly::plot_ly()\n\nfig&lt;- fig %&gt;%\n  add_trace(y =data_for_plotting$Step.count,\n            x=data_for_plotting$Date, \n            name=\"Steps \", \n            type =\"scatter\", mode=\"lines\")\n\nfig&lt;- fig %&gt;% add_trace (y=data_for_plotting$Distance..m., \n                         x=data_for_plotting$Date , \n                         name=\"distance\", \n                         type =\"bar\" )\n\n\nfig&lt;- fig %&gt;% add_trace (y=data_for_plotting$Calories..kcal., \n                         x=data_for_plotting$Date , \n                         name=\"calories\",\n                         type =\"scatter\", mode=\"lines\")\n\n\n\nfig &lt;- fig %&gt;% layout(\n  title=\" &lt;b&gt;My movement in 2022&lt;/b&gt;\",\n  xaxis = list( title= \"&lt;b&gt;Days&lt;/b&gt;\"),\n  yaxis = list( title= \"&lt;b&gt; steps(n)&lt;/b&gt; / &lt;b&gt; distance(m)&lt;/b&gt; / &lt;b&gt; calories(kcal) &lt;/b&gt; \")\n)\nfig\n\n\n\n\n\nThe reason why there are not many diversity in the calories that I burnt might be because of two reasons: either the calories are not counted correctly by Google, which I believe is the case, or as the charts contain information for only the days that I run, I spent similar amounts of calories. Either way, we can see that the calories don’t change much even though took much more steps and spent more time moving on some days than some other days."
  },
  {
    "objectID": "posts/Visualisation of My Personal Google Data (2)/index.html#conclusion",
    "href": "posts/Visualisation of My Personal Google Data (2)/index.html#conclusion",
    "title": "Visualisation of My Personal Google Data (2): My exercise routine",
    "section": "Conclusion",
    "text": "Conclusion\nThe plotly package have many useful embedded features. Especially the hovering property of the charts make it much more appealing easily. Another beautiful feature of plotly is that you can use certain HTML tags on your charts. For example here in the last chart, we used &lt;b&gt; bolt &lt;/b&gt;tag in order for making our titles stand out in the layout function.\nFinally I must also mention that plotly have some features to create different types of graphs, animations and many other. Their website for R worths a visit."
  }
]