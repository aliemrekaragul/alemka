[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ali Emre Karagül",
    "section": "",
    "text": "Hey there!\nYou are visiting my personal blog, so here is a little bit of information about me:\nI am passionate about learning and exploring new ideas in the fields of Data Science, Psychometrics, Data Analytics, and Educational Assessment. I have a deep interest in language assessment and have conducted research in this field. My research focuses on understanding the impact of different assessment techniques on student outcomes. I am a driven individual who believes in hard work and dedication and I am always looking for new opportunities to make a positive impact. I am an avid reader and always looking to learn something new.\nCurrently, I work on my PhD in Educational Assessment and Evaluation. My current research interest is on Automated Essay Scoring, which has led me through machine learning and natural language processing."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Ali Emre Karagül",
    "section": "EDUCATION",
    "text": "EDUCATION\n\nGazi University\nFaculty of Education (2022-ongoing)\nPh.D. student in the Department of Educational Assessment and Evaluation\n\n\nAnkara University\nFaculty of Educational Sciences (2016-2020)\nMaster’s Degree in Assessment and Evaluation in Education\\\n\n\nGazi University\nFaculty of Education (2008-2014)\nBachelor’s degree in the Department of Foreign Language Education"
  },
  {
    "objectID": "index.html#contact-me",
    "href": "index.html#contact-me",
    "title": "Ali Emre Karagül",
    "section": "CONTACT ME",
    "text": "CONTACT ME\nAlways feel free to contact me about any feedback, questions, or maybe just to say “hi”."
  },
  {
    "objectID": "index1.html",
    "href": "index1.html",
    "title": "Psychometrics",
    "section": "",
    "text": "psychometrics\n\n\nSEM\n\n\n\n\nYEM: ‘Your View on Science’ ölçeğinden elde edilen 2015 PISA Turkiye örneklemi ile bir örnek çalışma. Bu çalışma Gazi üniversitesi’nde ‘Yapısal Eşitlik Modellemeleri’ dersi kapsamında rapor olarak hazırlanmıştır.\n\n\n\n\n\n\nOct 30, 2022\n\n\nAli Emre Karagül\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npsychometrics\n\n\nCTT\n\n\ntest equating\n\n\n\n\nEquating two test forms: This is a simple test equating study. The data used in this study is simulated from real data. We don’t use the real data for privacy purposes here.\n\n\n\n\n\n\nSep 20, 2022\n\n\nAli Emre Karagül\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npsychometrics\n\n\nIRT\n\n\n\n\nMadde Tepki Kuramı temelli madde analizi. Hem çok kategorili hem de iki kategorili maddeler için MTK temelli madde analizi uygulama süreçleri…\n\n\n\n\n\n\nNov 20, 2021\n\n\nAli Emre Karagül\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index2.html",
    "href": "index2.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Google Data\n\n\nMaps\n\n\nData-viz\n\n\n\n\nThis post is a part of a series that demonstrates how to gain insights from personal Google data. Its purpose is to show how to visualize the locations I visited within a time period on a map.\n\n\n\n\n\n\nNov 16, 2022\n\n\nAli Emre Karagül\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Item Analysis with Item Response Theory (in Turkish)/index.html",
    "href": "posts/Item Analysis with Item Response Theory (in Turkish)/index.html",
    "title": "Item Analysis with Item Response Theory (in Turkish)",
    "section": "",
    "text": "Bu çalışmada çok kategorili puanlanan maddelerden elde edilen bir veri seti kullanılmıştır. Çalışmanın ilk kısmında çok kategorili maddelere yönelik MTK analizleri yürütülmüştür. Daha sonra aynı veri seti iki kategorili verilere dönüştürülmüştür. Yine MTK süreçleri bu sefer de iki kategorili maddeler için yürütülmüştür. İzlenen adımlar şu şekildedir:\n1. Çoklu puanlanan maddelere yönelik olarak;\n2. Her bir maddeyi, kendi madde ortalamasından keserek 1-0 verisine dönüştürünüz. Buna göre iki kategorili puanlanan maddelere yönelik olarak;\n3. İlk iki araştırma sorusunda elde edilen birey yetenek puanları arasındaki korelasyon nasıldır?\n4. İlk iki araştırma sorusunda elde edilen birey yetenek puanları;"
  },
  {
    "objectID": "posts/Item Analysis with Item Response Theory (in Turkish)/index.html#to-be-continued-d",
    "href": "posts/Item Analysis with Item Response Theory (in Turkish)/index.html#to-be-continued-d",
    "title": "Item Analysis with Item Response Theory (in Turkish)",
    "section": "TO BE CONTINUED :D",
    "text": "TO BE CONTINUED :D"
  },
  {
    "objectID": "posts/Structural Equation Modelling (in Turkish)/index.html",
    "href": "posts/Structural Equation Modelling (in Turkish)/index.html",
    "title": "Structural Equation Modelling (in Turkish)",
    "section": "",
    "text": "Bu çalışmada PISA 2015 “student questionnaire: paper based verison” içerisindeki en son alt ölçek olan ‘Your View on Science’ ölçeğinden elde edilen veriler kullanılmıştır. Türkiye örnekleminden faydalanılmıştır. Bu örneklem ile bahsi geçen ölçeğin yapısal eşitlik modelleri oluşturulmuş ve karşılaştırılmıştır. Adım adım veri setinin R ile analize hazır hale getirilmesi anlatılmaktadır. Doğrudan analiz ile ilgileniyorsanız aşağıda ‘Analiz Süreci’ başlığına ilerleyiniz."
  },
  {
    "objectID": "posts/Structural Equation Modelling (in Turkish)/index.html#pre-processing-süreci",
    "href": "posts/Structural Equation Modelling (in Turkish)/index.html#pre-processing-süreci",
    "title": "Structural Equation Modelling (in Turkish)",
    "section": "Pre-processing süreci",
    "text": "Pre-processing süreci\nÖncelikle OECD tarafından yayınlanan veri setini şuradan indiriyoruz. İndirilen bu dosya .sav uzantılı bbir dosya. Bu tür dosyaları açmak için haven paketi read_sav fonksiyonundan faydalanabiliriz. Veri setinin tamamına ihtiyacımız yok, zaten oldukça büyük bir veri. Ülke değilşkeni ve ilgili ölçeğin maddeleri yeterli olacaktır. İndirdiğimiz .sav uzantılı dosyayı working directory’mize taşıdıktan sonra çalıştıracağımız kod satırı şunlar:\n\nlibrary(haven) \ndata <- na.omit(read_sav(\n  \"CY6_MS_CMB_STU_QQQ.sav\",\n  col_select = c(\n    \"CNT\",\n    \"ST092Q01TA\",\n    \"ST092Q02TA\",\n    \"ST092Q04TA\",\n    \"ST092Q05TA\",\n    \"ST092Q06NA\",\n    \"ST092Q08NA\",\n    \"ST092Q09NA\",\n    \n    \"ST094Q01NA\",\n    \"ST094Q02NA\",\n    \"ST094Q03NA\",\n    \"ST094Q04NA\",\n    \"ST094Q05NA\",\n    \n    \"ST113Q01TA\",\n    \"ST113Q02TA\",\n    \"ST113Q03TA\",\n    \"ST113Q04TA\",\n    \n    \"ST129Q01TA\",\n    \"ST129Q02TA\",\n    \"ST129Q03TA\",\n    \"ST129Q04TA\",\n    \"ST129Q05TA\",\n    \"ST129Q06TA\",\n    \"ST129Q07TA\",\n    \"ST129Q08TA\",\n    \n    \"ST131Q01NA\",\n    \"ST131Q03NA\",\n    \"ST131Q04NA\",\n    \"ST131Q06NA\",\n    \"ST131Q08NA\",\n    \"ST131Q11NA\"\n  )\n))\n\nÜlke değişkenine artık ihtiyacımız yok. O nedenle veri setimizi sadece ölçek maddelerini içerecek şekilde yeniden tanımlayalım:\n\nlibrary(dplyr)\nscience_data<-filter(data, CNT==\"TUR\")[,-1]\n\nBu veri setini şu kod satırını yürüterek .csv uzantılı bir dosya olarak bilgisayarıma kaydediyorum. Böylece analiz aşamasında o halini de paylaşabileceğim:\n\nwrite.csv(science_data, \"science_data.csv\", row.names = FALSE)"
  },
  {
    "objectID": "posts/Structural Equation Modelling (in Turkish)/index.html#analiz-süreci",
    "href": "posts/Structural Equation Modelling (in Turkish)/index.html#analiz-süreci",
    "title": "Structural Equation Modelling (in Turkish)",
    "section": "Analiz Süreci",
    "text": "Analiz Süreci\nİlgili ölçekten elde edilen veriyi .csv uzantılı dosya olarak indirebilirsiniz. Bunu read.csv fonksiyonu ile R ortamına aktarabilirsiniz. Veri setimiz hazır. Analizimizde öncelikle doğrulayı faktör analizi kullanacağız. Daha sonra da bi-faktör modelleme yapacağız. Veri setimizi tekrar yükleyelim:\n\n\n\n\nDoğrulayıcı Faktör Analizi (DFA)\nÇalışmamıza konu olan ölçek beş faktörden oluşmaktadır. Bu yapı ile ölçek geliştirme sürecinde tanımlanmıştır. Her faktörde farklı sayılarda maddeler yer almaktadır. Veri setinde bunlar ST ön eki ve faktör numarası ile tanımlanmıştır. Faktörleri ve veri setindeki kodlamalarını şu şekilde listeleyelim:\n\nST092: How informed are you about the following environmental issues?\nST094: How much do you disagree or agree with the statements about yourself below?\nST113: How much do you agree with the statements below?\nST129: How easy do you think it would be for you to perform the following tasks on your own?\nST131: How much do you disagree or agree with the statements below?\n\nBu beş faktörün altında tanımlanan yapıya göre DFA uygulayacağız ve bir geçerlilik çalışması yürüteceğiz. Bunun için öncelikle modelimizi R’a tanıtalım:\n\nScience.model <- 'ST092 =~ 1* ST092Q01TA+\n                              ST092Q02TA+\n                              ST092Q04TA+\n                              ST092Q05TA+\n                              ST092Q06NA+\n                              ST092Q08NA+\n                              ST092Q09NA\n                              \n                  ST094 =~ 1* ST094Q01NA+\n                              ST094Q02NA+\n                              ST094Q03NA+\n                              ST094Q04NA+\n                              ST094Q05NA\n                              \n                  ST113 =~ 1* ST113Q01TA+\n                              ST113Q02TA+\n                              ST113Q03TA+\n                              ST113Q04TA\n                  \n                  ST129 =~ 1* ST129Q01TA+\n                              ST129Q02TA+\n                              ST129Q03TA+\n                              ST129Q04TA+\n                              ST129Q05TA+\n                              ST129Q06TA+\n                              ST129Q07TA+\n                              ST129Q08TA\n                              \n                  ST131 =~ 1* ST131Q01NA+\n                              ST131Q03NA+\n                              ST131Q04NA+\n                              ST131Q06NA+\n                              ST131Q08NA+\n                              ST131Q11NA            \n                    \n                           \n                            ST092 ~~ ST094\n                            ST092 ~~ ST113\n                            ST092 ~~ ST129\n                            ST092 ~~ ST131\n                            \n                            ST094 ~~ ST113\n                            ST094 ~~ ST129\n                            ST094 ~~ ST131\n                            \n                            ST113 ~~ ST129\n                            ST113 ~~ ST131\n                            \n                            ST129 ~~ ST131'\n\nŞimdi Türkiye örnekleminden elde edilen verimizin bu modele uyum sağlayıp sağlamadığına bakacağız. Bu aşamada çeşitli modelleme yaklaşımlarından çıktılar alarak bunları karşılaştıracağız. Bu modelleme yaklaşımları şunlardır:\n\nmaximum likelihood model (MLM)\nweighted least squares (WLS)\nrobust maximum likelihood model (RMLM)\ndiagonally weighted least squares (DWLS)\n\nYukarıdaki lsitede görüldüğü sıra ile modellerimizi lavaan paketiyle oluşturalım:\n\nlibrary(lavaan)\nmodel_mlm <- cfa(Science.model, data = science_data)\nmodel_wls <- cfa(Science.model, WLS.V = TRUE, data = science_data)\nmodel_rml <- cfa(Science.model, estimator = \"MLM\", se = \"robust.mlm\", data = science_data)\nmodel_dwls<-cfa(Science.model, data = science_data, estimator=\"DWLS\")\n\nHer biri için de ayrı ayrı analiz çıktılarını summary() fonksiyonu ile tanımlayalım:\n\nscience.mlm<-summary(\n  model_mlm,\n  fit.measures = TRUE,\n  standardized = TRUE,\n  rsquare = TRUE,\n  modindices = TRUE\n)\nscience.wls<-summary(\n  model_wls,\n  fit.measures = TRUE,\n  standardized = TRUE,\n  rsquare = TRUE,\n  modindices = TRUE\n)\nscience.rml<-summary(\n  model_rml,\n  fit.measures = TRUE,\n  standardized = TRUE,\n  rsquare = TRUE,\n  modindices = TRUE\n)\nscience.dwls<-summary(\n  model_dwls,\n  fit.measures = TRUE,\n  standardized = TRUE,\n  rsquare = TRUE,\n  modindices = TRUE\n)\n\nTabi ki hem literatürde en yaygın kullanılan hem de alan uzmanları tarafından en ok önerilen yöntem olması sebebiyle MLM yöntemi önceliğimiz. Bu yöntemde normallik varsayımı karşılandığı sürece güçlü analizler elde edilebilmektedir. Veri setimiz de büyük bir örneklemden elde edildiği için bu varsayımın karşılandığı düşünülmektedir. Grafik incelemelerinde de bu durum görülecektir. Dolayısıyla öncelikle MLM’ye ait uyum indekslerini görelim:\n\nscience.mlm[[\"fit\"]]\n\n\n\nThis is lavaan 0.6-12\nlavaan is FREE software! Please report any bugs.\n\n\n             npar              fmin             chisq                df \n           70.000             0.652          6055.193           395.000 \n           pvalue    baseline.chisq       baseline.df   baseline.pvalue \n            0.000         96221.017           435.000             0.000 \n              cfi               tli              logl unrestricted.logl \n            0.941             0.935       -135531.110       -132503.514 \n              aic               bic            ntotal              bic2 \n       271202.220        271653.284          4646.000        271430.850 \n            rmsea    rmsea.ci.lower    rmsea.ci.upper      rmsea.pvalue \n            0.056             0.054             0.057             0.000 \n             srmr \n            0.039 \n\n\nBu çıktılar incelendiğinde, 70 parametreli 395 serbestlik derecesinde bir model oluştuğu görülmektedir. CFI ve TLI uyum indekleri .90 eşik değerin üzerindeyken, RMSEA ve SRMR .06’nın altında yer almaktadır. Bu durumda modelimizin uyumlu olduğu düşünülebilir. Yine de diğer modeller ile de karşılaştırmak gerekir. Onların uyum indeklerini de görelim:\nWLS model:\n\nscience.wls[[\"fit\"]]\n\n\n\n             npar              fmin             chisq                df \n           70.000             0.652          6055.193           395.000 \n           pvalue    baseline.chisq       baseline.df   baseline.pvalue \n            0.000         96221.017           435.000             0.000 \n              cfi               tli              logl unrestricted.logl \n            0.941             0.935       -135531.110       -132503.514 \n              aic               bic            ntotal              bic2 \n       271202.220        271653.284          4646.000        271430.850 \n            rmsea    rmsea.ci.lower    rmsea.ci.upper      rmsea.pvalue \n            0.056             0.054             0.057             0.000 \n             srmr \n            0.039 \n\n\nRML model:\n\nscience.rml[[\"fit\"]]\n\n\n\n                         npar                          fmin \n                       70.000                         0.652 \n                        chisq                            df \n                     6055.193                       395.000 \n                       pvalue                  chisq.scaled \n                        0.000                      4484.352 \n                    df.scaled                 pvalue.scaled \n                      395.000                         0.000 \n         chisq.scaling.factor                baseline.chisq \n                        1.350                     96221.017 \n                  baseline.df               baseline.pvalue \n                      435.000                         0.000 \n        baseline.chisq.scaled            baseline.df.scaled \n                    75815.224                       435.000 \n       baseline.pvalue.scaled baseline.chisq.scaling.factor \n                        0.000                         1.269 \n                          cfi                           tli \n                        0.941                         0.935 \n                   cfi.scaled                    tli.scaled \n                        0.946                         0.940 \n                   cfi.robust                    tli.robust \n                        0.942                         0.936 \n                         logl             unrestricted.logl \n                  -135531.110                   -132503.514 \n                          aic                           bic \n                   271202.220                    271653.284 \n                       ntotal                          bic2 \n                     4646.000                    271430.850 \n                        rmsea                rmsea.ci.lower \n                        0.056                         0.054 \n               rmsea.ci.upper                  rmsea.pvalue \n                        0.057                         0.000 \n                 rmsea.scaled         rmsea.ci.lower.scaled \n                        0.047                         0.046 \n        rmsea.ci.upper.scaled           rmsea.pvalue.scaled \n                        0.048                         1.000 \n                 rmsea.robust         rmsea.ci.lower.robust \n                        0.055                         0.053 \n        rmsea.ci.upper.robust           rmsea.pvalue.robust \n                        0.056                            NA \n                         srmr \n                        0.039 \n\n\nDWLS model:\n\nscience.dwls[[\"fit\"]]\n\n\n\n           npar            fmin           chisq              df          pvalue \n         70.000           0.188        1744.348         395.000           0.000 \n baseline.chisq     baseline.df baseline.pvalue             cfi             tli \n     120867.135         435.000           0.000           0.989           0.988 \n          rmsea  rmsea.ci.lower  rmsea.ci.upper    rmsea.pvalue            srmr \n          0.027           0.026           0.028           1.000           0.034 \n\n\nTüm bu çıktılar incelendiğinde, en uygun modelin diagonally weighted least squares (DWLS) olduğu düşünülmektedir. Veri setinin farklı ölçek düzeyinde olması ve kategorik olması bu durumun sebebi olabilir. DWLS modeli, liteatürde yaygın bir şekilde bu tür veri setleri için önerilmektedir.\n\n\nBI-FACTOR MODELLER\nBi faktör modellemede yapıyı oluşturan faktörlerin tüm maddelerden oluşan genel faktör ile ilişkisi incelenerek karar verilir. Bu amaçla DFA örneğinde olduğu gibi model tanımlamamızı yapıyoruz:\n\nScience.bifactormodel <- 'general.factor =~\n                              ST092Q01TA+\n                              ST092Q02TA+\n                              ST092Q04TA+\n                              ST092Q05TA+\n                              ST092Q06NA+\n                              ST092Q08NA+\n                              ST092Q09NA+\n                              ST094Q01NA+\n                              ST094Q02NA+\n                              ST094Q03NA+\n                              ST094Q04NA+\n                              ST094Q05NA+\n                              ST113Q01TA+\n                              ST113Q02TA+\n                              ST113Q03TA+\n                              ST113Q04TA+\n                              ST129Q01TA+\n                              ST129Q02TA+\n                              ST129Q03TA+\n                              ST129Q04TA+\n                              ST129Q05TA+\n                              ST129Q06TA+\n                              ST129Q07TA+\n                              ST129Q08TA+\n                              ST131Q01NA+\n                              ST131Q03NA+\n                              ST131Q04NA+\n                              ST131Q06NA+\n                              ST131Q08NA+\n                              ST131Q11NA\n\n                  ST092 =~ 1* ST092Q01TA+\n                              ST092Q02TA+\n                              ST092Q04TA+\n                              ST092Q05TA+\n                              ST092Q06NA+\n                              ST092Q08NA+\n                              ST092Q09NA\n\n                  ST094 =~ 1* ST094Q01NA+\n                              ST094Q02NA+\n                              ST094Q03NA+\n                              ST094Q04NA+\n                              ST094Q05NA\n\n                  ST113 =~ 1* ST113Q01TA+\n                              ST113Q02TA+\n                              ST113Q03TA+\n                              ST113Q04TA\n\n                  ST129 =~ 1* ST129Q01TA+\n                              ST129Q02TA+\n                              ST129Q03TA+\n                              ST129Q04TA+\n                              ST129Q05TA+\n                              ST129Q06TA+\n                              ST129Q07TA+\n                              ST129Q08TA\n\n                  ST131 =~ 1* ST131Q01NA+\n                              ST131Q03NA+\n                              ST131Q04NA+\n                              ST131Q06NA+\n                              ST131Q08NA+\n                              ST131Q11NA\n                              \n\n                general.factor  ~~ 0*ST092\n                general.factor  ~~ 0*ST094\n                general.factor  ~~ 0*ST113\n                general.factor  ~~ 0*ST129\n                general.factor  ~~ 0*ST131\n                \n                            ST092 ~~ ST094\n                            ST092 ~~ ST113\n                            ST092 ~~ ST129\n                            ST092 ~~ ST131\n\n                            ST094 ~~ ST113\n                            ST094 ~~ ST129\n                            ST094 ~~ ST131\n\n                            ST113 ~~ ST129\n                            ST113 ~~ ST131\n\n                            ST129 ~~ ST131'\n\nArdından modellerimizi veri setimiz ile sınıyoruz.\n\nScience.bifactormodel_mlm <-\n  cfa(\n    Science.bifactormodel,\n    data = science_data,\n    std.lv = TRUE,\n    information = \"observed\"\n  )\nScience.bifactormodel_rml <-\n  cfa(\n    Science.bifactormodel,\n    data = science_data,\n    estimator = \"MLM\",\n    se = \"robust.mlm\",\n    std.lv = TRUE,\n    information = \"observed\"\n  )\nbifactor_mlm <- summary(Science.bifactormodel_mlm ,\n        fit.measures = TRUE,\n        standardized = TRUE)\nbifactor_rml <- summary(Science.bifactormodel_rml ,\n        fit.measures = TRUE,\n        standardized = TRUE)\n\nbifactor_mlm[['fit']]\nbifactor_rml[['fit']]\n\nBI-FAKTÖR ML model:\n\n\n             npar              fmin             chisq                df \n           95.000             0.634          5891.081           370.000 \n           pvalue    baseline.chisq       baseline.df   baseline.pvalue \n            0.000         96221.017           435.000             0.000 \n              cfi               tli              logl unrestricted.logl \n            0.942             0.932       -135449.054       -132503.514 \n              aic               bic            ntotal              bic2 \n       271088.109        271700.266          4646.000        271398.392 \n            rmsea    rmsea.ci.lower    rmsea.ci.upper      rmsea.pvalue \n            0.057             0.055             0.058             0.000 \n             srmr \n            0.328 \n\n\nBI-FAKTÖR ROBUST ML model:\n\n\n                         npar                          fmin \n                       95.000                         0.634 \n                        chisq                            df \n                     5891.081                       370.000 \n                       pvalue                  chisq.scaled \n                        0.000                      4166.363 \n                    df.scaled                 pvalue.scaled \n                      370.000                         0.000 \n         chisq.scaling.factor                baseline.chisq \n                        1.414                     96221.017 \n                  baseline.df               baseline.pvalue \n                      435.000                         0.000 \n        baseline.chisq.scaled            baseline.df.scaled \n                    19393.334                       435.000 \n       baseline.pvalue.scaled baseline.chisq.scaling.factor \n                        0.000                         4.962 \n                          cfi                           tli \n                        0.942                         0.932 \n                   cfi.scaled                    tli.scaled \n                        0.800                         0.765 \n                   cfi.robust                    tli.robust \n                        0.943                         0.933 \n                         logl             unrestricted.logl \n                  -135449.054                   -132503.514 \n                          aic                           bic \n                   271088.109                    271700.266 \n                       ntotal                          bic2 \n                     4646.000                    271398.392 \n                        rmsea                rmsea.ci.lower \n                        0.057                         0.055 \n               rmsea.ci.upper                  rmsea.pvalue \n                        0.058                         0.000 \n                 rmsea.scaled         rmsea.ci.lower.scaled \n                        0.047                         0.046 \n        rmsea.ci.upper.scaled           rmsea.pvalue.scaled \n                        0.048                         1.000 \n                 rmsea.robust         rmsea.ci.lower.robust \n                        0.056                         0.054 \n        rmsea.ci.upper.robust           rmsea.pvalue.robust \n                        0.057                            NA \n                         srmr \n                        0.328 \n\n\nBu çıktılar incelendiğinde de bi faktör modelin verimize DWLS model kadar uyumlu olmadığı görülmektedir. Bu nedenle DWLS modelin en uyumlu model olduğu düşünülmektedir. Bu modelin çıktılarını tablolaştıralım.\n\n\nEN UYUMLU MODEL:DWLS çıktıları\nNOT: Faktör varyansları sabitlenerek model oluşturulduğu için faktör varyansları tablosu raporlaştırılmamıştır.\n\nModel uyum indeksleri\n\nParametre Tahminleri\n\n\n\nFaktör Kovaryansları\n\n\n\nArtık (residual) Varyanslar\n\n\n\nModel gösterimi"
  },
  {
    "objectID": "posts/Test Equating/index.html",
    "href": "posts/Test Equating/index.html",
    "title": "Test Equating",
    "section": "",
    "text": "2020-2021 Fall Term A Level’s first quiz has 40 items. 2022-2023 Fall Term A Level’s first quiz has 40 items. 35 of the items in each test forms are unique items while 5 of them are common, thus will be called as “anchor items” in this study. For the readers interest, the items belong to four main domains (listening, structure, vocabbulary and reading), yet the common items are only in the reading section. This is obviously a violation of assumptions of test equating. Still, this study is conveyed for demonstration purposes. Therefore, let’s continue:\nTo ensure statistical equation of these two forms, we first introduced the data in R Studio and the first five rows can be seen below:\n\nQ1 <- read.csv(\"kitap1.csv\", header = TRUE)\nhead(Q1)\n\n  L1 L2 L3 L4 L5 L6 L7 L8 L9 L10 S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 V1 V2 V3 V4 V5\n1  0  1  0  1  1  1  0  1  1   0  1  1  0  1  0  0  0  0  0   1  0  1  1  0  1\n2  1  1  1  1  1  1  1  1  1   1  0  1  1  0  1  1  0  0  0   0  1  1  1  1  0\n3  1  1  1  1  1  1  0  1  1   0  1  1  0  1  1  0  0  1  1   0  0  1  0  1  0\n4  1  1  1  1  1  1  1  0  1   0  1  1  0  1  1  0  1  1  0   0  0  1  1  0  1\n5  1  1  1  1  1  1  1  0  1   1  1  1  1  1  0  1  0  1  0   0  1  1  1  0  0\n6  1  1  1  1  0  0  1  1  1   1  0  0  0  0  0  1  1  1  1   1  1  1  1  0  0\n  V6 V7 V8 V9 V10 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 form\n1  0  1  1  1   1  1  1  0  1  1  1  1  1  1   1    x\n2  0  1  1  0   0  1  0  1  1  0  1  1  0  1   1    x\n3  1  0  1  1   1  1  0  0  1  1  1  0  1  1   1    x\n4  1  1  1  0   1  1  1  1  1  0  1  0  0  1   1    x\n5  1  1  1  0   1  1  1  1  0  1  1  1  0  1   1    x\n6  1  1  1  1   1  1  1  1  1  1  1  1  1  1   1    x\n\n\nLater, we introduced the unique and anchor items separately. First 35 items are unique items, and the last 5 items are anchor items.\n\n# Calculate total scores based on unique items\nQ1$total <- rowSums(Q1[, 1:35])\n\n# Calculate scores based on anchor items\nQ1$anchor <- rowSums(Q1[, 36:40])\n\nAs we will use the equate package, the data should be contained as frequency tables: Form x (20-21 fall) had a sample of 200 while form y (22-23 fall) had a sample of 133 students. They are defined as:\n\n#first introduce the equate package:\nlibrary(equate)\n# Create frequency tables (total score range: 0-35; anchor score range: 0-5)\nQ1_x <- freqtab(Q1[1:200, c(\"total\", \"anchor\")], scales = list(0:35, 0:5))\nQ1_y <- freqtab(Q1[201:334, c(\"total\", \"anchor\")], scales = list(0:35, 0:5))\n\nTo consideration of the reader one more time, we must state that these forms are the first quizzes of the students. They usually get high marks. For instance, you can see below that the students’ scores are distributed left-skewed in both forms. Their total correct answers are distributed between 20 and 35 for the unique items , and between 3 and 5 for anchor items in form X. The situation isn’t different for form y.\n\n#distrubution of the data among forms and unique/common items\nplot(Q1_x, xlab = \"Total Scores Form X\", ylab = \"Common Anchor Scores Form X\")\n\n\n\nplot(Q1_y, xlab = \"Total Scores Form y\", ylab = \"Common Anchor Scores Form y\")\n\n\n\n\nStill, let’s continue… with the smoothing procedure. For both forms, we utilized loglinear presmoothing. Of course there are several other methods, yet literature shows not much a big difference between them, thus not much care given to this issue. again as this is a study with demonstration purposes. After the smoothing, it can be realized that the distribution is highly eye-pleasing right now. Also it is much easier to match the scores even if there isn’t an equivalent of it in the other form.\n\n#PRESMOOTHING\nsmooth_x <- presmoothing(Q1_x, smoothmethod = \"loglinear\")\n\nWarning: glm.fit: fitted rates numerically 0 occurred\n\nsmooth_y <- presmoothing(Q1_y, smoothmethod = \"loglinear\")\n\nWarning: glm.fit: algorithm did not converge\n\nWarning: glm.fit: fitted rates numerically 0 occurred\n\nplot(smooth_x, xlab = \"Total Scores Form X\", ylab = \"Common Anchor Scores Form X\")\n\n\n\nplot(smooth_y, xlab = \"Total Scores Form y\", ylab = \"Common Anchor Scores Form y\")\n\n\n\n\nNow, it can be roughly said that the forms are ready to be equated. Before we try several methods, lets see the results of the Tucker method as it can produce equating error as well:\n\n## Linear Tucker Equating\nQ1_tucker <- equate(Q1_x, Q1_y, type = \"linear\", method = \"tucker\")\nQ1_tucker$concordance\n\n   scale        yx      se.n      se.g\n1      0  6.597617 2.0378528 3.3547355\n2      1  7.404998 1.9751403 3.2551073\n3      2  8.212379 1.9124541 3.1554875\n4      3  9.019759 1.8497970 3.0558770\n5      4  9.827140 1.7871721 2.9562768\n6      5 10.634520 1.7245827 2.8566879\n7      6 11.441901 1.6620331 2.7571116\n8      7 12.249282 1.5995277 2.6575492\n9      8 13.056662 1.5370720 2.5580024\n10     9 13.864043 1.4746724 2.4584730\n11    10 14.671423 1.4123363 2.3589634\n12    11 15.478804 1.3500723 2.2594761\n13    12 16.286185 1.2878911 2.1600141\n14    13 17.093565 1.2258052 2.0605812\n15    14 17.900946 1.1638299 1.9611818\n16    15 18.708326 1.1019838 1.8618212\n17    16 19.515707 1.0402899 1.7625060\n18    17 20.323088 0.9787772 1.6632444\n19    18 21.130468 0.9174818 1.5640464\n20    19 21.937849 0.8564507 1.4649252\n21    20 22.745229 0.7957445 1.3658973\n22    21 23.552610 0.7354438 1.2669847\n23    22 24.359991 0.6756570 1.1682166\n24    23 25.167371 0.6165338 1.0696330\n25    24 25.974752 0.5582849 0.9712903\n26    25 26.782132 0.5012154 0.8732696\n27    26 27.589513 0.4457784 0.7756933\n28    27 28.396894 0.3926659 0.6787528\n29    28 29.204274 0.3429596 0.5827655\n30    29 30.011655 0.2983669 0.4882941\n31    30 30.819035 0.2615166 0.3964236\n32    31 31.626416 0.2360629 0.3094791\n33    32 32.433797 0.2258919 0.2330408\n34    33 33.241177 0.2330134 0.1809526\n35    34 34.048558 0.2559883 0.1763087\n36    35 34.855938 0.2910866 0.2221053\n\n\nYou will see that the equating errors are above 1 before the score of 25 as there isn’t much data in the low scores. Also, as we investigate the lower marks, we see that the gap between equated scores are increasing. For instance, 0 on form X is equal to 6.597617 on form Y. This is because there isn’t data in these regions of the scores. Despite that, equated scores get more meaningful after 20. Especially after the total score 30, the equated scores are too close and the equation error is too low, which would be quite better if the situation was like that on all total score ranges. Let’s see some other equating methods:\n\n## Comparing Multiple Methods\n# Nominal method with mean equating\nQ1_nom <- equate(Q1_x, Q1_y, type = \"mean\", method = \"nom\")\n\n# Frequency method with equipercentile\nQ1_freq <- equate(Q1_x, Q1_y, type = \"equip\", method = \"freq\")\n\n# Braun method with linear equating\nQ1_braun <- equate(Q1_x, Q1_y, type = \"linear\", method = \"braun\")\n\n# Compare equated scores\nround(cbind(xscale = 0:35, \n            nominal = Q1_nom$concordance$yx,\n            tucker = Q1_tucker$concordance$yx, \n            freq = Q1_freq$concordance$yx, \n            braun = Q1_braun$concordance$yx), 2)\n\n      xscale nominal tucker  freq braun\n [1,]      0   -0.01   6.60 -0.50  5.65\n [2,]      1    0.99   7.40 -0.50  6.49\n [3,]      2    1.99   8.21 -0.50  7.32\n [4,]      3    2.99   9.02 -0.50  8.16\n [5,]      4    3.99   9.83 -0.50  9.00\n [6,]      5    4.99  10.63 -0.50  9.83\n [7,]      6    5.99  11.44 -0.50 10.67\n [8,]      7    6.99  12.25 -0.50 11.50\n [9,]      8    7.99  13.06 -0.50 12.34\n[10,]      9    8.99  13.86 -0.50 13.17\n[11,]     10    9.99  14.67 -0.50 14.01\n[12,]     11   10.99  15.48 -0.50 14.84\n[13,]     12   11.99  16.29 -0.50 15.68\n[14,]     13   12.99  17.09 -0.50 16.51\n[15,]     14   13.99  17.90 -0.50 17.35\n[16,]     15   14.99  18.71 -0.50 18.19\n[17,]     16   15.99  19.52 -0.50 19.02\n[18,]     17   16.99  20.32 -0.50 19.86\n[19,]     18   17.99  21.13 -0.50 20.69\n[20,]     19   18.99  21.94 -0.50 21.53\n[21,]     20   19.99  22.75 -0.50 22.36\n[22,]     21   20.99  23.55 23.66 23.20\n[23,]     22   21.99  24.36 23.82 24.03\n[24,]     23   22.99  25.17 24.10 24.87\n[25,]     24   23.99  25.97 24.38 25.71\n[26,]     25   24.99  26.78 24.57 26.54\n[27,]     26   25.99  27.59 25.35 27.38\n[28,]     27   26.99  28.40 27.28 28.21\n[29,]     28   27.99  29.20 29.14 29.05\n[30,]     29   28.99  30.01 30.65 29.88\n[31,]     30   29.99  30.82 31.34 30.72\n[32,]     31   30.99  31.63 31.76 31.55\n[33,]     32   31.99  32.43 32.35 32.39\n[34,]     33   32.99  33.24 33.09 33.22\n[35,]     34   33.99  34.05 33.88 34.06\n[36,]     35   34.99  34.86 34.86 34.90\n\n\nAlthough the equating methods vary, the results are similar to those of Tucker method. Especially Frequency Estimation method shows how important it is to have data in different score ranges because there is no meaningful equation before the scale score of 20 and all lower scores are equated to -.5 in this method. Let’s also see the plotting of the chart above:\n\n# Plot the results\nplot(Q1_tucker, Q1_nom, Q1_freq, Q1_braun, lty=c(1,2,3,4),\n     col=c(\"blue\", \"black\", \"red\", \"forestgreen\"), addident = FALSE)\n\n\n\n\nAs also can be seen in the plot above, after the scale score of 20, all equating methods are quite similar to each other. Scores lower than 20 are equated with linear methods much better than the equi-percentile method as there isn’t adequate data in those score ranges.\nThis study is conducted for demonstrative purposes and still we can say that scale scores over 30 can be equated in the given forms."
  },
  {
    "objectID": "posts2/Visualisation of My Personal Google Data (1)/index.html",
    "href": "posts2/Visualisation of My Personal Google Data (1)/index.html",
    "title": "Visualisation of My Personal Google Data (1): My Locations",
    "section": "",
    "text": "If you give permission to Google to store your location data, they will keep them in their databases forever. You can also allow them to store it for a while and then ask them delete it. They will directly do so.\nWhat makes this study a fun project is its being very personal. I decided to analyze my personal data in August, 2022. Therefore, I granted many new permissions to Google along with many previously granted permissions. They keep them in various formats including .csv, .json, .mbox etc. When you query for your personal data, they provide it within a couple of days depending on the size of the data you queried.\nUsually, I provide the readers with the data in my posts. However, in this series, the data are very personal and so I will not."
  },
  {
    "objectID": "posts2/Visualisation of My Personal Google Data (1)/index.html#introduction-my-locations",
    "href": "posts2/Visualisation of My Personal Google Data (1)/index.html#introduction-my-locations",
    "title": "Visualisation of My Personal Google Data (1): My Locations",
    "section": "Introduction: “My Locations”",
    "text": "Introduction: “My Locations”\nIn this part of the series, we will investigate my personal location data. We will visualize the spots I visited within a period of time. This way, I personally will gain insights about how boring my days are :)\nThe R packages that we use in this post are as follows: rjson, tidyr, dplyr, purrr, lubridate, sp and leaflet.\n\n##packs for data processing\nlibrary(rjson)      # to read .JSON files.\nlibrary(tidyr)      # to process data\nlibrary(dplyr)      # to process data\nlibrary(purrr)      # to process data\nlibrary(lubridate)  # to deal with date variables\n#packs for data viz\nlibrary(sp)         # a pack for spatial objects\nlibrary(leaflet)    # map and its functions\n\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\n\nWarning: package 'leaflet' was built under R version 4.2.2"
  },
  {
    "objectID": "posts2/Visualisation of My Personal Google Data (1)/index.html#understand-the-data",
    "href": "posts2/Visualisation of My Personal Google Data (1)/index.html#understand-the-data",
    "title": "Visualisation of My Personal Google Data (1): My Locations",
    "section": "Understand the Data",
    "text": "Understand the Data\nInside the takeout folder that I received from Google, there is a folder named “Location History”. Inside it, “Semantic Location History” contains the location data based on the months and years. From that folder, I have called the locations I visited in November. Thus, we will use 2022_NOVEMBER.json file. Let’s investigate the data. Start with reading the file into R environment.\n\nmy_locations <- fromJSON(file = \"2022_NOVEMBER.json\")\n\nThen, let’s try to understand the structure of the data, how and what kind of information is stored into its cells. The list object my_locations contains many lists inside it. Let’s try to understand each one of them one by one:\n\nsummary(my_locations[[1]])\n\n      Length Class  Mode\n [1,] 1      -none- list\n [2,] 1      -none- list\n [3,] 1      -none- list\n [4,] 1      -none- list\n [5,] 1      -none- list\n [6,] 1      -none- list\n [7,] 1      -none- list\n [8,] 1      -none- list\n [9,] 1      -none- list\n[10,] 1      -none- list\n[11,] 1      -none- list\n[12,] 1      -none- list\n[13,] 1      -none- list\n[14,] 1      -none- list\n[15,] 1      -none- list\n[16,] 1      -none- list\n[17,] 1      -none- list\n\n\nThere are many smaller lists in the first indexed list. Let’s try the first one and see what’s inside:\n\nsummary(my_locations[[1]][[1]])\n\n           Length Class  Mode\nplaceVisit 11     -none- list\n\n\nThere is a single list inside. Sad :( Let’s dive one more step:\n\nsummary(my_locations[[1]][[1]][[1]])\n\n                        Length Class  Mode     \nlocation                8      -none- list     \nduration                2      -none- list     \nplaceConfidence         1      -none- character\ncenterLatE7             1      -none- numeric  \ncenterLngE7             1      -none- numeric  \nvisitConfidence         1      -none- numeric  \notherCandidateLocations 4      -none- list     \neditConfirmationStatus  1      -none- character\nlocationConfidence      1      -none- numeric  \nplaceVisitType          1      -none- character\nplaceVisitImportance    1      -none- character\n\n\nFinally, here we have several items. There is a list called location containing 8 items inside. There is duration with 2 items and otherCandidateLocations with 4 items. Other lists contain only one item each. Let’s check these one by one:\n\nsummary(my_locations[[1]][[1]][[1]]$location)\n\n                      Length Class  Mode     \nlatitudeE7            1      -none- numeric  \nlongitudeE7           1      -none- numeric  \nplaceId               1      -none- character\naddress               1      -none- character\nsemanticType          1      -none- character\nsourceInfo            1      -none- list     \nlocationConfidence    1      -none- numeric  \ncalibratedProbability 1      -none- numeric  \n\n\n\nsummary(my_locations[[1]][[1]][[1]]$duration)\n\n               Length Class  Mode     \nstartTimestamp 1      -none- character\nendTimestamp   1      -none- character\n\n\n\nsummary(my_locations[[1]][[1]][[1]]$otherCandidateLocations)\n\n     Length Class  Mode\n[1,] 7      -none- list\n[2,] 7      -none- list\n[3,] 7      -none- list\n[4,] 7      -none- list\n\n\nWe can obtain much information through this investigation process. For instance, inside the location I can see information about the latitude, longitude, address, the confidence that I have to this place, and some other. Here, if you are following along with me, please spare some time to understand your data. Delve into them and digest as much information as you can. I will see you in the next section: data processing."
  },
  {
    "objectID": "posts2/Visualisation of My Personal Google Data (1)/index.html#pre-processing",
    "href": "posts2/Visualisation of My Personal Google Data (1)/index.html#pre-processing",
    "title": "Visualisation of My Personal Google Data (1): My Locations",
    "section": "Pre-processing",
    "text": "Pre-processing\nYou can use as many items as you want in your work. You should decide the meaningful information while understanding your data. Now let’s re-define our lists as a dataframe.\n\ndf <- map_dfr(my_locations[[\"timelineObjects\"]], as.data.frame)\nView(df)\n# there is one empty row after each entry. Let's drop them through one of the complete columns:\ndf <- drop_na(df, placeVisit.location.latitudeE7)\n\nThere are many columns, some of which I won’t need. Especially, I am not interested in the locations defined as “candidate”. I will exclude them from my study. They are probably the locations that might be the place that I visited ordered by possibility. I just need the one with the highest possibility, which is tagged with placeVisit.location. . These locations are also defined as “HIGH CONFIDENCE”. Let’s continue the analysis with these locations, only.\nAlso, there are some columns with no entry. Let me exclude them with a function. Let the function be called not_all_na. This is a function that drops all the columns which are completely empty:\n\nnot_all_na <- function(x)\n  any(!is.na(x))\n#use the function on the dataframe:\ndf <- df %>% select(where(not_all_na))\n\nNow, I have a dataframe with 150+ columns. However, I just need the information about latitude, altitude, date and address of the locations that I visited. Let’s write a query to get this data into a new dataframe:\n\nlat <- select(df, contains(\"placeVisit.location.latitudeE7\"))\nlon <- select(df, contains(\"placeVisit.location.longitudeE7\"))\naddress <- select(df, contains(\"placeVisit.location.address\"))\ndate <- select(df, contains(\"placeVisit.duration.startTimestamp\"))\n\nThe chunks of code above ask for columns whose names contain the extensions written in quotation marks in them. Still, this raw information isn’t enough for several reasons. Firstly, lat and lot are coordinates in E7 format. With a quick research on the internet, I learned that they simply need to be divided by 10000000. Also, date contains day, month, year, hour, minute, second and time zone (which is in GMT+0 format) information all in the same column. They need to be handled. Let’s start with the second issue (the one about date):\n\n#re-name the only column:\nnames(date) <- \"Date\"\nhead(date)\n\n                      Date\n1 2022-11-06T13:12:38.091Z\n2 2022-11-06T13:24:32.338Z\n3 2022-11-06T13:59:00.539Z\n4 2022-11-06T14:17:15.462Z\n5 2022-11-06T14:39:29.138Z\n6 2022-11-07T05:32:14.277Z\n\n\nAs can be seen above, there are two separators: One is “T” separating day and time info. The other is “.” separating time and time zone info. Follow the notes in the code to grasp the process:\n\n#divide the day and hour info from the time zone info, then drop the time zone:\ndate <-\n  separate(\n    data = date,\n    col = Date,\n    into = c(\"Date\", \"zone\"),\n    sep = \"\\\\.\"\n  )\ndate <- date[-c(2)]\n\n#Now, transform the time in local time zone which is GMT+3:\ndate$Date<-as.POSIXct(date$Date, format=\"%Y-%m-%dT%H:%M:%S\", tz=Sys.timezone())+ hours(3)\n\n#divide the day and hour info:\ndate <-\n  separate(\n    data = date,\n    col = Date,\n    into = c(\"Day\", \"Hour\"),\n    sep = \" \"\n  )\n#see the new format:\nhead(date)\n\n         Day     Hour\n1 2022-11-06 16:12:38\n2 2022-11-06 16:24:32\n3 2022-11-06 16:59:00\n4 2022-11-06 17:17:15\n5 2022-11-06 17:39:29\n6 2022-11-07 08:32:14\n\n\nNicely done! Now gather all the information that we need into a dataframe. Again follow along the notes in the code:\n\ncoords <-\n  drop_na(data.frame(\n    lat = unlist(lat, use.names = FALSE) / 10000000, #divide lat and lon by 10000000 to get rid of the E7 format\n    lon = unlist(lon, use.names = FALSE) / 10000000, \n    address = unlist(address, use.names = FALSE),\n    date # we processed this before\n  ))\n\nSo far, we have worked to prepare for the data visualization process. Our data is ready with the name coords. Let’s continue with the visualization."
  },
  {
    "objectID": "posts2/Visualisation of My Personal Google Data (1)/index.html#data-visualization",
    "href": "posts2/Visualisation of My Personal Google Data (1)/index.html#data-visualization",
    "title": "Visualisation of My Personal Google Data (1): My Locations",
    "section": "Data Visualization",
    "text": "Data Visualization\nAt this point, we will visualize the locations I visited in November of 2022 on a world map. You can’t be as disappointed as me when you see that I live a life between home and work. Yet, the point here is to see the process of visualization. We owe this beautiful project to the R package leaflet. It is actually a javascript library, all its arguments are deployed into R environment too. Therefore, we can work with it. If you are still with me, I mhighly recommend you to read the documentation of the package leaflet. Then, follow along the notes in the code and try to understand it if you are not familiar with it.\n\ncoordinates(coords) <- ~ lon + lat\nleaflet(coords,\n\n# formating the outer of the map:\n        width = \"800px\",\n        height = \"400px\", \n        padding = 10) %>% \n  addTiles() %>%\n\n#formating the markers on the map:\n  addCircleMarkers(\n    color = \"tomato\", #my favorite colour\n    fillOpacity = 1,\n    radius = 7,\n    stroke = FALSE,\n    \n#address pops up when you click on a marker:\n    popup = coords$address,\n\n#the date and hour shows up with a fancy personal note when you hover on a marker:\n    label =  paste0(\"I have been around here on \", coords$Day, \" at around \", coords$Hour),\n\n#formating the label that shows up when you hover:\n    labelOptions = labelOptions(\n      noHide = F,\n      direction = \"top\",\n      style = list(\n        \"color\" = \"black\",\n        \"font-family\" = \"calibri\", #I love calibri\n        \"box-shadow\" = \"3px 3px rgba(0,0,0,0.25)\",\n        \"font-size\" = \"12px\",\n        \"border-color\" = \"rgba(0,0,0,0.5)\"\n      )\n    )\n  )"
  },
  {
    "objectID": "posts2/Visualisation of My Personal Google Data (1)/index.html#conclusion",
    "href": "posts2/Visualisation of My Personal Google Data (1)/index.html#conclusion",
    "title": "Visualisation of My Personal Google Data (1): My Locations",
    "section": "Conclusion",
    "text": "Conclusion\nWorking with your personal data gives you the opportunity to understand your own habits, likes, dislikes, and maybe future expectations. Here, you can only see my locations in November. When I worked on longer periods, I realized that I need to travel and see new places more often. Even if they are in my own city, a new place is a new vision of life.\nVisualizing data on spatial environments is a new challenge for me. Rather than graphs and charts, working with maps are more attractive obviously. While visualizing location data on maps, leaflet is an amazing, open source library. There are other options. One needs a mention here: ggmap. Yet, to use this package you need an API key obtained from Google. For more information about API keys, visit here. As of the package, you can visit the CRAN page of ggmap. Under the title “Google Maps API key”, you will see the procedure to buy a personal API key. It reads as follows:\nGOOGLE MAPS API KEY [@ggmap]\nA few years ago Google has changed its API requirements, and ggmap users are now required to register with Google. From a user’s perspective, there are essentially three ramifications of this:\n\nUsers must register with Google. You can do this at https://mapsplatform.google.com. While it will require a valid credit card (sorry!), there seems to be a fair bit of free use before you incur charges, and even then the charges are modest for light use.\nUsers must enable the APIs they intend to use. What may appear to ggmap users as one overarching “Google Maps” product, Google in fact has several services that it provides as geo-related solutions. For example, the Maps Static API provides map images, while the Geocoding API provides geocoding and reverse geocoding services. Apart from the relevant Terms of Service, generally ggmap users don’t need to think about the different services. For example, you just need to remember that get_googlemap() gets maps, geocode() geocodes (with Google, DSK is done), etc., and ggmap handles the queries for you. However, you do need to enable the APIs before you use them. You’ll only need to do that once, and then they’ll be ready for you to use. Enabling the APIs just means clicking a few radio buttons on the Google Maps Platform web interface listed above, so it’s easy.\nInside R, after loading the new version of ggmap, you’ll need provide ggmap with your API key, a hash value (think string of jibberish) that authenticates you to Google’s servers. This can be done on a temporary basis with register_google(key = \"[your key]\") or permanently using register_google(key = \"[your key]\", write = TRUE) (note: this will overwrite your ~/.Renviron file by replacing/adding the relevant line). If you use the former, know that you’ll need to re-do it every time you reset R.\n\nYour API key is private and unique to you, so be careful not to share it online, for example in a GitHub issue or saving it in a shared R script file. If you share it inadvertantly, just get on Google’s website and regenerate your key - this will retire the old one. Keeping your key private is made a bit easier by ggmap scrubbing the key out of queries by default, so when URLs are shown in your console, they’ll look something like key=xxx. (Read the details section of the register_google() documentation for a bit more info on this point.)\n\nStay tuned!\nThis series continues with the visualization of my Google Fit data. We will delve into my exercise habbits."
  }
]