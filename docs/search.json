[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ali Emre Karagül",
    "section": "",
    "text": "Hey there!\nYou are visiting my personal blog, so here is a little bit of information about me:\nI am passionate about learning and exploring new ideas in the fields of Data Science, Psychometrics, Data Analytics, and Educational Assessment. I have a deep interest in language assessment and have conducted research in this field. My research focuses on understanding the impact of different assessment techniques on student outcomes. I am a driven individual who believes in hard work and dedication and I am always looking for new opportunities to make a positive impact. I am an avid reader and always looking to learn something new.\nCurrently, I work on my PhD in Educational Assessment and Evaluation. My current research interest is on Automated Essay Scoring, which has led me through machine learning and natural language processing."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Ali Emre Karagül",
    "section": "EDUCATION",
    "text": "EDUCATION\n\nGazi University\nFaculty of Education (2022-ongoing)\nPh.D. student in the Department of Educational Assessment and Evaluation\n\n\nAnkara University\nFaculty of Educational Sciences (2016-2020)\nMaster’s Degree in Assessment and Evaluation in Education\\\n\n\nGazi University\nFaculty of Education (2008-2014)\nBachelor’s degree in the Department of Foreign Language Education"
  },
  {
    "objectID": "index.html#contact-me",
    "href": "index.html#contact-me",
    "title": "Ali Emre Karagül",
    "section": "CONTACT ME",
    "text": "CONTACT ME\nAlways feel free to contact me about any feedback, questions, or maybe just to say “hi”."
  },
  {
    "objectID": "index1.html",
    "href": "index1.html",
    "title": "Psychometrics",
    "section": "",
    "text": "psychometrics\n\n\nSEM\n\n\n\n\nYEM: ‘Your View on Science’ ölçeğinden elde edilen 2015 PISA Turkiye örneklemi ile bir örnek çalışma. Bu çalışma Gazi üniversitesi’nde ‘Yapısal Eşitlik Modellemeleri’ dersi kapsamında rapor olarak hazırlanmıştır.\n\n\n\n\n\n\nOct 30, 2022\n\n\nAli Emre Karagül\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npsychometrics\n\n\nCTT\n\n\ntest equating\n\n\n\n\nEquating two test forms: This is a simple test equating study. The data used in this study is simulated from real data. We don’t use the real data for privacy purposes here.\n\n\n\n\n\n\nSep 20, 2022\n\n\nAli Emre Karagül\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npsychometrics\n\n\nIRT\n\n\n\n\nMadde Tepki Kuramı temelli madde analizi. Hem çok kategorili hem de iki kategorili maddeler için MTK temelli madde analizi uygulama süreçleri…\n\n\n\n\n\n\nNov 20, 2021\n\n\nAli Emre Karagül\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index2.html",
    "href": "index2.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Google Data\n\n\nData-viz\n\n\n\n\nThis post is a part of a series that demonstrates how to gain insights from personal Google data. Its purpose is to show how to visualize personal movement data.\n\n\n\n\n\n\nDec 11, 2022\n\n\nAli Emre Karagül\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nGoogle Data\n\n\nMaps\n\n\nData-viz\n\n\n\n\nThis post is a part of a series that demonstrates how to gain insights from personal Google data. Its purpose is to show how to visualize the locations I visited within a time period on a map.\n\n\n\n\n\n\nNov 16, 2022\n\n\nAli Emre Karagül\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Item Analysis with Item Response Theory (in Turkish)/index.html",
    "href": "posts/Item Analysis with Item Response Theory (in Turkish)/index.html",
    "title": "Item Analysis with Item Response Theory (in Turkish)",
    "section": "",
    "text": "Bu çalışmada çok kategorili puanlanan maddelerden elde edilen bir veri seti kullanılmıştır. Çalışmanın ilk kısmında çok kategorili maddelere yönelik MTK analizleri yürütülmüştür. Daha sonra aynı veri seti iki kategorili verilere dönüştürülmüştür. Yine MTK süreçleri bu sefer de iki kategorili maddeler için yürütülmüştür. İzlenen adımlar şu şekildedir:\n1. Çoklu puanlanan maddelere yönelik olarak;\n\nUygun MTK modeli nedir?\nBu modele göre madde ve test parametreleri nasıldır?\nİdeal ve sorunlu madde örnekleri nasıldır?\nTest bilgi fonksiyonunu nasıldır?\nBirey yetenek puanlarının dağılımı nasıldır?\n\n2. Her bir maddeyi, kendi madde ortalamasından keserek 1-0 verisine dönüştürünüz. Buna göre iki kategorili puanlanan maddelere yönelik olarak;\n\nUygun MTK modeli nedir?\nBu modele göre madde ve test parametreleri nasıldır?\nİdeal ve sorunlu madde örnekleri nasıldır?\nTest bilgi fonksiyonunu nasıldır?\nBirey yetenek puanlarının dağılımı nasıldır?\n\n\n\nKullanılan paketleri listeleyelim:\n\nlibrary(psych)\nlibrary(GPArotation)\nlibrary(sirt)\nlibrary(ltm)\n\nTabi ki işe öncelikle verinin working directory’den yüklenmesi ve ön düzenleme süreçleri ile başladık:\n\ndata1<- read.csv2(\"sampledata.csv\")\nstr(data1) #yapısını incelemek için.\n\n'data.frame':   531 obs. of  25 variables:\n $ SIRA    : int  1 2 3 4 5 6 7 8 9 10 ...\n $ Fakulte : chr  \"Diger\" \"Diger\" \"Diger\" \"Diger\" ...\n $ Sinif   : int  4 4 5 4 5 4 4 4 4 4 ...\n $ Cinsiyet: chr  \"Kad\\xfdn\" \"Kad\\xfdn\" \"Kad\\xfdn\" \"Kad\\xfdn\" ...\n $ Ortalama: chr  \"2.88\" \"2.93\" \"3.12\" \"3.28\" ...\n $ L1      : int  2 1 2 2 1 3 3 1 1 2 ...\n $ L2      : int  1 1 4 2 1 4 4 3 4 1 ...\n $ L3      : int  2 4 4 3 1 4 3 1 4 4 ...\n $ L4      : int  1 2 4 3 2 4 4 1 2 3 ...\n $ L5      : int  2 4 4 4 2 5 4 1 3 4 ...\n $ L6      : int  1 5 4 3 4 3 3 5 4 2 ...\n $ L7      : int  2 1 4 3 5 4 4 1 2 3 ...\n $ L8      : int  1 3 3 3 1 4 4 1 2 1 ...\n $ L9      : int  2 1 4 4 1 3 5 1 3 4 ...\n $ L10     : int  1 1 2 3 2 4 4 1 4 2 ...\n $ L11     : int  2 1 4 3 3 4 5 4 3 2 ...\n $ L12     : int  1 3 4 3 5 4 5 2 3 2 ...\n $ L13     : int  2 1 2 2 1 2 3 1 1 4 ...\n $ L14     : int  1 1 1 2 1 3 4 1 2 5 ...\n $ L15     : int  3 3 4 3 2 3 4 1 2 4 ...\n $ L16     : int  1 1 1 2 1 2 2 2 2 5 ...\n $ L17     : int  2 2 2 2 1 3 4 1 2 4 ...\n $ L18     : int  2 2 2 3 4 3 2 1 2 3 ...\n $ L19     : int  2 2 2 3 2 4 3 1 4 2 ...\n $ L20     : int  1 1 2 3 1 3 4 1 3 4 ...\n\n\nVeri setinde ilk sütunun sıra sayıları olduğunu görünce aman tanrım dedik ve sildik.\n\ndata1<-data1[,-1] # fazla kalabalığa gerek yok, aynı isimle devam.\n\nKayıp veri olup olmadığını anlamak için:\n\ndata1[data1 == 0] <- NA\nsum(is.na(data1))\n\n[1] 44\n\n\nNeredeyse %8 oranında missing value var. Too much! Alayını atıyoruz. Artık adını da değiştirelim.\n\ndata2<- na.omit(data1)\n\nMadem ki öylesine bir veri seti ile öylesine bir analiz yapıyoruz ve practical kaygılarımız yok, veri setimizi büyütelim. 1000 kişi olsun:\n\nset.seed(16611106)  # olur da sen de denemek istersen diye aynı veri setini üretmemizi sağlar.\ndata3 <- data2[sample(1:495, 1000, replace = T), ] #adını da değiştirelim \n\nSon olarak, veri setinde işimize yaramayacak bir sürü demografik detay var. Atıyoruz:\n\nrow.names(data3)<- NULL #önce adlarını silelim.\n\ndata4<- data3[,5:24]  #5. sütundan sonrası çöp  \n\nÇalışmanın birinci araştırma sorusu kapsamında “data4” adlı veri seti kullanılmıştır. Bu veri seti 1000 gözlemden ve 20 değişkenden oluşmaktadır. Değişkenler, 1 ile 5 arasında bir tam sayı değeri almaktadır. Çalışmanın ikinci araştırma sorusu kapsamında ise her bir madde kendi madde ortalamasından kesilerek iki kategorili veriye dönüştürülmüştür. Bu aşamalar ilgili başlık altında raporlaştırılmıştır. Ön düzenlemelerin ardından araştırma sorularına cevap aramak için ileri analizlere devam edilmiştir."
  },
  {
    "objectID": "posts/Item Analysis with Item Response Theory (in Turkish)/index.html#çok-boyutlu-maddelere-yönelik-aşamalar",
    "href": "posts/Item Analysis with Item Response Theory (in Turkish)/index.html#çok-boyutlu-maddelere-yönelik-aşamalar",
    "title": "Item Analysis with Item Response Theory (in Turkish)",
    "section": "1. Çok Boyutlu Maddelere Yönelik Aşamalar",
    "text": "1. Çok Boyutlu Maddelere Yönelik Aşamalar\nÇalışmanın ilk araştırma sorusu kapsamında, yapılan analizler çok kategorili puanlanan maddeler üzerinden yürütülmüştür.\n\n1.a. Varsayımların kontrolü ve uygun MTK modelinin belirlenmesi\nÇok kategorili puanlanan maddelerden oluşan ölçeğin Madde Tepki Kuramı çerçevesinde incelenmesi sürecinde öncelikle uygun MTK modelinin belirlenebilmesi için varsayım kontrolleri yapılmıştır. Bu bağlamda, tek boyutluluk ve yerel bağımsızlık varsayımları ile model-veri uyumu kontrol edilmiştir. Tek boyutluluk varsayımı kontrolü için paralel analiz, yamaç birikinti grafiği ve faktör analizi kullanılmıştır. Bu aşamada kullanılan paketler: Psych ve GPArotation.\nParalel Analizden elde edilen yamaç birikinti grafiği şöyledir:\n\nfa.parallel(data4, n.obs = 1000, cor = \"poly\")\n\n\n\n\nParallel analysis suggests that the number of factors =  5  and the number of components =  2 \n\n\nYamaç birikinti grafiği iki bileşenli bir yapıyı göstermektedir.  Son olarak faktör analizi yardımı ile hem tek hem de iki bileşenli modeller oluşturulmuştur:\n\nfa_model1 <- fa.poly(data4)\nfa_model2 <- fa(data4, nfactors = 2, cor=\"poly\")\n\nBu iki modelin burada çıktılarını alsak baya uzun oluyor. Ama özetle; ben beğendiğim ve devam analizi için seçtiğim 2 faktörlü model ile devam ediyorum. Bu modelin çıktıları incelendiğinde, 1-12 numaralı maddelerin bir boyutta, 13-20 numaralı maddelerin ise diğer bir boyutta yüklendiği görülmektedir. Bu nedenle veri seti aşağıdaki adımlar izlenerek ikiye bölünmüş ve ileri analizler her iki faktör için de ayrı ayrı yürütülmüştür.\n\ndata4a<-data4[1:12]\ndata4b<-data4[13:20]\n\nYerel bağımsızlık varsayımının kontrolü için Yen’in Q analizi (Yen, 1984) her iki bileşen için de uygulanmıştır. Bu süreçte sirt paketinden (Robitzsch, 2020) yararlanılmış ve aşağıdaki adımlar izlenmiştir.  \nMod1 <- TAM::tam.mml( resp=data4a )\nMod1.wle <- TAM::tam.wle(Mod1)\nMod1.q3 <- sirt::Q3( dat=data4a, theta=Mod1.wle$theta, b=Mod1$item_irt[[3]] )\nMod2 <- TAM::tam.mml( resp=data4b )\nMod2.wle <- TAM::tam.wle(Mod2)\nMod2.q3 <- sirt::Q3( dat=data4b, theta=Mod2.wle$theta, b=Mod2$item_irt[[3]] )\nBu üstteki kodun çıktısı çooook uzun. Buraya koymuyorum. Tabi biz veriyi bootstrap ile çoğalttığımız için bu varsayım karşılanmadı ama gerçek veri ile çalışsaydık bu varsayımın karşılanmaması durumunda çöp olacaktı analiz. Yani örneklem yetersiz, daha çok örneklem lazım diyecektik. Ya da modeli veya madde sayılarını inceleyecektik vs. vs.\nBirinci araştırma sorusunun son aşamasında ise model veri uyumunun incelenmesi ve en uygun modelin seçilmesi yer almaktadır. Model-veri uyumu incelemesi ltm paketi (Rizopoulos, 2006) yardımı ile her iki bileşen için de ayrı ayrı GRM modeli ile yürütülmüştür. Her iki bileşende de model-1, ayırt edicilik düzeylerinin her madde için farklılaştığı modeli betimlemektedir. Model-2 ise ayırt edicilik düzeylerinin her madde için eşit tutulduğu modeldir.\n\nmodel1_d4a<- grm(data4a)\nmodel2_d4a<- grm(data4a, constrained = TRUE)\nmodel1_d4b<- grm(data4b)\nmodel2_d4b<- grm(data4b, constrained = TRUE)\nanova(model2_d4a, model1_d4a)\n\n\n Likelihood Ratio Table\n                AIC      BIC   log.Lik    LRT df p.value\nmodel2_d4a 31785.45 32025.93 -15843.72                  \nmodel1_d4a 31660.51 31954.98 -15770.26 146.94 11  <0.001\n\nanova(model2_d4b, model1_d4b)\n\n\n Likelihood Ratio Table\n                AIC      BIC   log.Lik   LRT df p.value\nmodel2_d4b 21742.58 21904.54 -10838.29                 \nmodel1_d4b 21664.77 21861.08 -10792.38 91.81  7  <0.001\n\n\nModeller arasında manidar farklılık anlamına gelen p değerlerine (<.05) sahip olmasının yanı sıra, Akaike ve Bayesian bilgi kriter değerleri en düşük olan modellerin her iki bileşen için de model-1 olduğu görülmektedir. Bu nedenle model-1 ile daha iyi bir model-veri uyumu sağlanmaktadır. Devam analizleri her iki bileşen için de model-1 ile yürütülmüştür.\n\n\n1.b. Madde parametreleri\nModel-veri uyumu sınandıktan ve en uygun model belirlendikten sonra, madde parametrelerinin incelenmesi aşamasına geçilmiştir. Model-1 üzerinden aşağıdaki kod satırları kullanılarak elde edilen madde parametreleri görülebilir.\n\ncoef(model1_d4a)\n\n    Extrmt1 Extrmt2 Extrmt3 Extrmt4 Dscrmn\nL1   -1.259  -0.119   0.576   1.625  2.361\nL2   -0.994   0.224   1.002   2.265  1.383\nL3   -1.157  -0.363   0.254   1.498  2.401\nL4   -1.270  -0.276   0.610   1.718  2.355\nL5   -1.130  -0.312   0.337   1.382  2.248\nL6   -1.565  -0.657   0.322   1.309  1.895\nL7   -1.061  -0.257   0.460   1.453  2.129\nL8   -1.201  -0.401   0.314   1.367  2.635\nL9   -1.102  -0.322   0.363   1.246  2.296\nL10  -1.222  -0.295   0.476   1.473  1.815\nL11  -1.218  -0.457   0.345   1.320  2.123\nL12  -1.096  -0.249   0.737   1.819  1.661\n\ncoef(model1_d4b)\n\n    Extrmt1 Extrmt2 Extrmt3 Extrmt4 Dscrmn\nL13  -0.665   0.150   0.830   1.786  2.353\nL14  -0.791   0.148   0.917   1.802  2.290\nL15  -1.388  -0.736   0.198   0.920  2.308\nL16  -1.210   0.064   1.206   1.969  1.718\nL17  -0.998   0.150   1.077   1.832  1.898\nL18  -1.775  -0.720   0.598   1.664  1.338\nL19  -1.751  -0.514   0.694   1.600  1.505\nL20  -1.262  -0.351   0.505   1.277  2.007\n\n\nTablo 4 incelendiğinde, her madde için eşik parametrelerinin beklendik bir şekilde birinciden dördüncüye doğru arttığı görülmektedir. Bazı maddelerin birinci eşik parametresinin -1’den daha büyük bir değerle başladığı dikkat çekmektedir. Örneğin ikinci bileşene ait ilk madde olan 13. Maddenin ilk kategorisini seçen bir bireyin yetenek seviyesi %50 ihtimalle -.66’dan daha düşüktür. \n\n\n1.c. Örnek Madde Karakteristik Eğrileri\nÇalışmanın bu aşamasında madde karakteristik eğrisi ideal ve sorunlu olan birer madde incelenmiş ve yorumlanmıştır. Bu nedenle, öncelikle birinci bileşeni oluşturan tüm maddelerin madde karakteristik eğrileri plot() fonksiyonu ile oluşturulmuştur. İdeal bir madde karakteristik eğrisine sahip olduğu düşünülen sekizinci madde ve kısmi sorunlu olduğu düşünülen ikinci maddenin madde karakteristik eğrileri aşağıdaki kod satırları yürütülerek oluşturulmuştur.\n\nplot(model1_d4a, type=\"ICC\",item=6, xlab= \"YETENEK\", cex.main = 1, main = \"MADDE KARAKTERİSTİK EĞRİSİ- Madde: 6\", ylab = \"OLASILIK\" , lwd= 2, col.main= \"red\", font.axis= 3, font.lab=2)\n\n\n\nplot(model1_d4a, type=\"ICC\",xlab= \"YETENEK\", cex.main = 1, ylab = \"OLASILIK\" , lwd= 2, col.main= \"red\", font.axis= 3, font.lab=2, main = \"MADDE KARAKTERİSTİK EĞRİSİ- Madde: 2\", items = 2)\n\n\n\n\nGörülen ilk grafik diğer maddelere göre daha ideal dağılım gösteren bir maddeye aittir. Bu maddenin eğrileri tüm yetenek düzeylerini kapsayacak şekilde sivrilip dağılmaktadır. Örneğin, sıfır yetenek düzeyinde bir bireyin üçüncü kategoride yer alma olasılığı en yüksek düzeydedir. Benzer şekillerde diğer kategorilerin de yüksek olasılık ile temsil ettikleri yetenek düzeyleri belirgin bir şekilde görülmektedir. Bu durumda bu maddenin ayırt edicilik düzeyinin yüksek olması beklenir. Madde parametreleri çıktısında da görüleceği üzere, bu maddeye ait ayırt edicilik parametresi birinci bileşenin en yüksek ayırt edicilik düzeyidir.\nİkinci grafik ise kısmen problemli olduğu düşünülen bir grafiktir. Bu grafiğin daha iyi yorumlanabilmesi için, eksenlerinde yer alan değerler axis() fonksiyonu ile daha detaylı hale getirilmiştir. Ayrıca abline() fonksiyonu ile 0.6 yetenek düzeyine dikey bir çizgi eklenmiştir.\n\nplot(model1_d4a, type=\"ICC\",\n     xlab= \"YETENEK\", \n     cex.main = 1, \n     ylab = \"OLASILIK\" , \n     lwd= 2, col.main= \"red\", \n     font.axis= 3, \n     font.lab=2, \n     main = \"MADDE KARAKTERİSTİK EĞRİSİ- Madde: 2\", \n     items = 2)\naxis(2, at = seq(0, 1, by = .1))\naxis(1, at = seq(-4, 4, by = .1))\nabline(v=.6)\n\n\n\n\nMaddenin eğrileri incelendiğinde, 0.6 yetenek düzeyinde bir bireyin ikinci, üçüncü ve dördüncü kategorileri seçme olasılıklarının birbirlerine çok yakın olduğu görülmektedir. Bu durum, maddenin ayırt ediciliğini olumsuz olarak etkilemektedir. Daha önceki çıktılardan bilindiği üzere bu maddeye ait ayırt edicilik parametresi birinci bileşenin en düşük ayırt edicilik düzeyidir. Ayrıca, bu maddenin üçüncü kategorisinin sivrilmediği de dikkat çekmektedir. Bu durumda bu maddenin üçüncü kategorisinin çıkarılarak dört kategorili bir maddeye dönüştürülmesi düşünülebilir. Bir başka seçenek ise bu maddenin ölçekten çıkarılması ve analizlerin yeniden yapılması olabilir. \n\n\n1.d. Test Bilgi Fonksiyonu\nHer iki bileşen için de test bilgi fonksiyonları aşağıdaki kod satırları kullanılarak elde edilmiştir:\n\nplot(model1_d4a, type=\"IIC\", items = 0, xlab= \"YETENEK\", cex.main = 1, main = \"TEST   BİLGİ   FONKSİYONU\", ylab = \"BİLGİ\" , lwd= 2, col.main= \"red\", col=\"blue\", font.axis= 3, font.lab=2)\n\n\n\nplot(model1_d4b, type=\"IIC\", items = 0, xlab= \"YETENEK\", cex.main = 1, main = \"TEST   BİLGİ   FONKSİYONU\", ylab = \"BİLGİ\" , lwd= 2, col.main= \"red\", col=\"blue\", font.axis= 3, font.lab=2)\n\n\n\n\nTest bilgi fonksiyonlarının 0 yetenek düzeyinde en yüksek seviyede olduğu, -/+ 2 yetenek düzeylerinde sert bir şekilde düşmeye başladığı ve -/+ 4 yetenek düzeylerinde en düşük seviyesinde olduğu görülmektedir. Bu durumda her iki bileşenin de en çok 0 yetenek düzeyinde bilgi sağladığı ve -2 ile +2 aralığında yüksek düzeyde bilgi sağladığı söylenebilir. Ancak bu aralığın ötesinde sağlanan bilginin hızla azaldığı düşünülebilir. \nÇalışmanın bu aşamasında madde bilgi eğrileri de aşağıdaki kod satırları kullanılarak elde edilmiştir:\n\nplot(model1_d4a, type=\"IIC\",xlab= \"YETENEK\", cex.main = 1, ylab = \"BİLGİ\" , col.main= \"red\", font.axis= 3, font.lab=2, main = \"1. BİLEŞEN MADDE BİLGİ  EĞRİLERİ\")\n\n\n\nplot(model1_d4b, type=\"IIC\",xlab= \"YETENEK\", cex.main = 1, ylab = \"BİLGİ\" , col.main= \"red\", font.axis= 3, font.lab=2, main = \"2. BİLEŞEN MADDE BİLGİ  EĞRİLERİ\")\n\n\n\n\nBunlar incelendiğinde ise her iki bileşen için de maddelerin ölçülen özelliği geniş bir yetenek puanı ölçeğinde ölçtüğü görülmektedir. Birinci bileşen içerisinde en düşük bilgi sağlayan maddenin ikinci madde olduğu dikkat çekmektedir. Yine bilgi eğrisi en yüksek maddenin birinci bileşen için sekizinci madde olduğu da görülmektedir.  Bunlar, bir önceki bölümde kısmi problemli ve ideal dağılımlı olarak incelenen maddelerdir. Eğer ikinci bileşen için birer madde seçilecek olsaydı, o bileşene ait altı ve üç numaralı maddeler sırasıyla kısmi problemli ve ideal maddelere örnek olarak seçilebilirdi. \n\n\n1.e. Yetenek Puanları\nÇok kategorili puanlanan maddelere yönelik olarak yürütülen analizlerin son aşamasında bireylere ait yetenek puanları hesaplanmış ve bunların dağılımı incelenmiştir. Analizlerin bu aşamasında şu kod satırları kullanılarak birinci ve ikinci bileşen için yetenek puanları ayrı ayrı hesaplanmıştır. \n\n\nCode\n#BİLEŞEN-1\nscore_d4a<- factor.scores(model1_d4a)\noruntu_d4a<- score_d4a[[1]]\noruntu_d4a$toplam<- rowSums((oruntu_d4a[,1:12]))\nscore_d4a<- factor.scores(model1_d4a)\noruntu_d4a<- score_d4a[[1]]\noruntu_d4a$toplam<- rowSums((oruntu_d4a[,1:12]))\ntheta_d4a <- numeric()\nfor (i in 1:419){ \n for (j in 1:1000){\n         if (sum (oruntu_d4a[i, 1: 12] == data4a[j, 1:12])==12)\n              theta_d4a[j] <- oruntu_d4a[i, 15]      }}\ndata4a$theta<-theta_d4a\ndata4a$toplam<-rowSums(data4a[1:12])\n\n\n\n#BİLEŞEN-2\nscore_d4b<- factor.scores(model1_d4b)\noruntu_d4b<- score_d4b[[1]]\noruntu_d4b$toplam<- rowSums((oruntu_d4b[,1:12]))\ntheta_d4b <- numeric()\nfor (i in 1:404){\n  for (j in 1:1000){\n      if (sum (oruntu_d4b[i, 1: 8] == data4b[j, 1:8])==8)\n             theta_d4b[j] <- oruntu_d4b[i, 11] }}\ndata4b$theta<-theta_d4b\ndata4b$toplam<-rowSums(data4b[1:8])\n\n\nBunun ardından describe() fonksiyonu ile birey yetenek puanlarının betimsel istatikleri elde edilmiştir. Ayrıca hist() fonksiyonu ile histogram grafikleri oluşturulmuştur. Şu kod satırları kullanılmıştır. \n\n\nCode\n#BİLEŞEN-1\ndescribe(data4a$theta)\n\n\n   vars    n mean   sd median trimmed  mad  min  max range skew kurtosis   se\nX1    1 1000 0.05 0.93   0.02    0.05 0.92 -2.3 2.59  4.88 0.03     -0.2 0.03\n\n\nCode\nhist(data4a$theta, xlab= \"YETENEK\", cex.main = 1, ylab = \"FREKANS\" , col.main= \"red\", font.axis= 3, font.lab=2, main = \"1. BİLEŞEN BİREY YETENEK PUANLARI\")\n\n\n\n\n\nCode\n#BİLEŞEN-2\ndescribe(data4b$theta)\n\n\n   vars    n  mean   sd median trimmed  mad   min max range  skew kurtosis   se\nX1    1 1000 -0.01 0.92   0.05       0 0.89 -2.19 2.5  4.69 -0.04    -0.14 0.03\n\n\nCode\nhist(data4b$theta, xlab= \"YETENEK\", cex.main = 1, ylab = \"FREKANS\" , col.main= \"red\", font.axis= 3, font.lab=2, main = \"2. BİLEŞEN BİREY YETENEK PUANLARI\")\n\n\n\n\n\nHer iki bileşene ait betimsel istatikler ve histogram grafikleri incelendiğinde, ortalamalarının sıfıra, standart sapmalarının da bire çok yakın olduğu görülmektedir. Bu durum her iki bileşenden elde edilen verinin de normal dağılım sergilediğini göstermektedir."
  },
  {
    "objectID": "posts/Item Analysis with Item Response Theory (in Turkish)/index.html#iki-kategorili-maddelere-yönelik-aşamalar",
    "href": "posts/Item Analysis with Item Response Theory (in Turkish)/index.html#iki-kategorili-maddelere-yönelik-aşamalar",
    "title": "Item Analysis with Item Response Theory (in Turkish)",
    "section": "2. İki Kategorili Maddelere Yönelik Aşamalar",
    "text": "2. İki Kategorili Maddelere Yönelik Aşamalar\nÇalışmanın ikinci araştırma sorusu kapsamında, çok kategorili maddelerden oluşan veri setinin iki kategoriye dönüştürülmesi gerekmektedir. Çok kategorili puanlanan maddelerin iki kategorili olarak kodlanmasında izlenen adımlar şunlardır:\n\ndata5<- data4\nfor(i in 1:1000) {\n   for(j in 1:20) {\nif(data5[i,j] <= mean(data5[,j])) {data5[i,j] <- 0} \nelse data5[i,j] <- 1\n }\n}\n\n\n2.a. Varsayımların Kontrolü ve Uygun MTK Modeli\nİki kategorili puanlanan maddelerden oluşan ölçeğin Madde Tepki Kuramı çerçevesinde incelenmesi sürecinde öncelikle uygun MTK modelinin belirlenebilmesi için varsayım kontrolleri yapılmıştır. Bu bağlamda, tek boyutluluk ve yerel bağımsızlık varsayımları ile model-veri uyumu kontrol edilmiştir. Tek boyutluluk varsayımı kontrolü için paralel analiz, yamaç birikinti grafiği ve faktör analizi kullanılmıştır. Bu amaçla, Psych paketinden (Revelle, 2020) faydalanılmıştır.\n\nfa.parallel(data5, main = \"PARALEL ANALİZ SAÇILIM GRAFİĞİ\", ylabel = \"Temel Bileşenler ve Faktör Analizi Özdeğerleri\")\n\n\n\n\nParallel analysis suggests that the number of factors =  4  and the number of components =  2 \n\n\nParalel analiz ve yamaç birikinti grafiği iki bileşenli bir yapıyı göstermektedir.  Bu nedenle hem iki hem de tek bileşenli modeller oluşturulmuştur. Aşağıda izlenen adımlar sonrası elde edilen modeller ait faktör yükleri görülebilir.\n\nmodel1 <- fa(data5, cor = \"tet\")\nmodel2 <- fa(data5, cor = \"tet\", nfactors = 2)\n\nFaktör yüklerinin her madde için her iki modelde de .60’nın üzerinde olduğu görülmektedir. Tek bileşenli yapının açıkladığı varyansın ise %61 olduğu anlaşılmaktadır. Bunun yanı sıra, iki bileşenli yapının birinci bileşenin %40, ikinci bileşenin %28 olmak üzere toplamda %68 oranında açıklanan varyansa sahip olduğu görülmektedir. Açıklanan varyans açısından modeller arasındaki farkın çok büyük olmadığı düşünüldüğünden, ileri analizlere tek bileşenli model ile devam edilmesine karar verilmiştir.  \nYerel bağımsızlık varsayımının kontrolü için Yen’in Q analizi (Yen, 1984)  uygulanmıştır. Bu süreçte sirt paketinden (Robitzsch, 2020) yararlanılmış ve aşağıdaki adımlar izlenmiştir. \n\n\n------------------------------------------------------------\nSemiparametric Marginal Maximum Likelihood Estimation \nRaschtype Model with generalized logistic link function: alpha1= 0 , alpha2= 0  \n------------------------------------------------------------\n...........................................................\nIteration 1     2022-12-11 18:10:28 \n   Deviance=16821.3972\n    Maximum b parameter change=0.177231 \n...........................................................\nIteration 2     2022-12-11 18:10:28 \n   Deviance=16266.0078 | Deviance change=555.389418\n    Maximum b parameter change=0.102223 \n...........................................................\nIteration 3     2022-12-11 18:10:28 \n   Deviance=16073.974 | Deviance change=192.033807\n    Maximum b parameter change=0.075399 \n...........................................................\nIteration 4     2022-12-11 18:10:28 \n   Deviance=16008.6979 | Deviance change=65.27609\n    Maximum b parameter change=0.059514 \n...........................................................\nIteration 5     2022-12-11 18:10:28 \n   Deviance=15982.8096 | Deviance change=25.888241\n    Maximum b parameter change=0.050211 \n...........................................................\nIteration 6     2022-12-11 18:10:28 \n   Deviance=15969.2378 | Deviance change=13.571861\n    Maximum b parameter change=0.04423 \n...........................................................\nIteration 7     2022-12-11 18:10:28 \n   Deviance=15960.115 | Deviance change=9.122827\n    Maximum b parameter change=0.039895 \n...........................................................\nIteration 8     2022-12-11 18:10:28 \n   Deviance=15953.0997 | Deviance change=7.015221\n    Maximum b parameter change=0.036411 \n...........................................................\nIteration 9     2022-12-11 18:10:28 \n   Deviance=15947.3857 | Deviance change=5.71405\n    Maximum b parameter change=0.033414 \n...........................................................\nIteration 10     2022-12-11 18:10:28 \n   Deviance=15942.6175 | Deviance change=4.768184\n    Maximum b parameter change=0.030741 \n...........................................................\nIteration 11     2022-12-11 18:10:28 \n   Deviance=15938.5938 | Deviance change=4.023702\n    Maximum b parameter change=0.028313 \n...........................................................\nIteration 12     2022-12-11 18:10:28 \n   Deviance=15935.1781 | Deviance change=3.41568\n    Maximum b parameter change=0.026094 \n...........................................................\nIteration 13     2022-12-11 18:10:28 \n   Deviance=15932.268 | Deviance change=2.910084\n    Maximum b parameter change=0.024057 \n...........................................................\nIteration 14     2022-12-11 18:10:28 \n   Deviance=15929.7824 | Deviance change=2.485665\n    Maximum b parameter change=0.022187 \n...........................................................\nIteration 15     2022-12-11 18:10:28 \n   Deviance=15927.6549 | Deviance change=2.127468\n    Maximum b parameter change=0.02047 \n...........................................................\nIteration 16     2022-12-11 18:10:28 \n   Deviance=15925.8308 | Deviance change=1.82414\n    Maximum b parameter change=0.018892 \n...........................................................\nIteration 17     2022-12-11 18:10:28 \n   Deviance=15924.2641 | Deviance change=1.566669\n    Maximum b parameter change=0.017442 \n...........................................................\nIteration 18     2022-12-11 18:10:28 \n   Deviance=15922.9164 | Deviance change=1.347715\n    Maximum b parameter change=0.01611 \n...........................................................\nIteration 19     2022-12-11 18:10:28 \n   Deviance=15921.7552 | Deviance change=1.161213\n    Maximum b parameter change=0.014885 \n...........................................................\nIteration 20     2022-12-11 18:10:28 \n   Deviance=15920.753 | Deviance change=1.002112\n    Maximum b parameter change=0.013759 \n...........................................................\nIteration 21     2022-12-11 18:10:28 \n   Deviance=15919.8869 | Deviance change=0.866184\n    Maximum b parameter change=0.012723 \n...........................................................\nIteration 22     2022-12-11 18:10:28 \n   Deviance=15919.137 | Deviance change=0.749885\n    Maximum b parameter change=0.011769 \n...........................................................\nIteration 23     2022-12-11 18:10:28 \n   Deviance=15918.4867 | Deviance change=0.650234\n    Maximum b parameter change=0.010891 \n...........................................................\nIteration 24     2022-12-11 18:10:28 \n   Deviance=15917.922 | Deviance change=0.564722\n    Maximum b parameter change=0.010082 \n...........................................................\nIteration 25     2022-12-11 18:10:28 \n   Deviance=15917.4308 | Deviance change=0.491235\n    Maximum b parameter change=0.009335 \n...........................................................\nIteration 26     2022-12-11 18:10:28 \n   Deviance=15917.0028 | Deviance change=0.427989\n    Maximum b parameter change=0.008647 \n...........................................................\nIteration 27     2022-12-11 18:10:28 \n   Deviance=15916.6293 | Deviance change=0.373478\n    Maximum b parameter change=0.008012 \n...........................................................\nIteration 28     2022-12-11 18:10:28 \n   Deviance=15916.3029 | Deviance change=0.326427\n    Maximum b parameter change=0.007426 \n...........................................................\nIteration 29     2022-12-11 18:10:28 \n   Deviance=15916.0171 | Deviance change=0.285755\n    Maximum b parameter change=0.006884 \n...........................................................\nIteration 30     2022-12-11 18:10:28 \n   Deviance=15915.7666 | Deviance change=0.250548\n    Maximum b parameter change=0.006384 \n...........................................................\nIteration 31     2022-12-11 18:10:28 \n   Deviance=15915.5466 | Deviance change=0.220026\n    Maximum b parameter change=0.005921 \n...........................................................\nIteration 32     2022-12-11 18:10:28 \n   Deviance=15915.353 | Deviance change=0.193528\n    Maximum b parameter change=0.005493 \n...........................................................\nIteration 33     2022-12-11 18:10:28 \n   Deviance=15915.1825 | Deviance change=0.170491\n    Maximum b parameter change=0.005097 \n...........................................................\nIteration 34     2022-12-11 18:10:28 \n   Deviance=15915.0321 | Deviance change=0.150434\n    Maximum b parameter change=0.004731 \n...........................................................\nIteration 35     2022-12-11 18:10:28 \n   Deviance=15914.8992 | Deviance change=0.132946\n    Maximum b parameter change=0.004392 \n...........................................................\nIteration 36     2022-12-11 18:10:28 \n   Deviance=15914.7815 | Deviance change=0.117677\n    Maximum b parameter change=0.004077 \n...........................................................\nIteration 37     2022-12-11 18:10:28 \n   Deviance=15914.6772 | Deviance change=0.104326\n    Maximum b parameter change=0.003786 \n...........................................................\nIteration 38     2022-12-11 18:10:28 \n   Deviance=15914.5845 | Deviance change=0.092635\n    Maximum b parameter change=0.003516 \n...........................................................\nIteration 39     2022-12-11 18:10:28 \n   Deviance=15914.5021 | Deviance change=0.082383\n    Maximum b parameter change=0.003266 \n...........................................................\nIteration 40     2022-12-11 18:10:28 \n   Deviance=15914.4288 | Deviance change=0.07338\n    Maximum b parameter change=0.003034 \n...........................................................\nIteration 41     2022-12-11 18:10:28 \n   Deviance=15914.3633 | Deviance change=0.065462\n    Maximum b parameter change=0.002819 \n...........................................................\nIteration 42     2022-12-11 18:10:28 \n   Deviance=15914.3048 | Deviance change=0.058488\n    Maximum b parameter change=0.002619 \n...........................................................\nIteration 43     2022-12-11 18:10:28 \n   Deviance=15914.2525 | Deviance change=0.052337\n    Maximum b parameter change=0.002434 \n...........................................................\nIteration 44     2022-12-11 18:10:28 \n   Deviance=15914.2056 | Deviance change=0.046904\n    Maximum b parameter change=0.002262 \n...........................................................\nIteration 45     2022-12-11 18:10:28 \n   Deviance=15914.1635 | Deviance change=0.042097\n    Maximum b parameter change=0.002102 \n...........................................................\nIteration 46     2022-12-11 18:10:28 \n   Deviance=15914.1256 | Deviance change=0.037839\n    Maximum b parameter change=0.001954 \n...........................................................\nIteration 47     2022-12-11 18:10:28 \n   Deviance=15914.0916 | Deviance change=0.034061\n    Maximum b parameter change=0.001817 \n...........................................................\nIteration 48     2022-12-11 18:10:28 \n   Deviance=15914.0609 | Deviance change=0.030704\n    Maximum b parameter change=0.001689 \n...........................................................\nIteration 49     2022-12-11 18:10:28 \n   Deviance=15914.0332 | Deviance change=0.027716\n    Maximum b parameter change=0.00157 \n...........................................................\nIteration 50     2022-12-11 18:10:28 \n   Deviance=15914.0081 | Deviance change=0.025053\n    Maximum b parameter change=0.00146 \n...........................................................\nIteration 51     2022-12-11 18:10:28 \n   Deviance=15913.9854 | Deviance change=0.022677\n    Maximum b parameter change=0.001358 \n...........................................................\nIteration 52     2022-12-11 18:10:28 \n   Deviance=15913.9649 | Deviance change=0.020552\n    Maximum b parameter change=0.001263 \n...........................................................\nIteration 53     2022-12-11 18:10:28 \n   Deviance=15913.9462 | Deviance change=0.018651\n    Maximum b parameter change=0.001174 \n...........................................................\nIteration 54     2022-12-11 18:10:28 \n   Deviance=15913.9293 | Deviance change=0.016946\n    Maximum b parameter change=0.001092 \n...........................................................\nIteration 55     2022-12-11 18:10:28 \n   Deviance=15913.9139 | Deviance change=0.015415\n    Maximum b parameter change=0.001016 \n...........................................................\nIteration 56     2022-12-11 18:10:28 \n   Deviance=15913.8998 | Deviance change=0.014039\n    Maximum b parameter change=0.000945 \n...........................................................\nIteration 57     2022-12-11 18:10:28 \n   Deviance=15913.887 | Deviance change=0.0128\n    Maximum b parameter change=0.000879 \n...........................................................\nIteration 58     2022-12-11 18:10:28 \n   Deviance=15913.8753 | Deviance change=0.011683\n    Maximum b parameter change=0.000817 \n...........................................................\nIteration 59     2022-12-11 18:10:28 \n   Deviance=15913.8647 | Deviance change=0.010675\n    Maximum b parameter change=0.00076 \n...........................................................\nIteration 60     2022-12-11 18:10:28 \n   Deviance=15913.8549 | Deviance change=0.009763\n    Maximum b parameter change=0.000707 \n...........................................................\nIteration 61     2022-12-11 18:10:28 \n   Deviance=15913.846 | Deviance change=0.008938\n    Maximum b parameter change=0.000658 \n...........................................................\nIteration 62     2022-12-11 18:10:28 \n   Deviance=15913.8378 | Deviance change=0.00819\n    Maximum b parameter change=0.000612 \n...........................................................\nIteration 63     2022-12-11 18:10:28 \n   Deviance=15913.8303 | Deviance change=0.007511\n    Maximum b parameter change=0.000569 \n...........................................................\nIteration 64     2022-12-11 18:10:28 \n   Deviance=15913.8234 | Deviance change=0.006894\n    Maximum b parameter change=0.00053 \n...........................................................\nIteration 65     2022-12-11 18:10:28 \n   Deviance=15913.817 | Deviance change=0.006333\n    Maximum b parameter change=0.000493 \n...........................................................\nIteration 66     2022-12-11 18:10:28 \n   Deviance=15913.8112 | Deviance change=0.005822\n    Maximum b parameter change=0.000458 \n...........................................................\nIteration 67     2022-12-11 18:10:28 \n   Deviance=15913.8059 | Deviance change=0.005357\n    Maximum b parameter change=0.000426 \n...........................................................\nIteration 68     2022-12-11 18:10:28 \n   Deviance=15913.8009 | Deviance change=0.004931\n    Maximum b parameter change=0.000397 \n...........................................................\nIteration 69     2022-12-11 18:10:28 \n   Deviance=15913.7964 | Deviance change=0.004543\n    Maximum b parameter change=0.000369 \n...........................................................\nIteration 70     2022-12-11 18:10:28 \n   Deviance=15913.7922 | Deviance change=0.004188\n    Maximum b parameter change=0.000343 \n...........................................................\nIteration 71     2022-12-11 18:10:28 \n   Deviance=15913.7883 | Deviance change=0.003862\n    Maximum b parameter change=0.00032 \n...........................................................\nIteration 72     2022-12-11 18:10:28 \n   Deviance=15913.7848 | Deviance change=0.003564\n    Maximum b parameter change=0.000297 \n...........................................................\nIteration 73     2022-12-11 18:10:28 \n   Deviance=15913.7815 | Deviance change=0.003291\n    Maximum b parameter change=0.000277 \n...........................................................\nIteration 74     2022-12-11 18:10:28 \n   Deviance=15913.7784 | Deviance change=0.00304\n    Maximum b parameter change=0.000257 \n...........................................................\nIteration 75     2022-12-11 18:10:28 \n   Deviance=15913.7756 | Deviance change=0.00281\n    Maximum b parameter change=0.00024 \n...........................................................\nIteration 76     2022-12-11 18:10:28 \n   Deviance=15913.773 | Deviance change=0.002598\n    Maximum b parameter change=0.000223 \n...........................................................\nIteration 77     2022-12-11 18:10:28 \n   Deviance=15913.7706 | Deviance change=0.002403\n    Maximum b parameter change=0.000207 \n...........................................................\nIteration 78     2022-12-11 18:10:28 \n   Deviance=15913.7684 | Deviance change=0.002224\n    Maximum b parameter change=0.000193 \n...........................................................\nIteration 79     2022-12-11 18:10:28 \n   Deviance=15913.7663 | Deviance change=0.002059\n    Maximum b parameter change=0.00018 \n...........................................................\nIteration 80     2022-12-11 18:10:28 \n   Deviance=15913.7644 | Deviance change=0.001906\n    Maximum b parameter change=0.000167 \n...........................................................\nIteration 81     2022-12-11 18:10:28 \n   Deviance=15913.7627 | Deviance change=0.001766\n    Maximum b parameter change=0.000155 \n...........................................................\nIteration 82     2022-12-11 18:10:28 \n   Deviance=15913.761 | Deviance change=0.001636\n    Maximum b parameter change=0.000145 \n...........................................................\nIteration 83     2022-12-11 18:10:28 \n   Deviance=15913.7595 | Deviance change=0.001517\n    Maximum b parameter change=0.000135 \n...........................................................\nIteration 84     2022-12-11 18:10:28 \n   Deviance=15913.7581 | Deviance change=0.001406\n    Maximum b parameter change=0.000125 \n...........................................................\nIteration 85     2022-12-11 18:10:28 \n   Deviance=15913.7568 | Deviance change=0.001304\n    Maximum b parameter change=0.000117 \n...........................................................\nIteration 86     2022-12-11 18:10:28 \n   Deviance=15913.7556 | Deviance change=0.001209\n    Maximum b parameter change=0.000108 \n...........................................................\nIteration 87     2022-12-11 18:10:28 \n   Deviance=15913.7545 | Deviance change=0.001122\n    Maximum b parameter change=0.000101 \n...........................................................\nIteration 88     2022-12-11 18:10:28 \n   Deviance=15913.7534 | Deviance change=0.001041\n    Maximum b parameter change=9.4e-05 \n------------------------------------------------------------\nStart: 2022-12-11 18:10:28 \nEnd: 2022-12-11 18:10:28 \nTime difference of 0.4380932 secs\nDifference: 0.4380932 \n------------------------------------------------------------\n\n\n\nWLE Reliability= 0.711 \n\n\nYen's Q3 Statistic based on an estimated theta score \n*** 20 Items | 190 item pairs\n*** Q3 Descriptives\n     M     SD    Min    10%    25%    50%    75%    90%    Max \n-0.030  0.110 -0.236 -0.167 -0.121 -0.039  0.049  0.120  0.325 \n\n\nYerel bağımsızlık varsayımının karşılandığı görülmektedir. Bu nedenle model-veri uyumu incleme aşamasına geçilmiştir. Bu aşamada ltm paketinden (Rizopoulos, 2006) faydalanılmıştır. Aşağıdaki kod satırları kullanılarak elde edilen Benzerlik oranı tabloları görülebilir.\n\nmodel1 <- rasch(data5)\nmodel2 <- ltm(data5 ~ z1)  \nmodel3 <- tpm(data5)\nanova(model1, model2)\n\n\n Likelihood Ratio Table\n            AIC      BIC  log.Lik    LRT df p.value\nmodel1 15975.79 16078.85 -7966.89                  \nmodel2 15913.21 16109.52 -7916.61 100.58 19  <0.001\n\nanova(model1, model3)\n\n\n Likelihood Ratio Table\n            AIC      BIC  log.Lik    LRT df p.value\nmodel1 15975.79 16078.85 -7966.89                  \nmodel3 15914.14 16208.61 -7897.07 139.64 39  <0.001\n\nanova(model2, model3)\n\n\n Likelihood Ratio Table\n            AIC      BIC  log.Lik   LRT df p.value\nmodel2 15913.21 16109.52 -7916.61                 \nmodel3 15914.14 16208.61 -7897.07 39.07 20   0.007\n\n\nAkaike ve Bayesian bilgi kriterleri incelendiğinde, üç model arasında ikinci modelin manidar farkla en uygun model olduğu görülmektedir. Bu nedenle, devam analizleri iki parametreli model ile yürütülmüştür.\n\n\n2.b. Madde Parametreleri\nModel-veri uyumu sınandıktan ve en uygun model belirlendikten sonra, madde parametrelerinin incelenmesi aşamasına geçilmiştir. Model-2 üzerinden coef() fonksiyonu kullanılarak elde edilen madde parametreleri aşağıdadır.\n\ncoef(model2)\n\n        Dffclt   Dscrmn\nL1  -0.8112296 2.832181\nL2  -0.7312085 1.933827\nL3  -0.8181936 2.659799\nL4  -0.8109842 3.100650\nL5  -0.8380050 2.394060\nL6  -1.0180088 2.070947\nL7  -0.8694781 2.103077\nL8  -0.8977956 2.683691\nL9  -0.7951705 2.798913\nL10 -0.8580558 2.207506\nL11 -0.8789145 2.322482\nL12 -0.8528908 1.705619\nL13 -0.6390827 2.246105\nL14 -0.6903395 2.247931\nL15 -0.8834606 2.517332\nL16 -0.7716486 2.330535\nL17 -0.7333232 2.334138\nL18 -1.1852158 1.497807\nL19 -1.0471877 1.896025\nL20 -0.8162163 2.864486\n\n\nÇıktılar incelendiğinde, güçlük parametresi en yüksek olan maddenin 13. madde olduğu göze çarpmaktadır. Yetenek düzeyi -.63’ten daha yüksek olan bireyler bu maddeyi %50’den daha yüksek bir ihtimalle doğru cevaplayacaklardır.\n\n\n2.c.  Örnek Madde Karakteristik Eğrileri\nÇalışmanın bu aşamasında, iki kategorili puanlanan maddelerin analizinde madde karakteristik eğrisi ideal ve sorunlu olan birer madde incelenmiş ve yorumlanmıştır. Bu nedenle, öncelikle birinci bileşeni oluşturan tüm maddelerin madde bilgi eğrileri plot() fonksiyonu ile oluşturulmuş, aşağıdaki kod satırları kullanılmıştır:\n\nplot(model2, type=\"IIC\",xlab= \"YETENEK\", cex.main = 1, ylab = \"BİLGİ\" , col.main= \"red\", font.axis= 3, font.lab=2, main = \"MADDE BİLGİ  EĞRİLERİ\")\n\n\n\n\nMadde bilgi eğrileri incelendiğinde, en çok bilgiyi dördüncü maddenin sağladığı görülmektedir. En düşük bilgi sağlayan maddelerden birisinin ise 12. madde olduğu görülmektedir. Bu nedenle madde karakteristik eğrilerinin incelenmesi sürecinde örnek olarak bu iki madde tercih edilmiştir. Bu maddelere ait karakteristik eğrileri aşağıdaki kod satırları ile elde edilmiştir.\n\nplot(model2, type=\"ICC\", items = c(4,12), labels = c(\"madde-4\", \"madde-12\"), legend = T, xlab= \"YETENEK\", cex.main = 1, main = \"MADDE KARAKTERİSTİK EĞRİSİ\", ylab = \"OLASILIK\" , lwd= 2)\npoints(-.81, .5, lwd= 3, pch= 3)\ntext(-.81, .5, lwd= 2, labels = \"b par.: -0.81\", pos = 4)\npoints(-.85, .5, lwd= 3, col= \"red\", pch=5)\ntext(-.85, .5, lwd= 3, labels = \"b par.: -0.85\", pos = 2, col= \"red\")\n\n\n\n\nDört ve 12 numaralı maddelerin güçlük parametreleri birbirlerine oldukça yakındır. Ancak dördüncü madde diğerine göre daha dik bir karakteristik eğrisine sahiptir. Bu durum iki maddenin ayırt edicilik parametrelerindeki farklılıktan kaynaklanmaktadır. Dördüncü madde yüksek bir ayırt edicilik parametresi ile ideal bir madde gibi görünürken, 12. madde düşük bir ayırt edicilik parametresi ile kısmi problemli bir madde görüntüsü sergilemektedir. Yine de 12 numaralı maddenin parametre değerlerinin kabul edilebilir olduğu da vurgulanmalıdır. \n\n\n2.d. Test Bilgi Fonksiyonu\nİki kategorili puanlanan maddelere yönelik test bilgi fonksiyonunu elde etmek için aşağıdaki kod satırları kullanılmıştır. locator() fonksiyonu aracılığı ile test bilgi fonksiyonunun tepe noktası tespit edilmiş ve ardından abline() fonksiyonu ile tepe noktasının koordinat çizgileri grafiğe eklenmiştir.\n\nplot(model2, type=\"IIC\", items = 0, xlab= \"YETENEK\", cex.main = 1, main = \"TEST   BİLGİ   FONKSİYONU\", ylab = \"BİLGİ\" , lwd= 2,col.main=\"red\",col=\"blue\", font.axis= 3, font.lab=2)\naxis(2, at = seq(0, 30, by =5))\naxis(1, at = seq(-4, 4, by = .1))\nabline(h=27.74, v=-.8451, lty=4)\n\n\n\n\nTest bilgi fonksiyonunun -0.84 yetenek düzeyinde en yüksek seviyede olduğu, -1.5 ile 0 yetenek düzeylerinde sert bir şekilde düşmeye başladığı ve -2.8 ve 1.1 yetenek düzeylerinde en düşük seviyesinde olduğu görülmektedir. Bu durumda en yüksek bilginin bu ölçekte -.8 yetenek düzeylerinde sağlandığı söylenebilir. \nBu durumda her iki bileşenin de en çok 0 yetenek düzeyinde bilgi sağladığı ve -2 ile +2 aralığında yüksek düzeyde bilgi sağladığı söylenebilir. Ancak bu aralığın ötesinde sağlanan bilginin hızla azaldığı düşünülebilir.\n\n\n2.e. Yetenek Puanları\nİki kategorili puanlanan maddelere yönelik olarak yürütülen analizlerin son aşamasında bireylere ait yetenek puanları hesaplanmış ve bunların dağılımı incelenmiştir. Şu kod satırları ile hesaplanan yetenek puanları veri setine eklenmiştir.\n\nscore_d5<- factor.scores.ltm(model2)\noruntu_d5 <- score_d5[[1]]\noruntu_d5$toplam <- rowSums((oruntu_d5[,1:12]))\ntheta_d5 <- numeric()\nfor (i in 1:454){\n     for (j in 1:1000){\n         if (sum (oruntu_d5[i, 1: 20] == data5[j, 1:20])==20)\n             theta_d5[j] <- oruntu_d5[i, 23] }}\ndata5$theta<-theta_d5\ndata5$toplam<-rowSums(data5[1:20])\n\nBunun ardından describe() fonksiyonu ile birey yetenek puanlarının betimsel istatikleri elde edilmiştir. Ayrıca hist() fonksiyonu ile histogram grafikleri oluşturulmuştur. Bu süreçlerde kullanılan kod satırları şunlardır:\n\n#BİLEŞEN-1\ndescribe(data5$theta)\n\n   vars    n  mean   sd median trimmed  mad   min  max range  skew kurtosis\nX1    1 1000 -0.03 0.81   0.05    0.04 1.24 -2.17 0.89  3.06 -0.45     -0.8\n     se\nX1 0.03\n\nhist(data5$theta, xlab= \"YETENEK\", cex.main = 1, ylab = \"FREKANS\" , col.main= \"red\", font.axis= 3, font.lab=2, main = \"BİREY YETENEK PUANLARI\")\n\n\n\n\nİncelenen iki kategorili puanlanan veriye ait betimsel istatikleri incelendiğinde, ortalamanın -.03, standart sapmanın da .81 olduğu görülmektedir. Bu durum verinin normal dağıldığı şeklinde yorumlanabilir, ancak histogram grafiği incelendiğinde yetenek puanlarının -2 ile 1 arasında dağılım gösterdiği ve 1 yetenek puanında bir yığılma olduğu görülmektedir. Bu durum, verilerin çok kategorili puanlanan maddelerden ortalamaları doğrultusunda iki kategorili puanlanan maddelere çevrilmesiyle ilişkisi olduğu düşünülmektedir."
  },
  {
    "objectID": "posts/Item Analysis with Item Response Theory (in Turkish)/index.html#sonuç",
    "href": "posts/Item Analysis with Item Response Theory (in Turkish)/index.html#sonuç",
    "title": "Item Analysis with Item Response Theory (in Turkish)",
    "section": "SONUÇ",
    "text": "SONUÇ\nBu çalışma simüle edilmiş veri ile yürütülmüştür. Çıktılarında bu nedenle temel problemler görülebilmektedir. Yine de aşama aşama MTK ile madde analizinin R ile nasıl yapılacağına dair bana güzel bir referans olmaktadır. Bu çalışma, bir ders raporu olarak hazırlanmıştır."
  },
  {
    "objectID": "posts/Structural Equation Modelling (in Turkish)/index.html",
    "href": "posts/Structural Equation Modelling (in Turkish)/index.html",
    "title": "Structural Equation Modelling (in Turkish)",
    "section": "",
    "text": "Bu çalışmada PISA 2015 “student questionnaire: paper based verison” içerisindeki en son alt ölçek olan ‘Your View on Science’ ölçeğinden elde edilen veriler kullanılmıştır. Türkiye örnekleminden faydalanılmıştır. Bu örneklem ile bahsi geçen ölçeğin yapısal eşitlik modelleri oluşturulmuş ve karşılaştırılmıştır. Adım adım veri setinin R ile analize hazır hale getirilmesi anlatılmaktadır. Doğrudan analiz ile ilgileniyorsanız aşağıda ‘Analiz Süreci’ başlığına ilerleyiniz."
  },
  {
    "objectID": "posts/Structural Equation Modelling (in Turkish)/index.html#pre-processing-süreci",
    "href": "posts/Structural Equation Modelling (in Turkish)/index.html#pre-processing-süreci",
    "title": "Structural Equation Modelling (in Turkish)",
    "section": "Pre-processing süreci",
    "text": "Pre-processing süreci\nÖncelikle OECD tarafından yayınlanan veri setini şuradan indiriyoruz. İndirilen bu dosya .sav uzantılı bbir dosya. Bu tür dosyaları açmak için haven paketi read_sav fonksiyonundan faydalanabiliriz. Veri setinin tamamına ihtiyacımız yok, zaten oldukça büyük bir veri. Ülke değilşkeni ve ilgili ölçeğin maddeleri yeterli olacaktır. İndirdiğimiz .sav uzantılı dosyayı working directory’mize taşıdıktan sonra çalıştıracağımız kod satırı şunlar:\n\nlibrary(haven) \ndata <- na.omit(read_sav(\n  \"CY6_MS_CMB_STU_QQQ.sav\",\n  col_select = c(\n    \"CNT\",\n    \"ST092Q01TA\",\n    \"ST092Q02TA\",\n    \"ST092Q04TA\",\n    \"ST092Q05TA\",\n    \"ST092Q06NA\",\n    \"ST092Q08NA\",\n    \"ST092Q09NA\",\n    \n    \"ST094Q01NA\",\n    \"ST094Q02NA\",\n    \"ST094Q03NA\",\n    \"ST094Q04NA\",\n    \"ST094Q05NA\",\n    \n    \"ST113Q01TA\",\n    \"ST113Q02TA\",\n    \"ST113Q03TA\",\n    \"ST113Q04TA\",\n    \n    \"ST129Q01TA\",\n    \"ST129Q02TA\",\n    \"ST129Q03TA\",\n    \"ST129Q04TA\",\n    \"ST129Q05TA\",\n    \"ST129Q06TA\",\n    \"ST129Q07TA\",\n    \"ST129Q08TA\",\n    \n    \"ST131Q01NA\",\n    \"ST131Q03NA\",\n    \"ST131Q04NA\",\n    \"ST131Q06NA\",\n    \"ST131Q08NA\",\n    \"ST131Q11NA\"\n  )\n))\n\nÜlke değişkenine artık ihtiyacımız yok. O nedenle veri setimizi sadece ölçek maddelerini içerecek şekilde yeniden tanımlayalım:\n\nlibrary(dplyr)\nscience_data<-filter(data, CNT==\"TUR\")[,-1]\n\nBu veri setini şu kod satırını yürüterek .csv uzantılı bir dosya olarak bilgisayarıma kaydediyorum. Böylece analiz aşamasında o halini de paylaşabileceğim:\n\nwrite.csv(science_data, \"science_data.csv\", row.names = FALSE)"
  },
  {
    "objectID": "posts/Structural Equation Modelling (in Turkish)/index.html#analiz-süreci",
    "href": "posts/Structural Equation Modelling (in Turkish)/index.html#analiz-süreci",
    "title": "Structural Equation Modelling (in Turkish)",
    "section": "Analiz Süreci",
    "text": "Analiz Süreci\nİlgili ölçekten elde edilen veriyi .csv uzantılı dosya olarak indirebilirsiniz. Bunu read.csv fonksiyonu ile R ortamına aktarabilirsiniz. Veri setimiz hazır. Analizimizde öncelikle doğrulayı faktör analizi kullanacağız. Daha sonra da bi-faktör modelleme yapacağız. Veri setimizi tekrar yükleyelim:\n\n\n\n\nDoğrulayıcı Faktör Analizi (DFA)\nÇalışmamıza konu olan ölçek beş faktörden oluşmaktadır. Bu yapı ile ölçek geliştirme sürecinde tanımlanmıştır. Her faktörde farklı sayılarda maddeler yer almaktadır. Veri setinde bunlar ST ön eki ve faktör numarası ile tanımlanmıştır. Faktörleri ve veri setindeki kodlamalarını şu şekilde listeleyelim:\n\nST092: How informed are you about the following environmental issues?\nST094: How much do you disagree or agree with the statements about yourself below?\nST113: How much do you agree with the statements below?\nST129: How easy do you think it would be for you to perform the following tasks on your own?\nST131: How much do you disagree or agree with the statements below?\n\nBu beş faktörün altında tanımlanan yapıya göre DFA uygulayacağız ve bir geçerlilik çalışması yürüteceğiz. Bunun için öncelikle modelimizi R’a tanıtalım:\n\n\nCode\nScience.model <- 'ST092 =~ 1* ST092Q01TA+\n                              ST092Q02TA+\n                              ST092Q04TA+\n                              ST092Q05TA+\n                              ST092Q06NA+\n                              ST092Q08NA+\n                              ST092Q09NA\n                              \n                  ST094 =~ 1* ST094Q01NA+\n                              ST094Q02NA+\n                              ST094Q03NA+\n                              ST094Q04NA+\n                              ST094Q05NA\n                              \n                  ST113 =~ 1* ST113Q01TA+\n                              ST113Q02TA+\n                              ST113Q03TA+\n                              ST113Q04TA\n                  \n                  ST129 =~ 1* ST129Q01TA+\n                              ST129Q02TA+\n                              ST129Q03TA+\n                              ST129Q04TA+\n                              ST129Q05TA+\n                              ST129Q06TA+\n                              ST129Q07TA+\n                              ST129Q08TA\n                              \n                  ST131 =~ 1* ST131Q01NA+\n                              ST131Q03NA+\n                              ST131Q04NA+\n                              ST131Q06NA+\n                              ST131Q08NA+\n                              ST131Q11NA            \n                    \n                           \n                            ST092 ~~ ST094\n                            ST092 ~~ ST113\n                            ST092 ~~ ST129\n                            ST092 ~~ ST131\n                            \n                            ST094 ~~ ST113\n                            ST094 ~~ ST129\n                            ST094 ~~ ST131\n                            \n                            ST113 ~~ ST129\n                            ST113 ~~ ST131\n                            \n                            ST129 ~~ ST131'\n\n\nŞimdi Türkiye örnekleminden elde edilen verimizin bu modele uyum sağlayıp sağlamadığına bakacağız. Bu aşamada çeşitli modelleme yaklaşımlarından çıktılar alarak bunları karşılaştıracağız. Bu modelleme yaklaşımları şunlardır:\n\nmaximum likelihood model (MLM)\nweighted least squares (WLS)\nrobust maximum likelihood model (RMLM)\ndiagonally weighted least squares (DWLS)\n\nYukarıdaki lsitede görüldüğü sıra ile modellerimizi lavaan paketiyle oluşturalım:\n\n\nCode\nlibrary(lavaan)\nmodel_mlm <- cfa(Science.model, data = science_data)\nmodel_wls <- cfa(Science.model, WLS.V = TRUE, data = science_data)\nmodel_rml <- cfa(Science.model, estimator = \"MLM\", se = \"robust.mlm\", data = science_data)\nmodel_dwls<-cfa(Science.model, data = science_data, estimator=\"DWLS\")\n\n\nHer biri için de ayrı ayrı analiz çıktılarını summary() fonksiyonu ile tanımlayalım:\n\n\nCode\nscience.mlm<-summary(\n  model_mlm,\n  fit.measures = TRUE,\n  standardized = TRUE,\n  rsquare = TRUE,\n  modindices = TRUE\n)\nscience.wls<-summary(\n  model_wls,\n  fit.measures = TRUE,\n  standardized = TRUE,\n  rsquare = TRUE,\n  modindices = TRUE\n)\nscience.rml<-summary(\n  model_rml,\n  fit.measures = TRUE,\n  standardized = TRUE,\n  rsquare = TRUE,\n  modindices = TRUE\n)\nscience.dwls<-summary(\n  model_dwls,\n  fit.measures = TRUE,\n  standardized = TRUE,\n  rsquare = TRUE,\n  modindices = TRUE\n)\n\n\nTabi ki hem literatürde en yaygın kullanılan hem de alan uzmanları tarafından en ok önerilen yöntem olması sebebiyle MLM yöntemi önceliğimiz. Bu yöntemde normallik varsayımı karşılandığı sürece güçlü analizler elde edilebilmektedir. Veri setimiz de büyük bir örneklemden elde edildiği için bu varsayımın karşılandığı düşünülmektedir. Grafik incelemelerinde de bu durum görülecektir. Dolayısıyla öncelikle MLM’ye ait uyum indekslerini görelim:\n\nscience.mlm[[\"fit\"]]\n\n\n\nCode\nlibrary(lavaan)\nScience.model <- 'ST092 =~ 1* ST092Q01TA+\n                              ST092Q02TA+\n                              ST092Q04TA+\n                              ST092Q05TA+\n                              ST092Q06NA+\n                              ST092Q08NA+\n                              ST092Q09NA\n                              \n                  ST094 =~ 1* ST094Q01NA+\n                              ST094Q02NA+\n                              ST094Q03NA+\n                              ST094Q04NA+\n                              ST094Q05NA\n                              \n                  ST113 =~ 1* ST113Q01TA+\n                              ST113Q02TA+\n                              ST113Q03TA+\n                              ST113Q04TA\n                  \n                  ST129 =~ 1* ST129Q01TA+\n                              ST129Q02TA+\n                              ST129Q03TA+\n                              ST129Q04TA+\n                              ST129Q05TA+\n                              ST129Q06TA+\n                              ST129Q07TA+\n                              ST129Q08TA\n                              \n                  ST131 =~ 1* ST131Q01NA+\n                              ST131Q03NA+\n                              ST131Q04NA+\n                              ST131Q06NA+\n                              ST131Q08NA+\n                              ST131Q11NA            \n                    \n                           \n                            ST092 ~~ ST094\n                            ST092 ~~ ST113\n                            ST092 ~~ ST129\n                            ST092 ~~ ST131\n                            \n                            ST094 ~~ ST113\n                            ST094 ~~ ST129\n                            ST094 ~~ ST131\n                            \n                            ST113 ~~ ST129\n                            ST113 ~~ ST131\n                            \n                            ST129 ~~ ST131'\nmodel_mlm <- cfa(Science.model, data = science_data)\nscience.mlm<-summary(\n  model_mlm,\n  fit.measures = TRUE,\n  standardized = TRUE,\n  rsquare = TRUE,\n  modindices = TRUE\n)\nscience.mlm[[\"fit\"]]\n\n\n             npar              fmin             chisq                df \n           70.000             0.652          6055.193           395.000 \n           pvalue    baseline.chisq       baseline.df   baseline.pvalue \n            0.000         96221.017           435.000             0.000 \n              cfi               tli              logl unrestricted.logl \n            0.941             0.935       -135531.110       -132503.514 \n              aic               bic            ntotal              bic2 \n       271202.220        271653.284          4646.000        271430.850 \n            rmsea    rmsea.ci.lower    rmsea.ci.upper      rmsea.pvalue \n            0.056             0.054             0.057             0.000 \n             srmr \n            0.039 \n\n\nBu çıktılar incelendiğinde, 70 parametreli 395 serbestlik derecesinde bir model oluştuğu görülmektedir. CFI ve TLI uyum indekleri .90 eşik değerin üzerindeyken, RMSEA ve SRMR .06’nın altında yer almaktadır. Bu durumda modelimizin uyumlu olduğu düşünülebilir. Yine de diğer modeller ile de karşılaştırmak gerekir. Onların uyum indeklerini de görelim:\nWLS model:\n\n\nCode\nscience.wls[[\"fit\"]]\n\n\n\n\n             npar              fmin             chisq                df \n           70.000             0.652          6055.193           395.000 \n           pvalue    baseline.chisq       baseline.df   baseline.pvalue \n            0.000         96221.017           435.000             0.000 \n              cfi               tli              logl unrestricted.logl \n            0.941             0.935       -135531.110       -132503.514 \n              aic               bic            ntotal              bic2 \n       271202.220        271653.284          4646.000        271430.850 \n            rmsea    rmsea.ci.lower    rmsea.ci.upper      rmsea.pvalue \n            0.056             0.054             0.057             0.000 \n             srmr \n            0.039 \n\n\nRML model:\n\nscience.rml[[\"fit\"]]\n\n\n\n                         npar                          fmin \n                       70.000                         0.652 \n                        chisq                            df \n                     6055.193                       395.000 \n                       pvalue                  chisq.scaled \n                        0.000                      4484.352 \n                    df.scaled                 pvalue.scaled \n                      395.000                         0.000 \n         chisq.scaling.factor                baseline.chisq \n                        1.350                     96221.017 \n                  baseline.df               baseline.pvalue \n                      435.000                         0.000 \n        baseline.chisq.scaled            baseline.df.scaled \n                    75815.224                       435.000 \n       baseline.pvalue.scaled baseline.chisq.scaling.factor \n                        0.000                         1.269 \n                          cfi                           tli \n                        0.941                         0.935 \n                   cfi.scaled                    tli.scaled \n                        0.946                         0.940 \n                   cfi.robust                    tli.robust \n                        0.942                         0.936 \n                         logl             unrestricted.logl \n                  -135531.110                   -132503.514 \n                          aic                           bic \n                   271202.220                    271653.284 \n                       ntotal                          bic2 \n                     4646.000                    271430.850 \n                        rmsea                rmsea.ci.lower \n                        0.056                         0.054 \n               rmsea.ci.upper                  rmsea.pvalue \n                        0.057                         0.000 \n                 rmsea.scaled         rmsea.ci.lower.scaled \n                        0.047                         0.046 \n        rmsea.ci.upper.scaled           rmsea.pvalue.scaled \n                        0.048                         1.000 \n                 rmsea.robust         rmsea.ci.lower.robust \n                        0.055                         0.053 \n        rmsea.ci.upper.robust           rmsea.pvalue.robust \n                        0.056                            NA \n                         srmr \n                        0.039 \n\n\nDWLS model:\n\nscience.dwls[[\"fit\"]]\n\n\n\n           npar            fmin           chisq              df          pvalue \n         70.000           0.188        1744.348         395.000           0.000 \n baseline.chisq     baseline.df baseline.pvalue             cfi             tli \n     120867.135         435.000           0.000           0.989           0.988 \n          rmsea  rmsea.ci.lower  rmsea.ci.upper    rmsea.pvalue            srmr \n          0.027           0.026           0.028           1.000           0.034 \n\n\nTüm bu çıktılar incelendiğinde, en uygun modelin diagonally weighted least squares (DWLS) olduğu düşünülmektedir. Veri setinin farklı ölçek düzeyinde olması ve kategorik olması bu durumun sebebi olabilir. DWLS modeli, liteatürde yaygın bir şekilde bu tür veri setleri için önerilmektedir.\n\n\nBI-FACTOR MODELLER\nBi faktör modellemede yapıyı oluşturan faktörlerin tüm maddelerden oluşan genel faktör ile ilişkisi incelenerek karar verilir. Bu amaçla DFA örneğinde olduğu gibi model tanımlamamızı yapıyoruz:\n\n\nCode\nScience.bifactormodel <- 'general.factor =~\n                              ST092Q01TA+\n                              ST092Q02TA+\n                              ST092Q04TA+\n                              ST092Q05TA+\n                              ST092Q06NA+\n                              ST092Q08NA+\n                              ST092Q09NA+\n                              ST094Q01NA+\n                              ST094Q02NA+\n                              ST094Q03NA+\n                              ST094Q04NA+\n                              ST094Q05NA+\n                              ST113Q01TA+\n                              ST113Q02TA+\n                              ST113Q03TA+\n                              ST113Q04TA+\n                              ST129Q01TA+\n                              ST129Q02TA+\n                              ST129Q03TA+\n                              ST129Q04TA+\n                              ST129Q05TA+\n                              ST129Q06TA+\n                              ST129Q07TA+\n                              ST129Q08TA+\n                              ST131Q01NA+\n                              ST131Q03NA+\n                              ST131Q04NA+\n                              ST131Q06NA+\n                              ST131Q08NA+\n                              ST131Q11NA\n\n                  ST092 =~ 1* ST092Q01TA+\n                              ST092Q02TA+\n                              ST092Q04TA+\n                              ST092Q05TA+\n                              ST092Q06NA+\n                              ST092Q08NA+\n                              ST092Q09NA\n\n                  ST094 =~ 1* ST094Q01NA+\n                              ST094Q02NA+\n                              ST094Q03NA+\n                              ST094Q04NA+\n                              ST094Q05NA\n\n                  ST113 =~ 1* ST113Q01TA+\n                              ST113Q02TA+\n                              ST113Q03TA+\n                              ST113Q04TA\n\n                  ST129 =~ 1* ST129Q01TA+\n                              ST129Q02TA+\n                              ST129Q03TA+\n                              ST129Q04TA+\n                              ST129Q05TA+\n                              ST129Q06TA+\n                              ST129Q07TA+\n                              ST129Q08TA\n\n                  ST131 =~ 1* ST131Q01NA+\n                              ST131Q03NA+\n                              ST131Q04NA+\n                              ST131Q06NA+\n                              ST131Q08NA+\n                              ST131Q11NA\n                              \n\n                general.factor  ~~ 0*ST092\n                general.factor  ~~ 0*ST094\n                general.factor  ~~ 0*ST113\n                general.factor  ~~ 0*ST129\n                general.factor  ~~ 0*ST131\n                \n                            ST092 ~~ ST094\n                            ST092 ~~ ST113\n                            ST092 ~~ ST129\n                            ST092 ~~ ST131\n\n                            ST094 ~~ ST113\n                            ST094 ~~ ST129\n                            ST094 ~~ ST131\n\n                            ST113 ~~ ST129\n                            ST113 ~~ ST131\n\n                            ST129 ~~ ST131'\n\n\nArdından modellerimizi veri setimiz ile sınıyoruz.\n\n\nCode\nScience.bifactormodel_mlm <-\n  cfa(\n    Science.bifactormodel,\n    data = science_data,\n    std.lv = TRUE,\n    information = \"observed\"\n  )\nScience.bifactormodel_rml <-\n  cfa(\n    Science.bifactormodel,\n    data = science_data,\n    estimator = \"MLM\",\n    se = \"robust.mlm\",\n    std.lv = TRUE,\n    information = \"observed\"\n  )\nbifactor_mlm <- summary(Science.bifactormodel_mlm ,\n        fit.measures = TRUE,\n        standardized = TRUE)\nbifactor_rml <- summary(Science.bifactormodel_rml ,\n        fit.measures = TRUE,\n        standardized = TRUE)\n\nbifactor_mlm[['fit']]\n\n\n             npar              fmin             chisq                df \n           95.000             0.634          5891.081           370.000 \n           pvalue    baseline.chisq       baseline.df   baseline.pvalue \n            0.000         96221.017           435.000             0.000 \n              cfi               tli              logl unrestricted.logl \n            0.942             0.932       -135449.054       -132503.514 \n              aic               bic            ntotal              bic2 \n       271088.109        271700.266          4646.000        271398.392 \n            rmsea    rmsea.ci.lower    rmsea.ci.upper      rmsea.pvalue \n            0.057             0.055             0.058             0.000 \n             srmr \n            0.328 \n\n\nCode\nbifactor_rml[['fit']]\n\n\n                         npar                          fmin \n                       95.000                         0.634 \n                        chisq                            df \n                     5891.081                       370.000 \n                       pvalue                  chisq.scaled \n                        0.000                      4166.363 \n                    df.scaled                 pvalue.scaled \n                      370.000                         0.000 \n         chisq.scaling.factor                baseline.chisq \n                        1.414                     96221.017 \n                  baseline.df               baseline.pvalue \n                      435.000                         0.000 \n        baseline.chisq.scaled            baseline.df.scaled \n                    19393.334                       435.000 \n       baseline.pvalue.scaled baseline.chisq.scaling.factor \n                        0.000                         4.962 \n                          cfi                           tli \n                        0.942                         0.932 \n                   cfi.scaled                    tli.scaled \n                        0.800                         0.765 \n                   cfi.robust                    tli.robust \n                        0.943                         0.933 \n                         logl             unrestricted.logl \n                  -135449.054                   -132503.514 \n                          aic                           bic \n                   271088.109                    271700.266 \n                       ntotal                          bic2 \n                     4646.000                    271398.392 \n                        rmsea                rmsea.ci.lower \n                        0.057                         0.055 \n               rmsea.ci.upper                  rmsea.pvalue \n                        0.058                         0.000 \n                 rmsea.scaled         rmsea.ci.lower.scaled \n                        0.047                         0.046 \n        rmsea.ci.upper.scaled           rmsea.pvalue.scaled \n                        0.048                         1.000 \n                 rmsea.robust         rmsea.ci.lower.robust \n                        0.056                         0.054 \n        rmsea.ci.upper.robust           rmsea.pvalue.robust \n                        0.057                            NA \n                         srmr \n                        0.328 \n\n\nBI-FAKTÖR ML model:\n\n\n             npar              fmin             chisq                df \n           95.000             0.634          5891.081           370.000 \n           pvalue    baseline.chisq       baseline.df   baseline.pvalue \n            0.000         96221.017           435.000             0.000 \n              cfi               tli              logl unrestricted.logl \n            0.942             0.932       -135449.054       -132503.514 \n              aic               bic            ntotal              bic2 \n       271088.109        271700.266          4646.000        271398.392 \n            rmsea    rmsea.ci.lower    rmsea.ci.upper      rmsea.pvalue \n            0.057             0.055             0.058             0.000 \n             srmr \n            0.328 \n\n\nBI-FAKTÖR ROBUST ML model:\n\n\n                         npar                          fmin \n                       95.000                         0.634 \n                        chisq                            df \n                     5891.081                       370.000 \n                       pvalue                  chisq.scaled \n                        0.000                      4166.363 \n                    df.scaled                 pvalue.scaled \n                      370.000                         0.000 \n         chisq.scaling.factor                baseline.chisq \n                        1.414                     96221.017 \n                  baseline.df               baseline.pvalue \n                      435.000                         0.000 \n        baseline.chisq.scaled            baseline.df.scaled \n                    19393.334                       435.000 \n       baseline.pvalue.scaled baseline.chisq.scaling.factor \n                        0.000                         4.962 \n                          cfi                           tli \n                        0.942                         0.932 \n                   cfi.scaled                    tli.scaled \n                        0.800                         0.765 \n                   cfi.robust                    tli.robust \n                        0.943                         0.933 \n                         logl             unrestricted.logl \n                  -135449.054                   -132503.514 \n                          aic                           bic \n                   271088.109                    271700.266 \n                       ntotal                          bic2 \n                     4646.000                    271398.392 \n                        rmsea                rmsea.ci.lower \n                        0.057                         0.055 \n               rmsea.ci.upper                  rmsea.pvalue \n                        0.058                         0.000 \n                 rmsea.scaled         rmsea.ci.lower.scaled \n                        0.047                         0.046 \n        rmsea.ci.upper.scaled           rmsea.pvalue.scaled \n                        0.048                         1.000 \n                 rmsea.robust         rmsea.ci.lower.robust \n                        0.056                         0.054 \n        rmsea.ci.upper.robust           rmsea.pvalue.robust \n                        0.057                            NA \n                         srmr \n                        0.328 \n\n\nBu çıktılar incelendiğinde de bi faktör modelin verimize DWLS model kadar uyumlu olmadığı görülmektedir. Bu nedenle DWLS modelin en uyumlu model olduğu düşünülmektedir. Bu modelin çıktılarını tablolaştıralım.\n\n\nEN UYUMLU MODEL:DWLS çıktıları\nNOT: Faktör varyansları sabitlenerek model oluşturulduğu için faktör varyansları tablosu raporlaştırılmamıştır.\n\nModel uyum indeksleri\n\nParametre Tahminleri\n\n\n\nFaktör Kovaryansları\n\n\n\nArtık (residual) Varyanslar\n\n\n\nModel gösterimi"
  },
  {
    "objectID": "posts/Test Equating/index.html",
    "href": "posts/Test Equating/index.html",
    "title": "Test Equating",
    "section": "",
    "text": "2020-2021 Fall Term A Level’s first quiz has 40 items. 2022-2023 Fall Term A Level’s first quiz has 40 items. 35 of the items in each test forms are unique items while 5 of them are common, thus will be called as “anchor items” in this study. For the readers interest, the items belong to four main domains (listening, structure, vocabbulary and reading), yet the common items are only in the reading section. This is obviously a violation of assumptions of test equating. Still, this study is conveyed for demonstration purposes. Therefore, let’s continue:\nTo ensure statistical equation of these two forms, we first introduced the data in R Studio and the first five rows can be seen below:\n\nQ1 <- read.csv(\"kitap1.csv\", header = TRUE)\nhead(Q1)\n\n  L1 L2 L3 L4 L5 L6 L7 L8 L9 L10 S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 V1 V2 V3 V4 V5\n1  0  1  0  1  1  1  0  1  1   0  1  1  0  1  0  0  0  0  0   1  0  1  1  0  1\n2  1  1  1  1  1  1  1  1  1   1  0  1  1  0  1  1  0  0  0   0  1  1  1  1  0\n3  1  1  1  1  1  1  0  1  1   0  1  1  0  1  1  0  0  1  1   0  0  1  0  1  0\n4  1  1  1  1  1  1  1  0  1   0  1  1  0  1  1  0  1  1  0   0  0  1  1  0  1\n5  1  1  1  1  1  1  1  0  1   1  1  1  1  1  0  1  0  1  0   0  1  1  1  0  0\n6  1  1  1  1  0  0  1  1  1   1  0  0  0  0  0  1  1  1  1   1  1  1  1  0  0\n  V6 V7 V8 V9 V10 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 form\n1  0  1  1  1   1  1  1  0  1  1  1  1  1  1   1    x\n2  0  1  1  0   0  1  0  1  1  0  1  1  0  1   1    x\n3  1  0  1  1   1  1  0  0  1  1  1  0  1  1   1    x\n4  1  1  1  0   1  1  1  1  1  0  1  0  0  1   1    x\n5  1  1  1  0   1  1  1  1  0  1  1  1  0  1   1    x\n6  1  1  1  1   1  1  1  1  1  1  1  1  1  1   1    x\n\n\nLater, we introduced the unique and anchor items separately. First 35 items are unique items, and the last 5 items are anchor items.\n\n# Calculate total scores based on unique items\nQ1$total <- rowSums(Q1[, 1:35])\n\n# Calculate scores based on anchor items\nQ1$anchor <- rowSums(Q1[, 36:40])\n\nAs we will use the equate package, the data should be contained as frequency tables: Form x (20-21 fall) had a sample of 200 while form y (22-23 fall) had a sample of 133 students. They are defined as:\n\n#first introduce the equate package:\nlibrary(equate)\n# Create frequency tables (total score range: 0-35; anchor score range: 0-5)\nQ1_x <- freqtab(Q1[1:200, c(\"total\", \"anchor\")], scales = list(0:35, 0:5))\nQ1_y <- freqtab(Q1[201:334, c(\"total\", \"anchor\")], scales = list(0:35, 0:5))\n\nTo consideration of the reader one more time, we must state that these forms are the first quizzes of the students. They usually get high marks. For instance, you can see below that the students’ scores are distributed left-skewed in both forms. Their total correct answers are distributed between 20 and 35 for the unique items , and between 3 and 5 for anchor items in form X. The situation isn’t different for form y.\n\n#distrubution of the data among forms and unique/common items\nplot(Q1_x, xlab = \"Total Scores Form X\", ylab = \"Common Anchor Scores Form X\")\n\n\n\nplot(Q1_y, xlab = \"Total Scores Form y\", ylab = \"Common Anchor Scores Form y\")\n\n\n\n\nStill, let’s continue… with the smoothing procedure. For both forms, we utilized loglinear presmoothing. Of course there are several other methods, yet literature shows not much a big difference between them, thus not much care given to this issue. again as this is a study with demonstration purposes. After the smoothing, it can be realized that the distribution is highly eye-pleasing right now. Also it is much easier to match the scores even if there isn’t an equivalent of it in the other form.\n\n#PRESMOOTHING\nsmooth_x <- presmoothing(Q1_x, smoothmethod = \"loglinear\")\n\nWarning: glm.fit: fitted rates numerically 0 occurred\n\nsmooth_y <- presmoothing(Q1_y, smoothmethod = \"loglinear\")\n\nWarning: glm.fit: algorithm did not converge\n\nWarning: glm.fit: fitted rates numerically 0 occurred\n\nplot(smooth_x, xlab = \"Total Scores Form X\", ylab = \"Common Anchor Scores Form X\")\n\n\n\nplot(smooth_y, xlab = \"Total Scores Form y\", ylab = \"Common Anchor Scores Form y\")\n\n\n\n\nNow, it can be roughly said that the forms are ready to be equated. Before we try several methods, lets see the results of the Tucker method as it can produce equating error as well:\n\n## Linear Tucker Equating\nQ1_tucker <- equate(Q1_x, Q1_y, type = \"linear\", method = \"tucker\")\nQ1_tucker$concordance\n\n   scale        yx      se.n      se.g\n1      0  6.597617 2.0378528 3.3547355\n2      1  7.404998 1.9751403 3.2551073\n3      2  8.212379 1.9124541 3.1554875\n4      3  9.019759 1.8497970 3.0558770\n5      4  9.827140 1.7871721 2.9562768\n6      5 10.634520 1.7245827 2.8566879\n7      6 11.441901 1.6620331 2.7571116\n8      7 12.249282 1.5995277 2.6575492\n9      8 13.056662 1.5370720 2.5580024\n10     9 13.864043 1.4746724 2.4584730\n11    10 14.671423 1.4123363 2.3589634\n12    11 15.478804 1.3500723 2.2594761\n13    12 16.286185 1.2878911 2.1600141\n14    13 17.093565 1.2258052 2.0605812\n15    14 17.900946 1.1638299 1.9611818\n16    15 18.708326 1.1019838 1.8618212\n17    16 19.515707 1.0402899 1.7625060\n18    17 20.323088 0.9787772 1.6632444\n19    18 21.130468 0.9174818 1.5640464\n20    19 21.937849 0.8564507 1.4649252\n21    20 22.745229 0.7957445 1.3658973\n22    21 23.552610 0.7354438 1.2669847\n23    22 24.359991 0.6756570 1.1682166\n24    23 25.167371 0.6165338 1.0696330\n25    24 25.974752 0.5582849 0.9712903\n26    25 26.782132 0.5012154 0.8732696\n27    26 27.589513 0.4457784 0.7756933\n28    27 28.396894 0.3926659 0.6787528\n29    28 29.204274 0.3429596 0.5827655\n30    29 30.011655 0.2983669 0.4882941\n31    30 30.819035 0.2615166 0.3964236\n32    31 31.626416 0.2360629 0.3094791\n33    32 32.433797 0.2258919 0.2330408\n34    33 33.241177 0.2330134 0.1809526\n35    34 34.048558 0.2559883 0.1763087\n36    35 34.855938 0.2910866 0.2221053\n\n\nYou will see that the equating errors are above 1 before the score of 25 as there isn’t much data in the low scores. Also, as we investigate the lower marks, we see that the gap between equated scores are increasing. For instance, 0 on form X is equal to 6.597617 on form Y. This is because there isn’t data in these regions of the scores. Despite that, equated scores get more meaningful after 20. Especially after the total score 30, the equated scores are too close and the equation error is too low, which would be quite better if the situation was like that on all total score ranges. Let’s see some other equating methods:\n\n## Comparing Multiple Methods\n# Nominal method with mean equating\nQ1_nom <- equate(Q1_x, Q1_y, type = \"mean\", method = \"nom\")\n\n# Frequency method with equipercentile\nQ1_freq <- equate(Q1_x, Q1_y, type = \"equip\", method = \"freq\")\n\n# Braun method with linear equating\nQ1_braun <- equate(Q1_x, Q1_y, type = \"linear\", method = \"braun\")\n\n# Compare equated scores\nround(cbind(xscale = 0:35, \n            nominal = Q1_nom$concordance$yx,\n            tucker = Q1_tucker$concordance$yx, \n            freq = Q1_freq$concordance$yx, \n            braun = Q1_braun$concordance$yx), 2)\n\n      xscale nominal tucker  freq braun\n [1,]      0   -0.01   6.60 -0.50  5.65\n [2,]      1    0.99   7.40 -0.50  6.49\n [3,]      2    1.99   8.21 -0.50  7.32\n [4,]      3    2.99   9.02 -0.50  8.16\n [5,]      4    3.99   9.83 -0.50  9.00\n [6,]      5    4.99  10.63 -0.50  9.83\n [7,]      6    5.99  11.44 -0.50 10.67\n [8,]      7    6.99  12.25 -0.50 11.50\n [9,]      8    7.99  13.06 -0.50 12.34\n[10,]      9    8.99  13.86 -0.50 13.17\n[11,]     10    9.99  14.67 -0.50 14.01\n[12,]     11   10.99  15.48 -0.50 14.84\n[13,]     12   11.99  16.29 -0.50 15.68\n[14,]     13   12.99  17.09 -0.50 16.51\n[15,]     14   13.99  17.90 -0.50 17.35\n[16,]     15   14.99  18.71 -0.50 18.19\n[17,]     16   15.99  19.52 -0.50 19.02\n[18,]     17   16.99  20.32 -0.50 19.86\n[19,]     18   17.99  21.13 -0.50 20.69\n[20,]     19   18.99  21.94 -0.50 21.53\n[21,]     20   19.99  22.75 -0.50 22.36\n[22,]     21   20.99  23.55 23.66 23.20\n[23,]     22   21.99  24.36 23.82 24.03\n[24,]     23   22.99  25.17 24.10 24.87\n[25,]     24   23.99  25.97 24.38 25.71\n[26,]     25   24.99  26.78 24.57 26.54\n[27,]     26   25.99  27.59 25.35 27.38\n[28,]     27   26.99  28.40 27.28 28.21\n[29,]     28   27.99  29.20 29.14 29.05\n[30,]     29   28.99  30.01 30.65 29.88\n[31,]     30   29.99  30.82 31.34 30.72\n[32,]     31   30.99  31.63 31.76 31.55\n[33,]     32   31.99  32.43 32.35 32.39\n[34,]     33   32.99  33.24 33.09 33.22\n[35,]     34   33.99  34.05 33.88 34.06\n[36,]     35   34.99  34.86 34.86 34.90\n\n\nAlthough the equating methods vary, the results are similar to those of Tucker method. Especially Frequency Estimation method shows how important it is to have data in different score ranges because there is no meaningful equation before the scale score of 20 and all lower scores are equated to -.5 in this method. Let’s also see the plotting of the chart above:\n\n# Plot the results\nplot(Q1_tucker, Q1_nom, Q1_freq, Q1_braun, lty=c(1,2,3,4),\n     col=c(\"blue\", \"black\", \"red\", \"forestgreen\"), addident = FALSE)\n\n\n\n\nAs also can be seen in the plot above, after the scale score of 20, all equating methods are quite similar to each other. Scores lower than 20 are equated with linear methods much better than the equi-percentile method as there isn’t adequate data in those score ranges.\nThis study is conducted for demonstrative purposes and still we can say that scale scores over 30 can be equated in the given forms."
  },
  {
    "objectID": "posts2/Visualisation of My Personal Google Data (1)/index.html",
    "href": "posts2/Visualisation of My Personal Google Data (1)/index.html",
    "title": "Visualisation of My Personal Google Data (1): My Locations",
    "section": "",
    "text": "If you give permission to Google to store your location data, they will keep them in their databases forever. You can also allow them to store it for a while and then ask them delete it. They will directly do so.\nWhat makes this study a fun project is its being very personal. I decided to analyze my personal data in August, 2022. Therefore, I granted many new permissions to Google along with many previously granted permissions. They keep them in various formats including .csv, .json, .mbox etc. When you query for your personal data, they provide it within a couple of days depending on the size of the data you queried.\nUsually, I provide the readers with the data in my posts. However, in this series, the data are very personal and so I will not."
  },
  {
    "objectID": "posts2/Visualisation of My Personal Google Data (1)/index.html#introduction-my-locations",
    "href": "posts2/Visualisation of My Personal Google Data (1)/index.html#introduction-my-locations",
    "title": "Visualisation of My Personal Google Data (1): My Locations",
    "section": "Introduction: “My Locations”",
    "text": "Introduction: “My Locations”\nIn this part of the series, we will investigate my personal location data. We will visualize the spots I visited within a period of time. This way, I personally will gain insights about how boring my days are :)\nThe R packages that we use in this post are as follows: rjson, tidyr, dplyr, purrr, lubridate, sp and leaflet.\n\n##packs for data processing\nlibrary(rjson)      # to read .JSON files.\nlibrary(tidyr)      # to process data\nlibrary(dplyr)      # to process data\nlibrary(purrr)      # to process data\nlibrary(lubridate)  # to deal with date variables\n#packs for data viz\nlibrary(sp)         # a pack for spatial objects\nlibrary(leaflet)    # map and its functions"
  },
  {
    "objectID": "posts2/Visualisation of My Personal Google Data (1)/index.html#understand-the-data",
    "href": "posts2/Visualisation of My Personal Google Data (1)/index.html#understand-the-data",
    "title": "Visualisation of My Personal Google Data (1): My Locations",
    "section": "Understand the Data",
    "text": "Understand the Data\nInside the takeout folder that I received from Google, there is a folder named “Location History”. Inside it, “Semantic Location History” contains the location data based on the months and years. From that folder, I have called the locations I visited in November. Thus, we will use 2022_NOVEMBER.json file. Let’s investigate the data. Start with reading the file into R environment.\n\nmy_locations <- fromJSON(file = \"2022_NOVEMBER.json\")\n\nThen, let’s try to understand the structure of the data, how and what kind of information is stored into its cells. The list object my_locations contains many lists inside it. Let’s try to understand each one of them one by one:\n\nsummary(my_locations[[1]])\n\n      Length Class  Mode\n [1,] 1      -none- list\n [2,] 1      -none- list\n [3,] 1      -none- list\n [4,] 1      -none- list\n [5,] 1      -none- list\n [6,] 1      -none- list\n [7,] 1      -none- list\n [8,] 1      -none- list\n [9,] 1      -none- list\n[10,] 1      -none- list\n[11,] 1      -none- list\n[12,] 1      -none- list\n[13,] 1      -none- list\n[14,] 1      -none- list\n[15,] 1      -none- list\n[16,] 1      -none- list\n[17,] 1      -none- list\n\n\nThere are many smaller lists in the first indexed list. Let’s try the first one and see what’s inside:\n\nsummary(my_locations[[1]][[1]])\n\n           Length Class  Mode\nplaceVisit 11     -none- list\n\n\nThere is a single list inside. Sad :( Let’s dive one more step:\n\nsummary(my_locations[[1]][[1]][[1]])\n\n                        Length Class  Mode     \nlocation                8      -none- list     \nduration                2      -none- list     \nplaceConfidence         1      -none- character\ncenterLatE7             1      -none- numeric  \ncenterLngE7             1      -none- numeric  \nvisitConfidence         1      -none- numeric  \notherCandidateLocations 4      -none- list     \neditConfirmationStatus  1      -none- character\nlocationConfidence      1      -none- numeric  \nplaceVisitType          1      -none- character\nplaceVisitImportance    1      -none- character\n\n\nFinally, here we have several items. There is a list called location containing 8 items inside. There is duration with 2 items and otherCandidateLocations with 4 items. Other lists contain only one item each. Let’s check these one by one:\n\nsummary(my_locations[[1]][[1]][[1]]$location)\n\n                      Length Class  Mode     \nlatitudeE7            1      -none- numeric  \nlongitudeE7           1      -none- numeric  \nplaceId               1      -none- character\naddress               1      -none- character\nsemanticType          1      -none- character\nsourceInfo            1      -none- list     \nlocationConfidence    1      -none- numeric  \ncalibratedProbability 1      -none- numeric  \n\n\n\nsummary(my_locations[[1]][[1]][[1]]$duration)\n\n               Length Class  Mode     \nstartTimestamp 1      -none- character\nendTimestamp   1      -none- character\n\n\n\nsummary(my_locations[[1]][[1]][[1]]$otherCandidateLocations)\n\n     Length Class  Mode\n[1,] 7      -none- list\n[2,] 7      -none- list\n[3,] 7      -none- list\n[4,] 7      -none- list\n\n\nWe can obtain much information through this investigation process. For instance, inside the location I can see information about the latitude, longitude, address, the confidence that I have to this place, and some other. Here, if you are following along with me, please spare some time to understand your data. Delve into them and digest as much information as you can. I will see you in the next section: data processing."
  },
  {
    "objectID": "posts2/Visualisation of My Personal Google Data (1)/index.html#pre-processing",
    "href": "posts2/Visualisation of My Personal Google Data (1)/index.html#pre-processing",
    "title": "Visualisation of My Personal Google Data (1): My Locations",
    "section": "Pre-processing",
    "text": "Pre-processing\nYou can use as many items as you want in your work. You should decide the meaningful information while understanding your data. Now let’s re-define our lists as a dataframe.\n\ndf <- map_dfr(my_locations[[\"timelineObjects\"]], as.data.frame)\nView(df)\n# there is one empty row after each entry. Let's drop them through one of the complete columns:\ndf <- drop_na(df, placeVisit.location.latitudeE7)\n\nThere are many columns, some of which I won’t need. Especially, I am not interested in the locations defined as “candidate”. I will exclude them from my study. They are probably the locations that might be the place that I visited ordered by possibility. I just need the one with the highest possibility, which is tagged with placeVisit.location. . These locations are also defined as “HIGH CONFIDENCE”. Let’s continue the analysis with these locations, only.\nAlso, there are some columns with no entry. Let me exclude them with a function. Let the function be called not_all_na. This is a function that drops all the columns which are completely empty:\n\nnot_all_na <- function(x)\n  any(!is.na(x))\n#use the function on the dataframe:\ndf <- df %>% select(where(not_all_na))\n\nNow, I have a dataframe with 150+ columns. However, I just need the information about latitude, altitude, date and address of the locations that I visited. Let’s write a query to get this data into a new dataframe:\n\nlat <- select(df, contains(\"placeVisit.location.latitudeE7\"))\nlon <- select(df, contains(\"placeVisit.location.longitudeE7\"))\naddress <- select(df, contains(\"placeVisit.location.address\"))\ndate <- select(df, contains(\"placeVisit.duration.startTimestamp\"))\n\nThe chunks of code above ask for columns whose names contain the extensions written in quotation marks in them. Still, this raw information isn’t enough for several reasons. Firstly, lat and lot are coordinates in E7 format. With a quick research on the internet, I learned that they simply need to be divided by 10000000. Also, date contains day, month, year, hour, minute, second and time zone (which is in GMT+0 format) information all in the same column. They need to be handled. Let’s start with the second issue (the one about date):\n\n#re-name the only column:\nnames(date) <- \"Date\"\nhead(date)\n\n                      Date\n1 2022-11-06T13:12:38.091Z\n2 2022-11-06T13:24:32.338Z\n3 2022-11-06T13:59:00.539Z\n4 2022-11-06T14:17:15.462Z\n5 2022-11-06T14:39:29.138Z\n6 2022-11-07T05:32:14.277Z\n\n\nAs can be seen above, there are two separators: One is “T” separating day and time info. The other is “.” separating time and time zone info. Follow the notes in the code to grasp the process:\n\n#divide the day and hour info from the time zone info, then drop the time zone:\ndate <-\n  separate(\n    data = date,\n    col = Date,\n    into = c(\"Date\", \"zone\"),\n    sep = \"\\\\.\"\n  )\ndate <- date[-c(2)]\n\n#Now, transform the time in local time zone which is GMT+3:\ndate$Date<-as.POSIXct(date$Date, format=\"%Y-%m-%dT%H:%M:%S\", tz=Sys.timezone())+ hours(3)\n\n#divide the day and hour info:\ndate <-\n  separate(\n    data = date,\n    col = Date,\n    into = c(\"Day\", \"Hour\"),\n    sep = \" \"\n  )\n#see the new format:\nhead(date)\n\n         Day     Hour\n1 2022-11-06 16:12:38\n2 2022-11-06 16:24:32\n3 2022-11-06 16:59:00\n4 2022-11-06 17:17:15\n5 2022-11-06 17:39:29\n6 2022-11-07 08:32:14\n\n\nNicely done! Now gather all the information that we need into a dataframe. Again follow along the notes in the code:\n\ncoords <-\n  drop_na(data.frame(\n    lat = unlist(lat, use.names = FALSE) / 10000000, #divide lat and lon by 10000000 to get rid of the E7 format\n    lon = unlist(lon, use.names = FALSE) / 10000000, \n    address = unlist(address, use.names = FALSE),\n    date # we processed this before\n  ))\n\nSo far, we have worked to prepare for the data visualization process. Our data is ready with the name coords. Let’s continue with the visualization."
  },
  {
    "objectID": "posts2/Visualisation of My Personal Google Data (1)/index.html#data-visualization",
    "href": "posts2/Visualisation of My Personal Google Data (1)/index.html#data-visualization",
    "title": "Visualisation of My Personal Google Data (1): My Locations",
    "section": "Data Visualization",
    "text": "Data Visualization\nAt this point, we will visualize the locations I visited in November of 2022 on a world map. You can’t be as disappointed as me when you see that I live a life between home and work. Yet, the point here is to see the process of visualization. We owe this beautiful project to the R package leaflet. It is actually a javascript library, all its arguments are deployed into R environment too. Therefore, we can work with it. If you are still with me, I mhighly recommend you to read the documentation of the package leaflet. Then, follow along the notes in the code and try to understand it if you are not familiar with it.\n\ncoordinates(coords) <- ~ lon + lat\nleaflet(coords,\n\n# formating the outer of the map:\n        width = \"800px\",\n        height = \"400px\", \n        padding = 10) %>% \n  addTiles() %>%\n\n#formating the markers on the map:\n  addCircleMarkers(\n    color = \"tomato\", #my favorite colour\n    fillOpacity = 1,\n    radius = 7,\n    stroke = FALSE,\n    \n#address pops up when you click on a marker:\n    popup = coords$address,\n\n#the date and hour shows up with a fancy personal note when you hover on a marker:\n    label =  paste0(\"I have been around here on \", coords$Day, \" at around \", coords$Hour),\n\n#formating the label that shows up when you hover:\n    labelOptions = labelOptions(\n      noHide = F,\n      direction = \"top\",\n      style = list(\n        \"color\" = \"black\",\n        \"font-family\" = \"calibri\", #I love calibri\n        \"box-shadow\" = \"3px 3px rgba(0,0,0,0.25)\",\n        \"font-size\" = \"12px\",\n        \"border-color\" = \"rgba(0,0,0,0.5)\"\n      )\n    )\n  )"
  },
  {
    "objectID": "posts2/Visualisation of My Personal Google Data (1)/index.html#conclusion",
    "href": "posts2/Visualisation of My Personal Google Data (1)/index.html#conclusion",
    "title": "Visualisation of My Personal Google Data (1): My Locations",
    "section": "Conclusion",
    "text": "Conclusion\nWorking with your personal data gives you the opportunity to understand your own habits, likes, dislikes, and maybe future expectations. Here, you can only see my locations in November. When I worked on longer periods, I realized that I need to travel and see new places more often. Even if they are in my own city, a new place is a new vision of life.\nVisualizing data on spatial environments is a new challenge for me. Rather than graphs and charts, working with maps are more attractive obviously. While visualizing location data on maps, leaflet is an amazing, open source library. There are other options. One needs a mention here: ggmap. Yet, to use this package you need an API key obtained from Google. For more information about API keys, visit here. As of the package, you can visit the CRAN page of ggmap. Under the title “Google Maps API key”, you will see the procedure to buy a personal API key. It reads as follows:\nGOOGLE MAPS API KEY [@ggmap]\nA few years ago Google has changed its API requirements, and ggmap users are now required to register with Google. From a user’s perspective, there are essentially three ramifications of this:\n\nUsers must register with Google. You can do this at https://mapsplatform.google.com. While it will require a valid credit card (sorry!), there seems to be a fair bit of free use before you incur charges, and even then the charges are modest for light use.\nUsers must enable the APIs they intend to use. What may appear to ggmap users as one overarching “Google Maps” product, Google in fact has several services that it provides as geo-related solutions. For example, the Maps Static API provides map images, while the Geocoding API provides geocoding and reverse geocoding services. Apart from the relevant Terms of Service, generally ggmap users don’t need to think about the different services. For example, you just need to remember that get_googlemap() gets maps, geocode() geocodes (with Google, DSK is done), etc., and ggmap handles the queries for you. However, you do need to enable the APIs before you use them. You’ll only need to do that once, and then they’ll be ready for you to use. Enabling the APIs just means clicking a few radio buttons on the Google Maps Platform web interface listed above, so it’s easy.\nInside R, after loading the new version of ggmap, you’ll need provide ggmap with your API key, a hash value (think string of jibberish) that authenticates you to Google’s servers. This can be done on a temporary basis with register_google(key = \"[your key]\") or permanently using register_google(key = \"[your key]\", write = TRUE) (note: this will overwrite your ~/.Renviron file by replacing/adding the relevant line). If you use the former, know that you’ll need to re-do it every time you reset R.\n\nYour API key is private and unique to you, so be careful not to share it online, for example in a GitHub issue or saving it in a shared R script file. If you share it inadvertantly, just get on Google’s website and regenerate your key - this will retire the old one. Keeping your key private is made a bit easier by ggmap scrubbing the key out of queries by default, so when URLs are shown in your console, they’ll look something like key=xxx. (Read the details section of the register_google() documentation for a bit more info on this point.)\n\nStay tuned!\nThis series continues with the visualization of my Google Fit data. We will delve into my exercise habbits."
  },
  {
    "objectID": "posts2/Visualisation of My Personal Google Data (2)/index.html",
    "href": "posts2/Visualisation of My Personal Google Data (2)/index.html",
    "title": "[under development] Visualisation of My Personal Google Data (2): My exercise routine",
    "section": "",
    "text": "If you give permission to Google to store your location data, they will keep them in their databases forever. You can also allow them to store it for a while and then ask them delete it. They will directly do so.\nWhat makes this study a fun project is its being very personal. I decided to analyze my personal data in August, 2022. Therefore, I granted many new permissions to Google along with many previously granted permissions. They keep them in various formats including .csv, .json, .mbox etc. When you query for your personal data, they provide it within a couple of days depending on the size of the data you queried.\nUsually, I provide the readers with the data in my posts. However, in this series, the data are very personal and so I will not."
  },
  {
    "objectID": "posts2/Visualisation of My Personal Google Data (2)/index.html#introduction-my-exercise-routine",
    "href": "posts2/Visualisation of My Personal Google Data (2)/index.html#introduction-my-exercise-routine",
    "title": "[under development] Visualisation of My Personal Google Data (2): My exercise routine",
    "section": "Introduction: “My exercise routine”",
    "text": "Introduction: “My exercise routine”\nIn this part of the series, we will investigate my personal location data. We will visualize the spots I visited within a period of time. This way, I personally will gain insights about how boring my days are :)\nThe R packages that we use in this post are as follows:\n\nlibrary(\"psych\")\nlibrary(\"dplyr\")\nlibrary(\"plotly\")"
  },
  {
    "objectID": "posts2/Visualisation of My Personal Google Data (2)/index.html#understand-the-data-pre-processing",
    "href": "posts2/Visualisation of My Personal Google Data (2)/index.html#understand-the-data-pre-processing",
    "title": "[under development] Visualisation of My Personal Google Data (2): My exercise routine",
    "section": "Understand the Data & Pre-processing",
    "text": "Understand the Data & Pre-processing\n\ndaily_metrics<-read.csv(\"Daily activity metrics.csv\",sep=\",\", header = TRUE)\ndaily_metrics <- daily_metrics%>% mutate_if(is.numeric, round, digits=0)\ndaily_metrics <- subset(daily_metrics, select = -c(5,6,8,9,14, 13,12,11) )\nplot(daily_metrics)\n\n\n\n#sadece running olan günlere ait veri:\ndata_for_plotting<-daily_metrics[!is.na(daily_metrics$Running.duration..ms.),]\n\n\nfig<-plotly::plot_ly(data=data_for_plotting, type =\"scatter\", mode=\"lines+markers\",  \n                     y=data_for_plotting$Step.count, x=~Date)\nfig"
  },
  {
    "objectID": "posts2/Visualisation of My Personal Google Data (2)/index.html#data-visualization",
    "href": "posts2/Visualisation of My Personal Google Data (2)/index.html#data-visualization",
    "title": "[under development] Visualisation of My Personal Google Data (2): My exercise routine",
    "section": "Data Visualization",
    "text": "Data Visualization\n\n########################\n# walking VS runing duration bars\n#########################\nay<- list(\n  tickfont =list(color =\"red\"),\n  overlaying = \"y\",\n  side= \"right\",\n  title= \"<b> secondary</b> y axis\"\n)\n\n\nfig<-plotly::plot_ly()\nfig<- fig %>%\n  add_trace(type =\"bar\",  \n            y =data_for_plotting$Walking.duration..ms.,\n            x=data_for_plotting$Date, \n            name=\"walking duration (ms)\"\n  )\nfig<- fig %>% add_trace (type =\"scatter\", mode=\"lines+markers\" , #yaxis=\"y2\",\n                         y=data_for_plotting$Running.duration..ms., \n                         x=data_for_plotting$Date , \n                         name=\"running duration (ms)\")\n\n\nfig <- fig %>% layout(\n  title=\"two axis\",\n  yaxis2 = ay,\n  xaxis = list( title= \"x axis title\"),\n  yaxis = list( title= \"<b> primary</b> y axis\")\n)\nfig\n\n\n\n\n\n\n########################\n# Minutes of movement VS distance in meters\n#########################\nay<- list(\n  tickfont =list(color =\"red\"),\n  overlaying = \"y\",\n  side= \"right\",\n  title= \"<b> Distance I walked in meters </b>\"\n)\n\n\nfig<-plotly::plot_ly()\n\nfig<- fig %>%\n  add_trace(y =data_for_plotting$Move.Minutes.count,\n            x=data_for_plotting$Date, \n            name=\"movemnt(min.)\", \n            type =\"bar\" )\n\n\nfig<- fig %>% add_trace (y=data_for_plotting$Distance..m., \n                         x=data_for_plotting$Date , \n                         name=\"distance(m.)\", yaxis=\"y2\",\n                         type =\"scatter\", mode=\"lines+markers\")\n\nfig <- fig %>% layout(\n  title=\"My movement VS distance  <b>in 2022</b>\",\n  yaxis2 = ay,\n  xaxis = list( title= \"<b>Days</b>\"),\n  yaxis = list( title= \"<b> Time I moved in minutes </b> \")\n)\nfig\n\n\n\n\n\n\n########################\n# steps VS distance VS calories \n#########################\n\n\nfig<-plotly::plot_ly()\n\nfig<- fig %>%\n  add_trace(y =data_for_plotting$Step.count,\n            x=data_for_plotting$Date, \n            name=\"Steps \", \n            type =\"scatter\", mode=\"lines\")\n\nfig<- fig %>% add_trace (y=data_for_plotting$Distance..m., \n                         x=data_for_plotting$Date , \n                         name=\"distance\", \n                         type =\"bar\" )\n\n\nfig<- fig %>% add_trace (y=data_for_plotting$Calories..kcal., \n                         x=data_for_plotting$Date , \n                         name=\"calories\",\n                         type =\"scatter\", mode=\"lines\")\n\n\n\nfig <- fig %>% layout(\n  title=\" <b>My movement in 2022</b>\",\n  xaxis = list( title= \"<b>Days</b>\"),\n  yaxis = list( title= \"<b> steps(n)</b> / <b> distance(m)</b> / <b> calories(kcal) </b> \")\n)\nfig"
  },
  {
    "objectID": "posts2/Visualisation of My Personal Google Data (2)/index.html#conclusion",
    "href": "posts2/Visualisation of My Personal Google Data (2)/index.html#conclusion",
    "title": "[under development] Visualisation of My Personal Google Data (2): My exercise routine",
    "section": "Conclusion",
    "text": "Conclusion"
  }
]