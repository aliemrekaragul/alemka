[
  {
    "objectID": "posts/Web scraping with Rvest/index.html",
    "href": "posts/Web scraping with Rvest/index.html",
    "title": "Web scraping with Rvest package",
    "section": "",
    "text": "In this post, we will delve into harvesting a web page, Ekşi Sözlük. This process won’t include the automation of the process. Yet, a completed web application for harvesting Ekşi Sözlük is available.\nEkşi Sözlük is a reddit-like satirical web site where users share their ideas on certain topics in Turkish. Our target topic is “veri bilimi” (a.k.a. data science in English). The R packages that we use in this post are as follows:\n\nlibrary(rvest)\nlibrary(dplyr)\nlibrary(tm)\nlibrary(stopwords)\nlibrary(wordcloud)\nlibrary(wordcloud2)"
  },
  {
    "objectID": "posts/Web scraping with Rvest/index.html#introduction",
    "href": "posts/Web scraping with Rvest/index.html#introduction",
    "title": "Web scraping with Rvest package",
    "section": "",
    "text": "In this post, we will delve into harvesting a web page, Ekşi Sözlük. This process won’t include the automation of the process. Yet, a completed web application for harvesting Ekşi Sözlük is available.\nEkşi Sözlük is a reddit-like satirical web site where users share their ideas on certain topics in Turkish. Our target topic is “veri bilimi” (a.k.a. data science in English). The R packages that we use in this post are as follows:\n\nlibrary(rvest)\nlibrary(dplyr)\nlibrary(tm)\nlibrary(stopwords)\nlibrary(wordcloud)\nlibrary(wordcloud2)"
  },
  {
    "objectID": "posts/Web scraping with Rvest/index.html#processing",
    "href": "posts/Web scraping with Rvest/index.html#processing",
    "title": "Web scraping with Rvest package",
    "section": "Processing",
    "text": "Processing\n“Rvest” is a package for web scraping and harvesting which was developed by Hadley Wickham. He has stated that he was inspired by a few of the python libraries such as “beautiful soup” and “RoboBrowser”.\nFirst things first; we start by introducing the webpage that we want to harvest.\n\nhtml &lt;- read_html(\"https://eksisozluk1923.com/veri-bilimi--3426406\")\n\nRvest allows us to collect any type of HTML tag from the current page. Let’s suppose we would like to collect all the links in a topic page. First, go to the page and press F12 to open the “devtools” tab. Then turn inspect mode on. Hover on an item and check the node name for a random link. This process is shown below.\n\n\n\n\n\nThen we use the functions html_nodes() on the node name and html_attr() on “href” HTML tags. This will get all the href attributes with the node name “a.url” on the given link and turn them into a list:\n\nlinks &lt;- html %&gt;% html_nodes(\"a.url\")  %&gt;%  html_attr(\"href\")\nlinks\n\n[1] \"http://e-k.in/introduction-to-data-science/\"                                                     \n[2] \"https://www.edx.org/course/introduction-big-data-apache-spark-uc-berkeleyx-cs100-1x#.VMK0q3v-t3U\"\n\n\nIf we would like to collect all the entries on the given page, we will need the div attribute name for entries (also shown below).\n\nNow, we will use the function html_text() to collect the entries in these div attributes. There are about 10 entries some of which are quite long. Therefore, let me print only the first three entries via the head() function.\n\nentries &lt;- html %&gt;% html_nodes(\".content\") %&gt;%html_text()    \n\n#Let's see the first three entries:\nhead(entries, 3)\n\n[1] \"\\r\\n    internette milyonlarca veri var. bu milyonlarca veriyi işleyen milyonlarca uygulama var. işte bu uygulamalar sayesinde var olan veriyi kullanıp yeni veri uygulamaları yaratma işine veri bilimi denir.\\r\\n  \"\n[2] \"\\r\\n    istatistik ile cok alakali bilim. (bkz: veri madenciligi)\\r\\n  \"                                                                                                                                              \n[3] \"\\r\\n    bana veri olmadan çalışan bir adet bilim türü gösterildiği anda varlığını ve özgünlüğünü kabul edeceğim ilim kategorisi.yoksa şununla yarışır bence:(bkz: deney bilimi) *\"                                    \n\n\nNow that you have all the entries in a page, it is easy to carry out a text analysis with it. Let’s simply create a word cloud. Here we’ll use the wordcloud and tm packages. Functions in tm package works on a corpus type input. Hence, we change our entry list to a corpus using Corpus() and VectorSource() functions.\n\n#turn entries into corpus\n\nentries&lt;-Corpus(VectorSource(entries))\n\nAs we do not want unnecessary items in our word cloud, we will get rid of numbers, punctuation marks and spaces with removeNumbers(), removePunctuation(), and stripWhitespace().\nAlso all letters will be transformed into lower case characters. We use tm_map() function to apply all the above mentioned functions on the corpus items.\n\n#apply several functions such as remove punctuation or numbers etc.\n\nentries &lt;- entries %&gt;%\n  tm_map(removeNumbers) %&gt;%\n  tm_map(removePunctuation) %&gt;%\n  tm_map(stripWhitespace)\nentries &lt;- tm_map(entries, content_transformer(tolower))\n\nOf course we would like to remove all the words that do not contribute to the context such as linkers, conjunctions, articles etc. For that, we can use the package stopwords. Turkish stopwords are defined in it and if you like you can add some other words to remove from your corpus via the append() function. We need to unlist() our corpus to apply removeWords() on it.\n\ntr_stopwords&lt;-stopwords::stopwords(\"tr\", source = \"stopwords-iso\")    \ntr_stopwords&lt;-append(tr_stopwords, c(\"bkz\",\"var\",\"vardir\",\"icin\", \"iste\", \"işte\",\"rn\",\"crn\"))\nentries&lt;-unlist(entries)\nentries &lt;- removeWords(entries, words=tr_stopwords)\n\nActually, right now our corpus is ready, filled with all the meaningful words in the entries. However, to create a word cloud we need it as a frequency table. That’s because a word’s frequency will be used while determining its colour and size. To get a frequency table, we need a term matrix first. Following lines of codes will do the magic in this regard:\n\n#turn into a matrix\nterm_matrix &lt;- as.matrix(TermDocumentMatrix(entries) )\n\n#frequency table:\nword_freqs &lt;- sort(rowSums(term_matrix),decreasing=TRUE) \nword_freqs &lt;- data.frame(word=names(word_freqs),freq=word_freqs )\n\nWe will use two different packages for creating our word clouds. Initially, the classic word cloud takes the column names in the frequency table as words and freq arguments. Other arguments are also defined next to them as notes:\n\nwordcloud::wordcloud(words = word_freqs$word, \n                     freq = word_freqs$freq, \n                     min.freq = 1,  ## freqs lower than that wonW't be in the cloud\n                     max.words=200, ## Total number of words in the cloud\n                     random.order=FALSE,  ## words will be ordered according to their freqs.\n                     rot.per=0.35, ## rotation degree\n                     colors=brewer.pal(8, \"Dark2\")) ## some custimization for colors\n\n\n\n\nThe second option is a more modern alternative. It is developed based on the JS framework with the same name. It gives you the opportunity to hover over the words and see their frequencies. Also, you can decide on a shape among several options such as 'circle', 'diamond', 'triangle-forward', 'triangle', 'pentagon', and 'star'. Yet, some of these features won’t be available if you want your cloud in .png format. That is because wordcloud2 package provides us with clouds in HTML format which makes it easier for web page embedding but harder for saving on your local disk as .png or any other format.\n\nwordcloud2::wordcloud2 (word_freqs, \n                  size = 0.4,  ## this is basically zooming in the cloud. default is 1\n                  shape = \"star\", \n                  color='random-light', ## another option is 'random-dark'\n                  backgroundColor=\"black\" ) ## another option is 'white'"
  },
  {
    "objectID": "posts/Web scraping with Rvest/index.html#conclusion",
    "href": "posts/Web scraping with Rvest/index.html#conclusion",
    "title": "Web scraping with Rvest package",
    "section": "Conclusion",
    "text": "Conclusion\nThere are many ways to use Rvest and scrap a website. We tried one of them in this post. In the end you can do many things with your output such as content analysis, clustering, sentimental analysis or even survival analysis. Here, in this post, we do not bare any pragmatic purposes so focused on creating something fun and eye-catching: word clouds. Also, scraping depends highly on your computer’s processor and the number of pages you want to scrap. If you want to automate this process, it may take some time to complete. You can find my Rshiny web application automating specifically the steps discussed here.\nAlthough we haven’t mentioned here, Rvest works in alignment with the polite package. It allows you to scrap a web page in a polite way with permissions and greetings. The motto of the package polite sums it all for itself: Be responsible when scraping data from websites by following polite principles: introduce yourself, ask for permission, take slowly and never ask twice."
  },
  {
    "objectID": "posts/Visualisation of My Personal Google Data (1)/index.html",
    "href": "posts/Visualisation of My Personal Google Data (1)/index.html",
    "title": "Visualisation of My Personal Google Data (1): My Locations",
    "section": "",
    "text": "If you give permission to Google to store your location data, they will keep them in their databases forever. You can also allow them to store it for a while and then ask them delete it. They will directly do so.\nWhat makes this study a fun project is its being very personal. I decided to analyze my personal data in August, 2022. Therefore, I granted many new permissions to Google along with many previously granted permissions. They keep them in various formats including .csv, .json, .mbox etc. When you query for your personal data, they provide it within a couple of days depending on the size of the data you queried.\nUsually, I provide the readers with the data in my posts. However, in this series, the data are very personal and so I will not."
  },
  {
    "objectID": "posts/Visualisation of My Personal Google Data (1)/index.html#introduction-to-the-series-visualisation-of-my-personal-google-data",
    "href": "posts/Visualisation of My Personal Google Data (1)/index.html#introduction-to-the-series-visualisation-of-my-personal-google-data",
    "title": "Visualisation of My Personal Google Data (1): My Locations",
    "section": "",
    "text": "If you give permission to Google to store your location data, they will keep them in their databases forever. You can also allow them to store it for a while and then ask them delete it. They will directly do so.\nWhat makes this study a fun project is its being very personal. I decided to analyze my personal data in August, 2022. Therefore, I granted many new permissions to Google along with many previously granted permissions. They keep them in various formats including .csv, .json, .mbox etc. When you query for your personal data, they provide it within a couple of days depending on the size of the data you queried.\nUsually, I provide the readers with the data in my posts. However, in this series, the data are very personal and so I will not."
  },
  {
    "objectID": "posts/Visualisation of My Personal Google Data (1)/index.html#introduction-my-locations",
    "href": "posts/Visualisation of My Personal Google Data (1)/index.html#introduction-my-locations",
    "title": "Visualisation of My Personal Google Data (1): My Locations",
    "section": "Introduction: “My Locations”",
    "text": "Introduction: “My Locations”\nIn this part of the series, we will investigate my personal location data. We will visualize the spots I visited within a period of time. This way, I personally will gain insights about how boring my days are :)\nThe R packages that we use in this post are as follows: rjson, tidyr, dplyr, purrr, lubridate, sp and leaflet.\n\n\nCode\n##packs for data processing\nlibrary(rjson)      # to read .JSON files.\nlibrary(tidyr)      # to process data\nlibrary(dplyr)      # to process data\nlibrary(purrr)      # to process data\nlibrary(lubridate)  # to deal with date variables\n#packs for data viz\nlibrary(sp)         # a pack for spatial objects\nlibrary(leaflet)    # map and its functions"
  },
  {
    "objectID": "posts/Visualisation of My Personal Google Data (1)/index.html#understand-the-data",
    "href": "posts/Visualisation of My Personal Google Data (1)/index.html#understand-the-data",
    "title": "Visualisation of My Personal Google Data (1): My Locations",
    "section": "Understand the Data",
    "text": "Understand the Data\nInside the takeout folder that I received from Google, there is a folder named “Location History”. Inside it, “Semantic Location History” contains the location data based on the months and years. From that folder, I have called the locations I visited in November. Thus, we will use 2022_NOVEMBER.json file. Let’s investigate the data. Start with reading the file into R environment.\n\nmy_locations &lt;- fromJSON(file = \"2022_NOVEMBER.json\")\n\nThen, let’s try to understand the structure of the data, how and what kind of information is stored into its cells. The list object my_locations contains many lists inside it. Let’s try to understand each one of them one by one:\n\nsummary(my_locations[[1]])\n\n      Length Class  Mode\n [1,] 1      -none- list\n [2,] 1      -none- list\n [3,] 1      -none- list\n [4,] 1      -none- list\n [5,] 1      -none- list\n [6,] 1      -none- list\n [7,] 1      -none- list\n [8,] 1      -none- list\n [9,] 1      -none- list\n[10,] 1      -none- list\n[11,] 1      -none- list\n[12,] 1      -none- list\n[13,] 1      -none- list\n[14,] 1      -none- list\n[15,] 1      -none- list\n[16,] 1      -none- list\n[17,] 1      -none- list\n\n\nThere are many smaller lists in the first indexed list. Let’s try the first one and see what’s inside:\n\nsummary(my_locations[[1]][[1]])\n\n           Length Class  Mode\nplaceVisit 11     -none- list\n\n\nThere is a single list inside. Sad :( Let’s dive one more step:\n\nsummary(my_locations[[1]][[1]][[1]])\n\n                        Length Class  Mode     \nlocation                8      -none- list     \nduration                2      -none- list     \nplaceConfidence         1      -none- character\ncenterLatE7             1      -none- numeric  \ncenterLngE7             1      -none- numeric  \nvisitConfidence         1      -none- numeric  \notherCandidateLocations 4      -none- list     \neditConfirmationStatus  1      -none- character\nlocationConfidence      1      -none- numeric  \nplaceVisitType          1      -none- character\nplaceVisitImportance    1      -none- character\n\n\nFinally, here we have several items. There is a list called location containing 8 items inside. There is duration with 2 items and otherCandidateLocations with 4 items. Other lists contain only one item each. Let’s check these one by one:\n\nsummary(my_locations[[1]][[1]][[1]]$location)\n\n                      Length Class  Mode     \nlatitudeE7            1      -none- numeric  \nlongitudeE7           1      -none- numeric  \nplaceId               1      -none- character\naddress               1      -none- character\nsemanticType          1      -none- character\nsourceInfo            1      -none- list     \nlocationConfidence    1      -none- numeric  \ncalibratedProbability 1      -none- numeric  \n\n\n\nsummary(my_locations[[1]][[1]][[1]]$duration)\n\n               Length Class  Mode     \nstartTimestamp 1      -none- character\nendTimestamp   1      -none- character\n\n\n\nsummary(my_locations[[1]][[1]][[1]]$otherCandidateLocations)\n\n     Length Class  Mode\n[1,] 7      -none- list\n[2,] 7      -none- list\n[3,] 7      -none- list\n[4,] 7      -none- list\n\n\nWe can obtain much information through this investigation process. For instance, inside the location I can see information about the latitude, longitude, address, the confidence that I have to this place, and some other. Here, if you are following along with me, please spare some time to understand your data. Delve into them and digest as much information as you can. I will see you in the next section: data processing."
  },
  {
    "objectID": "posts/Visualisation of My Personal Google Data (1)/index.html#pre-processing",
    "href": "posts/Visualisation of My Personal Google Data (1)/index.html#pre-processing",
    "title": "Visualisation of My Personal Google Data (1): My Locations",
    "section": "Pre-processing",
    "text": "Pre-processing\nYou can use as many items as you want in your work. You should decide the meaningful information while understanding your data. Now let’s re-define our lists as a dataframe.\n\ndf &lt;- map_dfr(my_locations[[\"timelineObjects\"]], as.data.frame)\nView(df)\n# there is one empty row after each entry. Let's drop them through one of the complete columns:\ndf &lt;- drop_na(df, placeVisit.location.latitudeE7)\n\nThere are many columns, some of which I won’t need. Especially, I am not interested in the locations defined as “candidate”. I will exclude them from my study. They are probably the locations that might be the place that I visited ordered by possibility. I just need the one with the highest possibility, which is tagged with placeVisit.location. . These locations are also defined as “HIGH CONFIDENCE”. Let’s continue the analysis with these locations, only.\nAlso, there are some columns with no entry. Let me exclude them with a function. Let the function be called not_all_na. This is a function that drops all the columns which are completely empty:\n\nnot_all_na &lt;- function(x)\n  any(!is.na(x))\n#use the function on the dataframe:\ndf &lt;- df %&gt;% select(where(not_all_na))\n\nNow, I have a dataframe with 150+ columns. However, I just need the information about latitude, altitude, date and address of the locations that I visited. Let’s write a query to get this data into a new dataframe:\n\nlat &lt;- select(df, contains(\"placeVisit.location.latitudeE7\"))\nlon &lt;- select(df, contains(\"placeVisit.location.longitudeE7\"))\naddress &lt;- select(df, contains(\"placeVisit.location.address\"))\ndate &lt;- select(df, contains(\"placeVisit.duration.startTimestamp\"))\n\nThe chunks of code above ask for columns whose names contain the extensions written in quotation marks in them. Still, this raw information isn’t enough for several reasons. Firstly, lat and lot are coordinates in E7 format. With a quick research on the internet, I learned that they simply need to be divided by 10000000. Also, date contains day, month, year, hour, minute, second and time zone (which is in GMT+0 format) information all in the same column. They need to be handled. Let’s start with the second issue (the one about date):\n\n#re-name the only column:\nnames(date) &lt;- \"Date\"\nhead(date)\n\n                      Date\n1 2022-11-06T13:12:38.091Z\n2 2022-11-06T13:24:32.338Z\n3 2022-11-06T13:59:00.539Z\n4 2022-11-06T14:17:15.462Z\n5 2022-11-06T14:39:29.138Z\n6 2022-11-07T05:32:14.277Z\n\n\nAs can be seen above, there are two separators: One is “T” separating day and time info. The other is “.” separating time and time zone info. Follow the notes in the code to grasp the process:\n\n#divide the day and hour info from the time zone info, then drop the time zone:\ndate &lt;-\n  separate(\n    data = date,\n    col = Date,\n    into = c(\"Date\", \"zone\"),\n    sep = \"\\\\.\"\n  )\ndate &lt;- date[-c(2)]\n\n#Now, transform the time in local time zone which is GMT+3:\ndate$Date&lt;-as.POSIXct(date$Date, format=\"%Y-%m-%dT%H:%M:%S\", tz=Sys.timezone())+ hours(3)\n\n#divide the day and hour info:\ndate &lt;-\n  separate(\n    data = date,\n    col = Date,\n    into = c(\"Day\", \"Hour\"),\n    sep = \" \"\n  )\n#see the new format:\nhead(date)\n\n         Day     Hour\n1 2022-11-06 16:12:38\n2 2022-11-06 16:24:32\n3 2022-11-06 16:59:00\n4 2022-11-06 17:17:15\n5 2022-11-06 17:39:29\n6 2022-11-07 08:32:14\n\n\nNicely done! Now gather all the information that we need into a dataframe. Again follow along the notes in the code:\n\ncoords &lt;-\n  drop_na(data.frame(\n    lat = unlist(lat, use.names = FALSE) / 10000000, #divide lat and lon by 10000000 to get rid of the E7 format\n    lon = unlist(lon, use.names = FALSE) / 10000000, \n    address = unlist(address, use.names = FALSE),\n    date # we processed this before\n  ))\n\nSo far, we have worked to prepare for the data visualization process. Our data is ready with the name coords. Let’s continue with the visualization."
  },
  {
    "objectID": "posts/Visualisation of My Personal Google Data (1)/index.html#data-visualization",
    "href": "posts/Visualisation of My Personal Google Data (1)/index.html#data-visualization",
    "title": "Visualisation of My Personal Google Data (1): My Locations",
    "section": "Data Visualization",
    "text": "Data Visualization\nAt this point, we will visualize the locations I visited in November of 2022 on a world map. You can’t be as disappointed as me when you see that I live a life between home and work. Yet, the point here is to see the process of visualization. We owe this beautiful project to the R package leaflet. It is actually a javascript library, all its arguments are deployed into R environment too. Therefore, we can work with it. If you are still with me, I mhighly recommend you to read the documentation of the package leaflet. Then, follow along the notes in the code and try to understand it if you are not familiar with it.\n\ncoordinates(coords) &lt;- ~ lon + lat\nleaflet(coords,\n\n# formating the outer of the map:\n        width = \"800px\",\n        height = \"400px\", \n        padding = 10) %&gt;% \n  addTiles() %&gt;%\n\n#formating the markers on the map:\n  addCircleMarkers(\n    color = \"tomato\", #my favorite colour\n    fillOpacity = 1,\n    radius = 7,\n    stroke = FALSE,\n    \n#address pops up when you click on a marker:\n    popup = coords$address,\n\n#the date and hour shows up with a fancy personal note when you hover on a marker:\n    label =  paste0(\"I have been around here on \", coords$Day, \" at around \", coords$Hour),\n\n#formating the label that shows up when you hover:\n    labelOptions = labelOptions(\n      noHide = F,\n      direction = \"top\",\n      style = list(\n        \"color\" = \"black\",\n        \"font-family\" = \"calibri\", #I love calibri\n        \"box-shadow\" = \"3px 3px rgba(0,0,0,0.25)\",\n        \"font-size\" = \"12px\",\n        \"border-color\" = \"rgba(0,0,0,0.5)\"\n      )\n    )\n  )"
  },
  {
    "objectID": "posts/Visualisation of My Personal Google Data (1)/index.html#conclusion",
    "href": "posts/Visualisation of My Personal Google Data (1)/index.html#conclusion",
    "title": "Visualisation of My Personal Google Data (1): My Locations",
    "section": "Conclusion",
    "text": "Conclusion\nWorking with your personal data gives you the opportunity to understand your own habits, likes, dislikes, and maybe future expectations. Here, you can only see my locations in November. When I worked on longer periods, I realized that I need to travel and see new places more often. Even if they are in my own city, a new place is a new vision of life.\nVisualizing data on spatial environments is a new challenge for me. Rather than graphs and charts, working with maps are more attractive obviously. While visualizing location data on maps, leaflet is an amazing, open source library. There are other options. One needs a mention here: ggmap. Yet, to use this package you need an API key obtained from Google. For more information about API keys, visit here. As of the package, you can visit the CRAN page of ggmap. Under the title “Google Maps API key”, you will see the procedure to buy a personal API key. It reads as follows:\nGOOGLE MAPS API KEY [@ggmap]\nA few years ago Google has changed its API requirements, and ggmap users are now required to register with Google. From a user’s perspective, there are essentially three ramifications of this:\n\nUsers must register with Google. You can do this at https://mapsplatform.google.com. While it will require a valid credit card (sorry!), there seems to be a fair bit of free use before you incur charges, and even then the charges are modest for light use.\nUsers must enable the APIs they intend to use. What may appear to ggmap users as one overarching “Google Maps” product, Google in fact has several services that it provides as geo-related solutions. For example, the Maps Static API provides map images, while the Geocoding API provides geocoding and reverse geocoding services. Apart from the relevant Terms of Service, generally ggmap users don’t need to think about the different services. For example, you just need to remember that get_googlemap() gets maps, geocode() geocodes (with Google, DSK is done), etc., and ggmap handles the queries for you. However, you do need to enable the APIs before you use them. You’ll only need to do that once, and then they’ll be ready for you to use. Enabling the APIs just means clicking a few radio buttons on the Google Maps Platform web interface listed above, so it’s easy.\nInside R, after loading the new version of ggmap, you’ll need provide ggmap with your API key, a hash value (think string of jibberish) that authenticates you to Google’s servers. This can be done on a temporary basis with register_google(key = \"[your key]\") or permanently using register_google(key = \"[your key]\", write = TRUE) (note: this will overwrite your ~/.Renviron file by replacing/adding the relevant line). If you use the former, know that you’ll need to re-do it every time you reset R.\n\nYour API key is private and unique to you, so be careful not to share it online, for example in a GitHub issue or saving it in a shared R script file. If you share it inadvertantly, just get on Google’s website and regenerate your key - this will retire the old one. Keeping your key private is made a bit easier by ggmap scrubbing the key out of queries by default, so when URLs are shown in your console, they’ll look something like key=xxx. (Read the details section of the register_google() documentation for a bit more info on this point.)\n\nStay tuned!\nThis series continues with the visualization of my Google Fit data. We will delve into my exercise habbits."
  },
  {
    "objectID": "posts/Test Equating/index.html",
    "href": "posts/Test Equating/index.html",
    "title": "Test Equating",
    "section": "",
    "text": "This is a simple test equating study. The data used in this study is simulated from real data. We don’t use the real data for privacy purposes here.\n2020-2021 Fall Term A Level’s first quiz has 40 items. 2022-2023 Fall Term A Level’s first quiz has 40 items. 35 of the items in each test forms are unique items while 5 of them are common, thus will be called as “anchor items” in this study. For the readers interest, the items belong to four main domains (listening, structure, vocabbulary and reading), yet the common items are only in the reading section. This is obviously a violation of assumptions of test equating. Still, this study is conveyed for demonstration purposes. Therefore, let’s continue:\nTo ensure statistical equation of these two forms, we first introduced the data in R Studio and the first five rows can be seen below:\n\n\nCode\nQ1 &lt;- read.csv(\"kitap1.csv\", header = TRUE)\nhead(Q1)\n\n\n  L1 L2 L3 L4 L5 L6 L7 L8 L9 L10 S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 V1 V2 V3 V4 V5\n1  0  1  0  1  1  1  0  1  1   0  1  1  0  1  0  0  0  0  0   1  0  1  1  0  1\n2  1  1  1  1  1  1  1  1  1   1  0  1  1  0  1  1  0  0  0   0  1  1  1  1  0\n3  1  1  1  1  1  1  0  1  1   0  1  1  0  1  1  0  0  1  1   0  0  1  0  1  0\n4  1  1  1  1  1  1  1  0  1   0  1  1  0  1  1  0  1  1  0   0  0  1  1  0  1\n5  1  1  1  1  1  1  1  0  1   1  1  1  1  1  0  1  0  1  0   0  1  1  1  0  0\n6  1  1  1  1  0  0  1  1  1   1  0  0  0  0  0  1  1  1  1   1  1  1  1  0  0\n  V6 V7 V8 V9 V10 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 form\n1  0  1  1  1   1  1  1  0  1  1  1  1  1  1   1    x\n2  0  1  1  0   0  1  0  1  1  0  1  1  0  1   1    x\n3  1  0  1  1   1  1  0  0  1  1  1  0  1  1   1    x\n4  1  1  1  0   1  1  1  1  1  0  1  0  0  1   1    x\n5  1  1  1  0   1  1  1  1  0  1  1  1  0  1   1    x\n6  1  1  1  1   1  1  1  1  1  1  1  1  1  1   1    x\n\n\nLater, we introduced the unique and anchor items separately. First 35 items are unique items, and the last 5 items are anchor items.\n\n\nCode\n# Calculate total scores based on unique items\nQ1$total &lt;- rowSums(Q1[, 1:35])\n\n# Calculate scores based on anchor items\nQ1$anchor &lt;- rowSums(Q1[, 36:40])\n\n\nAs we will use the equate package, the data should be contained as frequency tables: Form x (20-21 fall) had a sample of 200 while form y (22-23 fall) had a sample of 133 students. They are defined as:\n\n\nCode\n#first introduce the equate package:\nlibrary(equate)\n# Create frequency tables (total score range: 0-35; anchor score range: 0-5)\nQ1_x &lt;- freqtab(Q1[1:200, c(\"total\", \"anchor\")], scales = list(0:35, 0:5))\nQ1_y &lt;- freqtab(Q1[201:334, c(\"total\", \"anchor\")], scales = list(0:35, 0:5))\n\n\nTo consideration of the reader one more time, we must state that these forms are the first quizzes of the students. They usually get high marks. For instance, you can see below that the students’ scores are distributed left-skewed in both forms. Their total correct answers are distributed between 20 and 35 for the unique items , and between 3 and 5 for anchor items in form X. The situation isn’t different for form y.\n\n\nCode\n#distrubution of the data among forms and unique/common items\nplot(Q1_x, xlab = \"Total Scores Form X\", ylab = \"Common Anchor Scores Form X\")\n\n\n\n\n\nCode\nplot(Q1_y, xlab = \"Total Scores Form y\", ylab = \"Common Anchor Scores Form y\")\n\n\n\n\n\nStill, let’s continue… with the smoothing procedure. For both forms, we utilized loglinear presmoothing. Of course there are several other methods, yet literature shows not much a big difference between them, thus not much care given to this issue. again as this is a study with demonstration purposes. After the smoothing, it can be realized that the distribution is highly eye-pleasing right now. Also it is much easier to match the scores even if there isn’t an equivalent of it in the other form.\n\n\nCode\n#PRESMOOTHING\nsmooth_x &lt;- presmoothing(Q1_x, smoothmethod = \"loglinear\")\n\n\nWarning: glm.fit: fitted rates numerically 0 occurred\n\n\nCode\nsmooth_y &lt;- presmoothing(Q1_y, smoothmethod = \"loglinear\")\n\n\nWarning: glm.fit: algorithm did not converge\n\nWarning: glm.fit: fitted rates numerically 0 occurred\n\n\nCode\nplot(smooth_x, xlab = \"Total Scores Form X\", ylab = \"Common Anchor Scores Form X\")\n\n\n\n\n\nCode\nplot(smooth_y, xlab = \"Total Scores Form y\", ylab = \"Common Anchor Scores Form y\")\n\n\n\n\n\nNow, it can be roughly said that the forms are ready to be equated. Before we try several methods, lets see the results of the Tucker method as it can produce equating error as well:\n\n\nCode\n## Linear Tucker Equating\nQ1_tucker &lt;- equate(Q1_x, Q1_y, type = \"linear\", method = \"tucker\")\nQ1_tucker$concordance\n\n\n   scale        yx      se.n      se.g\n1      0  6.597617 2.0378528 3.3547355\n2      1  7.404998 1.9751403 3.2551073\n3      2  8.212379 1.9124541 3.1554875\n4      3  9.019759 1.8497970 3.0558770\n5      4  9.827140 1.7871721 2.9562768\n6      5 10.634520 1.7245827 2.8566879\n7      6 11.441901 1.6620331 2.7571116\n8      7 12.249282 1.5995277 2.6575492\n9      8 13.056662 1.5370720 2.5580024\n10     9 13.864043 1.4746724 2.4584730\n11    10 14.671423 1.4123363 2.3589634\n12    11 15.478804 1.3500723 2.2594761\n13    12 16.286185 1.2878911 2.1600141\n14    13 17.093565 1.2258052 2.0605812\n15    14 17.900946 1.1638299 1.9611818\n16    15 18.708326 1.1019838 1.8618212\n17    16 19.515707 1.0402899 1.7625060\n18    17 20.323088 0.9787772 1.6632444\n19    18 21.130468 0.9174818 1.5640464\n20    19 21.937849 0.8564507 1.4649252\n21    20 22.745229 0.7957445 1.3658973\n22    21 23.552610 0.7354438 1.2669847\n23    22 24.359991 0.6756570 1.1682166\n24    23 25.167371 0.6165338 1.0696330\n25    24 25.974752 0.5582849 0.9712903\n26    25 26.782132 0.5012154 0.8732696\n27    26 27.589513 0.4457784 0.7756933\n28    27 28.396894 0.3926659 0.6787528\n29    28 29.204274 0.3429596 0.5827655\n30    29 30.011655 0.2983669 0.4882941\n31    30 30.819035 0.2615166 0.3964236\n32    31 31.626416 0.2360629 0.3094791\n33    32 32.433797 0.2258919 0.2330408\n34    33 33.241177 0.2330134 0.1809526\n35    34 34.048558 0.2559883 0.1763087\n36    35 34.855938 0.2910866 0.2221053\n\n\nYou will see that the equating errors are above 1 before the score of 25 as there isn’t much data in the low scores. Also, as we investigate the lower marks, we see that the gap between equated scores are increasing. For instance, 0 on form X is equal to 6.597617 on form Y. This is because there isn’t data in these regions of the scores. Despite that, equated scores get more meaningful after 20. Especially after the total score 30, the equated scores are too close and the equation error is too low, which would be quite better if the situation was like that on all total score ranges. Let’s see some other equating methods:\n\n\nCode\n## Comparing Multiple Methods\n# Nominal method with mean equating\nQ1_nom &lt;- equate(Q1_x, Q1_y, type = \"mean\", method = \"nom\")\n\n# Frequency method with equipercentile\nQ1_freq &lt;- equate(Q1_x, Q1_y, type = \"equip\", method = \"freq\")\n\n# Braun method with linear equating\nQ1_braun &lt;- equate(Q1_x, Q1_y, type = \"linear\", method = \"braun\")\n\n# Compare equated scores\nround(cbind(xscale = 0:35, \n            nominal = Q1_nom$concordance$yx,\n            tucker = Q1_tucker$concordance$yx, \n            freq = Q1_freq$concordance$yx, \n            braun = Q1_braun$concordance$yx), 2)\n\n\n      xscale nominal tucker  freq braun\n [1,]      0   -0.01   6.60 -0.50  5.65\n [2,]      1    0.99   7.40 -0.50  6.49\n [3,]      2    1.99   8.21 -0.50  7.32\n [4,]      3    2.99   9.02 -0.50  8.16\n [5,]      4    3.99   9.83 -0.50  9.00\n [6,]      5    4.99  10.63 -0.50  9.83\n [7,]      6    5.99  11.44 -0.50 10.67\n [8,]      7    6.99  12.25 -0.50 11.50\n [9,]      8    7.99  13.06 -0.50 12.34\n[10,]      9    8.99  13.86 -0.50 13.17\n[11,]     10    9.99  14.67 -0.50 14.01\n[12,]     11   10.99  15.48 -0.50 14.84\n[13,]     12   11.99  16.29 -0.50 15.68\n[14,]     13   12.99  17.09 -0.50 16.51\n[15,]     14   13.99  17.90 -0.50 17.35\n[16,]     15   14.99  18.71 -0.50 18.19\n[17,]     16   15.99  19.52 -0.50 19.02\n[18,]     17   16.99  20.32 -0.50 19.86\n[19,]     18   17.99  21.13 -0.50 20.69\n[20,]     19   18.99  21.94 -0.50 21.53\n[21,]     20   19.99  22.75 -0.50 22.36\n[22,]     21   20.99  23.55 23.66 23.20\n[23,]     22   21.99  24.36 23.82 24.03\n[24,]     23   22.99  25.17 24.10 24.87\n[25,]     24   23.99  25.97 24.38 25.71\n[26,]     25   24.99  26.78 24.57 26.54\n[27,]     26   25.99  27.59 25.35 27.38\n[28,]     27   26.99  28.40 27.28 28.21\n[29,]     28   27.99  29.20 29.14 29.05\n[30,]     29   28.99  30.01 30.65 29.88\n[31,]     30   29.99  30.82 31.34 30.72\n[32,]     31   30.99  31.63 31.76 31.55\n[33,]     32   31.99  32.43 32.35 32.39\n[34,]     33   32.99  33.24 33.09 33.22\n[35,]     34   33.99  34.05 33.88 34.06\n[36,]     35   34.99  34.86 34.86 34.90\n\n\nAlthough the equating methods vary, the results are similar to those of Tucker method. Especially Frequency Estimation method shows how important it is to have data in different score ranges because there is no meaningful equation before the scale score of 20 and all lower scores are equated to -.5 in this method. Let’s also see the plotting of the chart above:\n\n\nCode\n# Plot the results\nplot(Q1_tucker, Q1_nom, Q1_freq, Q1_braun, lty=c(1,2,3,4),\n     col=c(\"blue\", \"black\", \"red\", \"forestgreen\"), addident = FALSE)\n\n\n\n\n\nAs also can be seen in the plot above, after the scale score of 20, all equating methods are quite similar to each other. Scores lower than 20 are equated with linear methods much better than the equi-percentile method as there isn’t adequate data in those score ranges.\nThis study is conducted for demonstrative purposes and still we can say that scale scores over 30 can be equated in the given forms."
  },
  {
    "objectID": "posts/Item Analysis with Item Response Theory (in Turkish)/index.html",
    "href": "posts/Item Analysis with Item Response Theory (in Turkish)/index.html",
    "title": "Item Analysis with Item Response Theory (in Turkish)",
    "section": "",
    "text": "Bu çalışmada çok kategorili puanlanan maddelerden elde edilen bir veri seti kullanılmıştır. Çalışmanın ilk kısmında çok kategorili maddelere yönelik MTK analizleri yürütülmüştür. Daha sonra aynı veri seti iki kategorili verilere dönüştürülmüştür. Yine MTK süreçleri bu sefer de iki kategorili maddeler için yürütülmüştür. İzlenen adımlar şu şekildedir:\n1. Çoklu puanlanan maddelere yönelik olarak;\n\nUygun MTK modeli nedir?\nBu modele göre madde ve test parametreleri nasıldır?\nİdeal ve sorunlu madde örnekleri nasıldır?\nTest bilgi fonksiyonunu nasıldır?\nBirey yetenek puanlarının dağılımı nasıldır?\n\n2. Her bir maddeyi, kendi madde ortalamasından keserek 1-0 verisine dönüştürünüz. Buna göre iki kategorili puanlanan maddelere yönelik olarak;\n\nUygun MTK modeli nedir?\nBu modele göre madde ve test parametreleri nasıldır?\nİdeal ve sorunlu madde örnekleri nasıldır?\nTest bilgi fonksiyonunu nasıldır?\nBirey yetenek puanlarının dağılımı nasıldır?\n\n\n\nKullanılan paketleri listeleyelim:\n\nlibrary(psych)\nlibrary(GPArotation)\nlibrary(sirt)\nlibrary(ltm)\n\nTabi ki işe öncelikle verinin working directory’den yüklenmesi ve ön düzenleme süreçleri ile başladık:\n\ndata1&lt;- read.csv2(\"sampledata.csv\")\nstr(data1) #yapısını incelemek için.\n\n'data.frame':   531 obs. of  25 variables:\n $ SIRA    : int  1 2 3 4 5 6 7 8 9 10 ...\n $ Fakulte : chr  \"Diger\" \"Diger\" \"Diger\" \"Diger\" ...\n $ Sinif   : int  4 4 5 4 5 4 4 4 4 4 ...\n $ Cinsiyet: chr  \"Kad\\xfdn\" \"Kad\\xfdn\" \"Kad\\xfdn\" \"Kad\\xfdn\" ...\n $ Ortalama: chr  \"2.88\" \"2.93\" \"3.12\" \"3.28\" ...\n $ L1      : int  2 1 2 2 1 3 3 1 1 2 ...\n $ L2      : int  1 1 4 2 1 4 4 3 4 1 ...\n $ L3      : int  2 4 4 3 1 4 3 1 4 4 ...\n $ L4      : int  1 2 4 3 2 4 4 1 2 3 ...\n $ L5      : int  2 4 4 4 2 5 4 1 3 4 ...\n $ L6      : int  1 5 4 3 4 3 3 5 4 2 ...\n $ L7      : int  2 1 4 3 5 4 4 1 2 3 ...\n $ L8      : int  1 3 3 3 1 4 4 1 2 1 ...\n $ L9      : int  2 1 4 4 1 3 5 1 3 4 ...\n $ L10     : int  1 1 2 3 2 4 4 1 4 2 ...\n $ L11     : int  2 1 4 3 3 4 5 4 3 2 ...\n $ L12     : int  1 3 4 3 5 4 5 2 3 2 ...\n $ L13     : int  2 1 2 2 1 2 3 1 1 4 ...\n $ L14     : int  1 1 1 2 1 3 4 1 2 5 ...\n $ L15     : int  3 3 4 3 2 3 4 1 2 4 ...\n $ L16     : int  1 1 1 2 1 2 2 2 2 5 ...\n $ L17     : int  2 2 2 2 1 3 4 1 2 4 ...\n $ L18     : int  2 2 2 3 4 3 2 1 2 3 ...\n $ L19     : int  2 2 2 3 2 4 3 1 4 2 ...\n $ L20     : int  1 1 2 3 1 3 4 1 3 4 ...\n\n\nVeri setinde ilk sütunun sıra sayıları olduğunu görünce aman tanrım dedik ve sildik.\n\ndata1&lt;-data1[,-1] # fazla kalabalığa gerek yok, aynı isimle devam.\n\nKayıp veri olup olmadığını anlamak için:\n\ndata1[data1 == 0] &lt;- NA\nsum(is.na(data1))\n\n[1] 44\n\n\nNeredeyse %8 oranında missing value var. Too much! Alayını atıyoruz. Artık adını da değiştirelim.\n\ndata2&lt;- na.omit(data1)\n\nMadem ki öylesine bir veri seti ile öylesine bir analiz yapıyoruz ve practical kaygılarımız yok, veri setimizi büyütelim. 1000 kişi olsun:\n\nset.seed(16611106)  # olur da sen de denemek istersen diye aynı veri setini üretmemizi sağlar.\ndata3 &lt;- data2[sample(1:495, 1000, replace = T), ] #adını da değiştirelim \n\nSon olarak, veri setinde işimize yaramayacak bir sürü demografik detay var. Atıyoruz:\n\nrow.names(data3)&lt;- NULL #önce adlarını silelim.\n\ndata4&lt;- data3[,5:24]  #5. sütundan sonrası çöp  \n\nÇalışmanın birinci araştırma sorusu kapsamında “data4” adlı veri seti kullanılmıştır. Bu veri seti 1000 gözlemden ve 20 değişkenden oluşmaktadır. Değişkenler, 1 ile 5 arasında bir tam sayı değeri almaktadır. Çalışmanın ikinci araştırma sorusu kapsamında ise her bir madde kendi madde ortalamasından kesilerek iki kategorili veriye dönüştürülmüştür. Bu aşamalar ilgili başlık altında raporlaştırılmıştır. Ön düzenlemelerin ardından araştırma sorularına cevap aramak için ileri analizlere devam edilmiştir."
  },
  {
    "objectID": "posts/Item Analysis with Item Response Theory (in Turkish)/index.html#giriş",
    "href": "posts/Item Analysis with Item Response Theory (in Turkish)/index.html#giriş",
    "title": "Item Analysis with Item Response Theory (in Turkish)",
    "section": "",
    "text": "Bu çalışmada çok kategorili puanlanan maddelerden elde edilen bir veri seti kullanılmıştır. Çalışmanın ilk kısmında çok kategorili maddelere yönelik MTK analizleri yürütülmüştür. Daha sonra aynı veri seti iki kategorili verilere dönüştürülmüştür. Yine MTK süreçleri bu sefer de iki kategorili maddeler için yürütülmüştür. İzlenen adımlar şu şekildedir:\n1. Çoklu puanlanan maddelere yönelik olarak;\n\nUygun MTK modeli nedir?\nBu modele göre madde ve test parametreleri nasıldır?\nİdeal ve sorunlu madde örnekleri nasıldır?\nTest bilgi fonksiyonunu nasıldır?\nBirey yetenek puanlarının dağılımı nasıldır?\n\n2. Her bir maddeyi, kendi madde ortalamasından keserek 1-0 verisine dönüştürünüz. Buna göre iki kategorili puanlanan maddelere yönelik olarak;\n\nUygun MTK modeli nedir?\nBu modele göre madde ve test parametreleri nasıldır?\nİdeal ve sorunlu madde örnekleri nasıldır?\nTest bilgi fonksiyonunu nasıldır?\nBirey yetenek puanlarının dağılımı nasıldır?\n\n\n\nKullanılan paketleri listeleyelim:\n\nlibrary(psych)\nlibrary(GPArotation)\nlibrary(sirt)\nlibrary(ltm)\n\nTabi ki işe öncelikle verinin working directory’den yüklenmesi ve ön düzenleme süreçleri ile başladık:\n\ndata1&lt;- read.csv2(\"sampledata.csv\")\nstr(data1) #yapısını incelemek için.\n\n'data.frame':   531 obs. of  25 variables:\n $ SIRA    : int  1 2 3 4 5 6 7 8 9 10 ...\n $ Fakulte : chr  \"Diger\" \"Diger\" \"Diger\" \"Diger\" ...\n $ Sinif   : int  4 4 5 4 5 4 4 4 4 4 ...\n $ Cinsiyet: chr  \"Kad\\xfdn\" \"Kad\\xfdn\" \"Kad\\xfdn\" \"Kad\\xfdn\" ...\n $ Ortalama: chr  \"2.88\" \"2.93\" \"3.12\" \"3.28\" ...\n $ L1      : int  2 1 2 2 1 3 3 1 1 2 ...\n $ L2      : int  1 1 4 2 1 4 4 3 4 1 ...\n $ L3      : int  2 4 4 3 1 4 3 1 4 4 ...\n $ L4      : int  1 2 4 3 2 4 4 1 2 3 ...\n $ L5      : int  2 4 4 4 2 5 4 1 3 4 ...\n $ L6      : int  1 5 4 3 4 3 3 5 4 2 ...\n $ L7      : int  2 1 4 3 5 4 4 1 2 3 ...\n $ L8      : int  1 3 3 3 1 4 4 1 2 1 ...\n $ L9      : int  2 1 4 4 1 3 5 1 3 4 ...\n $ L10     : int  1 1 2 3 2 4 4 1 4 2 ...\n $ L11     : int  2 1 4 3 3 4 5 4 3 2 ...\n $ L12     : int  1 3 4 3 5 4 5 2 3 2 ...\n $ L13     : int  2 1 2 2 1 2 3 1 1 4 ...\n $ L14     : int  1 1 1 2 1 3 4 1 2 5 ...\n $ L15     : int  3 3 4 3 2 3 4 1 2 4 ...\n $ L16     : int  1 1 1 2 1 2 2 2 2 5 ...\n $ L17     : int  2 2 2 2 1 3 4 1 2 4 ...\n $ L18     : int  2 2 2 3 4 3 2 1 2 3 ...\n $ L19     : int  2 2 2 3 2 4 3 1 4 2 ...\n $ L20     : int  1 1 2 3 1 3 4 1 3 4 ...\n\n\nVeri setinde ilk sütunun sıra sayıları olduğunu görünce aman tanrım dedik ve sildik.\n\ndata1&lt;-data1[,-1] # fazla kalabalığa gerek yok, aynı isimle devam.\n\nKayıp veri olup olmadığını anlamak için:\n\ndata1[data1 == 0] &lt;- NA\nsum(is.na(data1))\n\n[1] 44\n\n\nNeredeyse %8 oranında missing value var. Too much! Alayını atıyoruz. Artık adını da değiştirelim.\n\ndata2&lt;- na.omit(data1)\n\nMadem ki öylesine bir veri seti ile öylesine bir analiz yapıyoruz ve practical kaygılarımız yok, veri setimizi büyütelim. 1000 kişi olsun:\n\nset.seed(16611106)  # olur da sen de denemek istersen diye aynı veri setini üretmemizi sağlar.\ndata3 &lt;- data2[sample(1:495, 1000, replace = T), ] #adını da değiştirelim \n\nSon olarak, veri setinde işimize yaramayacak bir sürü demografik detay var. Atıyoruz:\n\nrow.names(data3)&lt;- NULL #önce adlarını silelim.\n\ndata4&lt;- data3[,5:24]  #5. sütundan sonrası çöp  \n\nÇalışmanın birinci araştırma sorusu kapsamında “data4” adlı veri seti kullanılmıştır. Bu veri seti 1000 gözlemden ve 20 değişkenden oluşmaktadır. Değişkenler, 1 ile 5 arasında bir tam sayı değeri almaktadır. Çalışmanın ikinci araştırma sorusu kapsamında ise her bir madde kendi madde ortalamasından kesilerek iki kategorili veriye dönüştürülmüştür. Bu aşamalar ilgili başlık altında raporlaştırılmıştır. Ön düzenlemelerin ardından araştırma sorularına cevap aramak için ileri analizlere devam edilmiştir."
  },
  {
    "objectID": "posts/Item Analysis with Item Response Theory (in Turkish)/index.html#çok-boyutlu-maddelere-yönelik-aşamalar",
    "href": "posts/Item Analysis with Item Response Theory (in Turkish)/index.html#çok-boyutlu-maddelere-yönelik-aşamalar",
    "title": "Item Analysis with Item Response Theory (in Turkish)",
    "section": "1. Çok Boyutlu Maddelere Yönelik Aşamalar",
    "text": "1. Çok Boyutlu Maddelere Yönelik Aşamalar\nÇalışmanın ilk araştırma sorusu kapsamında, yapılan analizler çok kategorili puanlanan maddeler üzerinden yürütülmüştür.\n\n1.a. Varsayımların kontrolü ve uygun MTK modelinin belirlenmesi\nÇok kategorili puanlanan maddelerden oluşan ölçeğin Madde Tepki Kuramı çerçevesinde incelenmesi sürecinde öncelikle uygun MTK modelinin belirlenebilmesi için varsayım kontrolleri yapılmıştır. Bu bağlamda, tek boyutluluk ve yerel bağımsızlık varsayımları ile model-veri uyumu kontrol edilmiştir. Tek boyutluluk varsayımı kontrolü için paralel analiz, yamaç birikinti grafiği ve faktör analizi kullanılmıştır. Bu aşamada kullanılan paketler: Psych ve GPArotation.\nParalel Analizden elde edilen yamaç birikinti grafiği şöyledir:\n\nfa.parallel(data4, n.obs = 1000, cor = \"poly\")\n\n\n\n\nParallel analysis suggests that the number of factors =  5  and the number of components =  2 \n\n\nYamaç birikinti grafiği iki bileşenli bir yapıyı göstermektedir.  Son olarak faktör analizi yardımı ile hem tek hem de iki bileşenli modeller oluşturulmuştur:\n\nfa_model1 &lt;- fa.poly(data4)\nfa_model2 &lt;- fa(data4, nfactors = 2, cor=\"poly\")\n\nBu iki modelin burada çıktılarını alsak baya uzun oluyor. Ama özetle; ben beğendiğim ve devam analizi için seçtiğim 2 faktörlü model ile devam ediyorum. Bu modelin çıktıları incelendiğinde, 1-12 numaralı maddelerin bir boyutta, 13-20 numaralı maddelerin ise diğer bir boyutta yüklendiği görülmektedir. Bu nedenle veri seti aşağıdaki adımlar izlenerek ikiye bölünmüş ve ileri analizler her iki faktör için de ayrı ayrı yürütülmüştür.\n\ndata4a&lt;-data4[1:12]\ndata4b&lt;-data4[13:20]\n\nYerel bağımsızlık varsayımının kontrolü için Yen’in Q analizi (Yen, 1984) her iki bileşen için de uygulanmıştır. Bu süreçte sirt paketinden (Robitzsch, 2020) yararlanılmış ve aşağıdaki adımlar izlenmiştir.  \nMod1 &lt;- TAM::tam.mml( resp=data4a )\nMod1.wle &lt;- TAM::tam.wle(Mod1)\nMod1.q3 &lt;- sirt::Q3( dat=data4a, theta=Mod1.wle$theta, b=Mod1$item_irt[[3]] )\nMod2 &lt;- TAM::tam.mml( resp=data4b )\nMod2.wle &lt;- TAM::tam.wle(Mod2)\nMod2.q3 &lt;- sirt::Q3( dat=data4b, theta=Mod2.wle$theta, b=Mod2$item_irt[[3]] )\nBu üstteki kodun çıktısı çooook uzun. Buraya koymuyorum. Tabi biz veriyi bootstrap ile çoğalttığımız için bu varsayım karşılanmadı ama gerçek veri ile çalışsaydık bu varsayımın karşılanmaması durumunda çöp olacaktı analiz. Yani örneklem yetersiz, daha çok örneklem lazım diyecektik. Ya da modeli veya madde sayılarını inceleyecektik vs. vs.\nBirinci araştırma sorusunun son aşamasında ise model veri uyumunun incelenmesi ve en uygun modelin seçilmesi yer almaktadır. Model-veri uyumu incelemesi ltm paketi (Rizopoulos, 2006) yardımı ile her iki bileşen için de ayrı ayrı GRM modeli ile yürütülmüştür. Her iki bileşende de model-1, ayırt edicilik düzeylerinin her madde için farklılaştığı modeli betimlemektedir. Model-2 ise ayırt edicilik düzeylerinin her madde için eşit tutulduğu modeldir.\n\nmodel1_d4a&lt;- grm(data4a)\nmodel2_d4a&lt;- grm(data4a, constrained = TRUE)\nmodel1_d4b&lt;- grm(data4b)\nmodel2_d4b&lt;- grm(data4b, constrained = TRUE)\nanova(model2_d4a, model1_d4a)\n\n\n Likelihood Ratio Table\n                AIC      BIC   log.Lik    LRT df p.value\nmodel2_d4a 31785.45 32025.93 -15843.72                  \nmodel1_d4a 31660.51 31954.98 -15770.26 146.94 11  &lt;0.001\n\nanova(model2_d4b, model1_d4b)\n\n\n Likelihood Ratio Table\n                AIC      BIC   log.Lik   LRT df p.value\nmodel2_d4b 21742.58 21904.54 -10838.29                 \nmodel1_d4b 21664.77 21861.08 -10792.38 91.81  7  &lt;0.001\n\n\nModeller arasında manidar farklılık anlamına gelen p değerlerine (&lt;.05) sahip olmasının yanı sıra, Akaike ve Bayesian bilgi kriter değerleri en düşük olan modellerin her iki bileşen için de model-1 olduğu görülmektedir. Bu nedenle model-1 ile daha iyi bir model-veri uyumu sağlanmaktadır. Devam analizleri her iki bileşen için de model-1 ile yürütülmüştür.\n\n\n1.b. Madde parametreleri\nModel-veri uyumu sınandıktan ve en uygun model belirlendikten sonra, madde parametrelerinin incelenmesi aşamasına geçilmiştir. Model-1 üzerinden aşağıdaki kod satırları kullanılarak elde edilen madde parametreleri görülebilir.\n\ncoef(model1_d4a)\n\n    Extrmt1 Extrmt2 Extrmt3 Extrmt4 Dscrmn\nL1   -1.259  -0.119   0.576   1.625  2.361\nL2   -0.994   0.224   1.002   2.265  1.383\nL3   -1.157  -0.363   0.254   1.498  2.401\nL4   -1.270  -0.276   0.610   1.718  2.355\nL5   -1.130  -0.312   0.337   1.382  2.248\nL6   -1.565  -0.657   0.322   1.309  1.895\nL7   -1.061  -0.257   0.460   1.453  2.129\nL8   -1.201  -0.401   0.314   1.367  2.635\nL9   -1.102  -0.322   0.363   1.246  2.296\nL10  -1.222  -0.295   0.476   1.473  1.815\nL11  -1.218  -0.457   0.345   1.320  2.123\nL12  -1.096  -0.249   0.737   1.819  1.661\n\ncoef(model1_d4b)\n\n    Extrmt1 Extrmt2 Extrmt3 Extrmt4 Dscrmn\nL13  -0.665   0.150   0.830   1.786  2.353\nL14  -0.791   0.148   0.917   1.802  2.290\nL15  -1.388  -0.736   0.198   0.920  2.308\nL16  -1.210   0.064   1.206   1.969  1.718\nL17  -0.998   0.150   1.077   1.832  1.898\nL18  -1.775  -0.720   0.598   1.664  1.338\nL19  -1.751  -0.514   0.694   1.600  1.505\nL20  -1.262  -0.351   0.505   1.277  2.007\n\n\nTablo 4 incelendiğinde, her madde için eşik parametrelerinin beklendik bir şekilde birinciden dördüncüye doğru arttığı görülmektedir. Bazı maddelerin birinci eşik parametresinin -1’den daha büyük bir değerle başladığı dikkat çekmektedir. Örneğin ikinci bileşene ait ilk madde olan 13. Maddenin ilk kategorisini seçen bir bireyin yetenek seviyesi %50 ihtimalle -.66’dan daha düşüktür. \n\n\n1.c. Örnek Madde Karakteristik Eğrileri\nÇalışmanın bu aşamasında madde karakteristik eğrisi ideal ve sorunlu olan birer madde incelenmiş ve yorumlanmıştır. Bu nedenle, öncelikle birinci bileşeni oluşturan tüm maddelerin madde karakteristik eğrileri plot() fonksiyonu ile oluşturulmuştur. İdeal bir madde karakteristik eğrisine sahip olduğu düşünülen sekizinci madde ve kısmi sorunlu olduğu düşünülen ikinci maddenin madde karakteristik eğrileri aşağıdaki kod satırları yürütülerek oluşturulmuştur.\n\nplot(model1_d4a, type=\"ICC\",item=6, xlab= \"YETENEK\", cex.main = 1, main = \"MADDE KARAKTERİSTİK EĞRİSİ- Madde: 6\", ylab = \"OLASILIK\" , lwd= 2, col.main= \"red\", font.axis= 3, font.lab=2)\n\n\n\nplot(model1_d4a, type=\"ICC\",xlab= \"YETENEK\", cex.main = 1, ylab = \"OLASILIK\" , lwd= 2, col.main= \"red\", font.axis= 3, font.lab=2, main = \"MADDE KARAKTERİSTİK EĞRİSİ- Madde: 2\", items = 2)\n\n\n\n\nGörülen ilk grafik diğer maddelere göre daha ideal dağılım gösteren bir maddeye aittir. Bu maddenin eğrileri tüm yetenek düzeylerini kapsayacak şekilde sivrilip dağılmaktadır. Örneğin, sıfır yetenek düzeyinde bir bireyin üçüncü kategoride yer alma olasılığı en yüksek düzeydedir. Benzer şekillerde diğer kategorilerin de yüksek olasılık ile temsil ettikleri yetenek düzeyleri belirgin bir şekilde görülmektedir. Bu durumda bu maddenin ayırt edicilik düzeyinin yüksek olması beklenir. Madde parametreleri çıktısında da görüleceği üzere, bu maddeye ait ayırt edicilik parametresi birinci bileşenin en yüksek ayırt edicilik düzeyidir.\nİkinci grafik ise kısmen problemli olduğu düşünülen bir grafiktir. Bu grafiğin daha iyi yorumlanabilmesi için, eksenlerinde yer alan değerler axis() fonksiyonu ile daha detaylı hale getirilmiştir. Ayrıca abline() fonksiyonu ile 0.6 yetenek düzeyine dikey bir çizgi eklenmiştir.\n\nplot(model1_d4a, type=\"ICC\",\n     xlab= \"YETENEK\", \n     cex.main = 1, \n     ylab = \"OLASILIK\" , \n     lwd= 2, col.main= \"red\", \n     font.axis= 3, \n     font.lab=2, \n     main = \"MADDE KARAKTERİSTİK EĞRİSİ- Madde: 2\", \n     items = 2)\naxis(2, at = seq(0, 1, by = .1))\naxis(1, at = seq(-4, 4, by = .1))\nabline(v=.6)\n\n\n\n\nMaddenin eğrileri incelendiğinde, 0.6 yetenek düzeyinde bir bireyin ikinci, üçüncü ve dördüncü kategorileri seçme olasılıklarının birbirlerine çok yakın olduğu görülmektedir. Bu durum, maddenin ayırt ediciliğini olumsuz olarak etkilemektedir. Daha önceki çıktılardan bilindiği üzere bu maddeye ait ayırt edicilik parametresi birinci bileşenin en düşük ayırt edicilik düzeyidir. Ayrıca, bu maddenin üçüncü kategorisinin sivrilmediği de dikkat çekmektedir. Bu durumda bu maddenin üçüncü kategorisinin çıkarılarak dört kategorili bir maddeye dönüştürülmesi düşünülebilir. Bir başka seçenek ise bu maddenin ölçekten çıkarılması ve analizlerin yeniden yapılması olabilir. \n\n\n1.d. Test Bilgi Fonksiyonu\nHer iki bileşen için de test bilgi fonksiyonları aşağıdaki kod satırları kullanılarak elde edilmiştir:\n\nplot(model1_d4a, type=\"IIC\", items = 0, xlab= \"YETENEK\", cex.main = 1, main = \"TEST   BİLGİ   FONKSİYONU\", ylab = \"BİLGİ\" , lwd= 2, col.main= \"red\", col=\"blue\", font.axis= 3, font.lab=2)\n\n\n\nplot(model1_d4b, type=\"IIC\", items = 0, xlab= \"YETENEK\", cex.main = 1, main = \"TEST   BİLGİ   FONKSİYONU\", ylab = \"BİLGİ\" , lwd= 2, col.main= \"red\", col=\"blue\", font.axis= 3, font.lab=2)\n\n\n\n\nTest bilgi fonksiyonlarının 0 yetenek düzeyinde en yüksek seviyede olduğu, -/+ 2 yetenek düzeylerinde sert bir şekilde düşmeye başladığı ve -/+ 4 yetenek düzeylerinde en düşük seviyesinde olduğu görülmektedir. Bu durumda her iki bileşenin de en çok 0 yetenek düzeyinde bilgi sağladığı ve -2 ile +2 aralığında yüksek düzeyde bilgi sağladığı söylenebilir. Ancak bu aralığın ötesinde sağlanan bilginin hızla azaldığı düşünülebilir. \nÇalışmanın bu aşamasında madde bilgi eğrileri de aşağıdaki kod satırları kullanılarak elde edilmiştir:\n\nplot(model1_d4a, type=\"IIC\",xlab= \"YETENEK\", cex.main = 1, ylab = \"BİLGİ\" , col.main= \"red\", font.axis= 3, font.lab=2, main = \"1. BİLEŞEN MADDE BİLGİ  EĞRİLERİ\")\n\n\n\nplot(model1_d4b, type=\"IIC\",xlab= \"YETENEK\", cex.main = 1, ylab = \"BİLGİ\" , col.main= \"red\", font.axis= 3, font.lab=2, main = \"2. BİLEŞEN MADDE BİLGİ  EĞRİLERİ\")\n\n\n\n\nBunlar incelendiğinde ise her iki bileşen için de maddelerin ölçülen özelliği geniş bir yetenek puanı ölçeğinde ölçtüğü görülmektedir. Birinci bileşen içerisinde en düşük bilgi sağlayan maddenin ikinci madde olduğu dikkat çekmektedir. Yine bilgi eğrisi en yüksek maddenin birinci bileşen için sekizinci madde olduğu da görülmektedir.  Bunlar, bir önceki bölümde kısmi problemli ve ideal dağılımlı olarak incelenen maddelerdir. Eğer ikinci bileşen için birer madde seçilecek olsaydı, o bileşene ait altı ve üç numaralı maddeler sırasıyla kısmi problemli ve ideal maddelere örnek olarak seçilebilirdi. \n\n\n1.e. Yetenek Puanları\nÇok kategorili puanlanan maddelere yönelik olarak yürütülen analizlerin son aşamasında bireylere ait yetenek puanları hesaplanmış ve bunların dağılımı incelenmiştir. Analizlerin bu aşamasında şu kod satırları kullanılarak birinci ve ikinci bileşen için yetenek puanları ayrı ayrı hesaplanmıştır. \n\n\nCode\n#BİLEŞEN-1\nscore_d4a&lt;- factor.scores(model1_d4a)\noruntu_d4a&lt;- score_d4a[[1]]\noruntu_d4a$toplam&lt;- rowSums((oruntu_d4a[,1:12]))\nscore_d4a&lt;- factor.scores(model1_d4a)\noruntu_d4a&lt;- score_d4a[[1]]\noruntu_d4a$toplam&lt;- rowSums((oruntu_d4a[,1:12]))\ntheta_d4a &lt;- numeric()\nfor (i in 1:419){ \n for (j in 1:1000){\n         if (sum (oruntu_d4a[i, 1: 12] == data4a[j, 1:12])==12)\n              theta_d4a[j] &lt;- oruntu_d4a[i, 15]      }}\ndata4a$theta&lt;-theta_d4a\ndata4a$toplam&lt;-rowSums(data4a[1:12])\n\n\n\n#BİLEŞEN-2\nscore_d4b&lt;- factor.scores(model1_d4b)\noruntu_d4b&lt;- score_d4b[[1]]\noruntu_d4b$toplam&lt;- rowSums((oruntu_d4b[,1:12]))\ntheta_d4b &lt;- numeric()\nfor (i in 1:404){\n  for (j in 1:1000){\n      if (sum (oruntu_d4b[i, 1: 8] == data4b[j, 1:8])==8)\n             theta_d4b[j] &lt;- oruntu_d4b[i, 11] }}\ndata4b$theta&lt;-theta_d4b\ndata4b$toplam&lt;-rowSums(data4b[1:8])\n\n\nBunun ardından describe() fonksiyonu ile birey yetenek puanlarının betimsel istatikleri elde edilmiştir. Ayrıca hist() fonksiyonu ile histogram grafikleri oluşturulmuştur. Şu kod satırları kullanılmıştır. \n\n\nCode\n#BİLEŞEN-1\ndescribe(data4a$theta)\n\n\n   vars    n mean   sd median trimmed  mad  min  max range skew kurtosis   se\nX1    1 1000 0.05 0.93   0.02    0.05 0.92 -2.3 2.59  4.88 0.03     -0.2 0.03\n\n\nCode\nhist(data4a$theta, xlab= \"YETENEK\", cex.main = 1, ylab = \"FREKANS\" , col.main= \"red\", font.axis= 3, font.lab=2, main = \"1. BİLEŞEN BİREY YETENEK PUANLARI\")\n\n\n\n\n\nCode\n#BİLEŞEN-2\ndescribe(data4b$theta)\n\n\n   vars    n  mean   sd median trimmed  mad   min max range  skew kurtosis   se\nX1    1 1000 -0.01 0.92   0.05       0 0.89 -2.19 2.5  4.69 -0.04    -0.14 0.03\n\n\nCode\nhist(data4b$theta, xlab= \"YETENEK\", cex.main = 1, ylab = \"FREKANS\" , col.main= \"red\", font.axis= 3, font.lab=2, main = \"2. BİLEŞEN BİREY YETENEK PUANLARI\")\n\n\n\n\n\nHer iki bileşene ait betimsel istatikler ve histogram grafikleri incelendiğinde, ortalamalarının sıfıra, standart sapmalarının da bire çok yakın olduğu görülmektedir. Bu durum her iki bileşenden elde edilen verinin de normal dağılım sergilediğini göstermektedir."
  },
  {
    "objectID": "posts/Item Analysis with Item Response Theory (in Turkish)/index.html#iki-kategorili-maddelere-yönelik-aşamalar",
    "href": "posts/Item Analysis with Item Response Theory (in Turkish)/index.html#iki-kategorili-maddelere-yönelik-aşamalar",
    "title": "Item Analysis with Item Response Theory (in Turkish)",
    "section": "2. İki Kategorili Maddelere Yönelik Aşamalar",
    "text": "2. İki Kategorili Maddelere Yönelik Aşamalar\nÇalışmanın ikinci araştırma sorusu kapsamında, çok kategorili maddelerden oluşan veri setinin iki kategoriye dönüştürülmesi gerekmektedir. Çok kategorili puanlanan maddelerin iki kategorili olarak kodlanmasında izlenen adımlar şunlardır:\n\ndata5&lt;- data4\nfor(i in 1:1000) {\n   for(j in 1:20) {\nif(data5[i,j] &lt;= mean(data5[,j])) {data5[i,j] &lt;- 0} \nelse data5[i,j] &lt;- 1\n }\n}\n\n\n2.a. Varsayımların Kontrolü ve Uygun MTK Modeli\nİki kategorili puanlanan maddelerden oluşan ölçeğin Madde Tepki Kuramı çerçevesinde incelenmesi sürecinde öncelikle uygun MTK modelinin belirlenebilmesi için varsayım kontrolleri yapılmıştır. Bu bağlamda, tek boyutluluk ve yerel bağımsızlık varsayımları ile model-veri uyumu kontrol edilmiştir. Tek boyutluluk varsayımı kontrolü için paralel analiz, yamaç birikinti grafiği ve faktör analizi kullanılmıştır. Bu amaçla, Psych paketinden (Revelle, 2020) faydalanılmıştır.\n\nfa.parallel(data5, main = \"PARALEL ANALİZ SAÇILIM GRAFİĞİ\", ylabel = \"Temel Bileşenler ve Faktör Analizi Özdeğerleri\")\n\n\n\n\nParallel analysis suggests that the number of factors =  4  and the number of components =  2 \n\n\nParalel analiz ve yamaç birikinti grafiği iki bileşenli bir yapıyı göstermektedir.  Bu nedenle hem iki hem de tek bileşenli modeller oluşturulmuştur. Aşağıda izlenen adımlar sonrası elde edilen modeller ait faktör yükleri görülebilir.\n\nmodel1 &lt;- fa(data5, cor = \"tet\")\nmodel2 &lt;- fa(data5, cor = \"tet\", nfactors = 2)\n\nFaktör yüklerinin her madde için her iki modelde de .60’nın üzerinde olduğu görülmektedir. Tek bileşenli yapının açıkladığı varyansın ise %61 olduğu anlaşılmaktadır. Bunun yanı sıra, iki bileşenli yapının birinci bileşenin %40, ikinci bileşenin %28 olmak üzere toplamda %68 oranında açıklanan varyansa sahip olduğu görülmektedir. Açıklanan varyans açısından modeller arasındaki farkın çok büyük olmadığı düşünüldüğünden, ileri analizlere tek bileşenli model ile devam edilmesine karar verilmiştir.  \nYerel bağımsızlık varsayımının kontrolü için Yen’in Q analizi (Yen, 1984)  uygulanmıştır. Bu süreçte sirt paketinden (Robitzsch, 2020) yararlanılmış ve aşağıdaki adımlar izlenmiştir. \n\n\nCode\nmodel &lt;- rasch.mml2(data5)\nmodel.wle &lt;-wle.rasch( dat=data5, b=model$item$b )\nyenq3 &lt;-Q3( dat=data5, theta=model.wle$theta, b=model$item$b)\n\n\nYerel bağımsızlık varsayımının karşılandığı görülmektedir. Bu nedenle model-veri uyumu incleme aşamasına geçilmiştir. Bu aşamada ltm paketinden (Rizopoulos, 2006) faydalanılmıştır. Aşağıdaki kod satırları kullanılarak elde edilen Benzerlik oranı tabloları görülebilir.\n\nmodel1 &lt;- rasch(data5)\nmodel2 &lt;- ltm(data5 ~ z1)  \nmodel3 &lt;- tpm(data5)\nanova(model1, model2)\n\n\n Likelihood Ratio Table\n            AIC      BIC  log.Lik    LRT df p.value\nmodel1 15975.79 16078.85 -7966.89                  \nmodel2 15913.21 16109.52 -7916.61 100.58 19  &lt;0.001\n\nanova(model1, model3)\n\n\n Likelihood Ratio Table\n            AIC      BIC  log.Lik    LRT df p.value\nmodel1 15975.79 16078.85 -7966.89                  \nmodel3 15914.14 16208.61 -7897.07 139.64 39  &lt;0.001\n\nanova(model2, model3)\n\n\n Likelihood Ratio Table\n            AIC      BIC  log.Lik   LRT df p.value\nmodel2 15913.21 16109.52 -7916.61                 \nmodel3 15914.14 16208.61 -7897.07 39.07 20   0.007\n\n\nAkaike ve Bayesian bilgi kriterleri incelendiğinde, üç model arasında ikinci modelin manidar farkla en uygun model olduğu görülmektedir. Bu nedenle, devam analizleri iki parametreli model ile yürütülmüştür.\n\n\n2.b. Madde Parametreleri\nModel-veri uyumu sınandıktan ve en uygun model belirlendikten sonra, madde parametrelerinin incelenmesi aşamasına geçilmiştir. Model-2 üzerinden coef() fonksiyonu kullanılarak elde edilen madde parametreleri aşağıdadır.\n\ncoef(model2)\n\n        Dffclt   Dscrmn\nL1  -0.8112296 2.832181\nL2  -0.7312085 1.933827\nL3  -0.8181936 2.659799\nL4  -0.8109842 3.100650\nL5  -0.8380050 2.394060\nL6  -1.0180088 2.070947\nL7  -0.8694781 2.103077\nL8  -0.8977956 2.683691\nL9  -0.7951705 2.798913\nL10 -0.8580558 2.207506\nL11 -0.8789145 2.322482\nL12 -0.8528908 1.705619\nL13 -0.6390827 2.246105\nL14 -0.6903395 2.247931\nL15 -0.8834606 2.517332\nL16 -0.7716486 2.330535\nL17 -0.7333232 2.334138\nL18 -1.1852158 1.497807\nL19 -1.0471877 1.896025\nL20 -0.8162163 2.864486\n\n\nÇıktılar incelendiğinde, güçlük parametresi en yüksek olan maddenin 13. madde olduğu göze çarpmaktadır. Yetenek düzeyi -.63’ten daha yüksek olan bireyler bu maddeyi %50’den daha yüksek bir ihtimalle doğru cevaplayacaklardır.\n\n\n2.c.  Örnek Madde Karakteristik Eğrileri\nÇalışmanın bu aşamasında, iki kategorili puanlanan maddelerin analizinde madde karakteristik eğrisi ideal ve sorunlu olan birer madde incelenmiş ve yorumlanmıştır. Bu nedenle, öncelikle birinci bileşeni oluşturan tüm maddelerin madde bilgi eğrileri plot() fonksiyonu ile oluşturulmuş, aşağıdaki kod satırları kullanılmıştır:\n\nplot(model2, type=\"IIC\",xlab= \"YETENEK\", cex.main = 1, ylab = \"BİLGİ\" , col.main= \"red\", font.axis= 3, font.lab=2, main = \"MADDE BİLGİ  EĞRİLERİ\")\n\n\n\n\nMadde bilgi eğrileri incelendiğinde, en çok bilgiyi dördüncü maddenin sağladığı görülmektedir. En düşük bilgi sağlayan maddelerden birisinin ise 12. madde olduğu görülmektedir. Bu nedenle madde karakteristik eğrilerinin incelenmesi sürecinde örnek olarak bu iki madde tercih edilmiştir. Bu maddelere ait karakteristik eğrileri aşağıdaki kod satırları ile elde edilmiştir.\n\nplot(model2, type=\"ICC\", items = c(4,12), labels = c(\"madde-4\", \"madde-12\"), legend = T, xlab= \"YETENEK\", cex.main = 1, main = \"MADDE KARAKTERİSTİK EĞRİSİ\", ylab = \"OLASILIK\" , lwd= 2)\npoints(-.81, .5, lwd= 3, pch= 3)\ntext(-.81, .5, lwd= 2, labels = \"b par.: -0.81\", pos = 4)\npoints(-.85, .5, lwd= 3, col= \"red\", pch=5)\ntext(-.85, .5, lwd= 3, labels = \"b par.: -0.85\", pos = 2, col= \"red\")\n\n\n\n\nDört ve 12 numaralı maddelerin güçlük parametreleri birbirlerine oldukça yakındır. Ancak dördüncü madde diğerine göre daha dik bir karakteristik eğrisine sahiptir. Bu durum iki maddenin ayırt edicilik parametrelerindeki farklılıktan kaynaklanmaktadır. Dördüncü madde yüksek bir ayırt edicilik parametresi ile ideal bir madde gibi görünürken, 12. madde düşük bir ayırt edicilik parametresi ile kısmi problemli bir madde görüntüsü sergilemektedir. Yine de 12 numaralı maddenin parametre değerlerinin kabul edilebilir olduğu da vurgulanmalıdır. \n\n\n2.d. Test Bilgi Fonksiyonu\nİki kategorili puanlanan maddelere yönelik test bilgi fonksiyonunu elde etmek için aşağıdaki kod satırları kullanılmıştır. locator() fonksiyonu aracılığı ile test bilgi fonksiyonunun tepe noktası tespit edilmiş ve ardından abline() fonksiyonu ile tepe noktasının koordinat çizgileri grafiğe eklenmiştir.\n\nplot(model2, type=\"IIC\", items = 0, xlab= \"YETENEK\", cex.main = 1, main = \"TEST   BİLGİ   FONKSİYONU\", ylab = \"BİLGİ\" , lwd= 2,col.main=\"red\",col=\"blue\", font.axis= 3, font.lab=2)\naxis(2, at = seq(0, 30, by =5))\naxis(1, at = seq(-4, 4, by = .1))\nabline(h=27.74, v=-.8451, lty=4)\n\n\n\n\nTest bilgi fonksiyonunun -0.84 yetenek düzeyinde en yüksek seviyede olduğu, -1.5 ile 0 yetenek düzeylerinde sert bir şekilde düşmeye başladığı ve -2.8 ve 1.1 yetenek düzeylerinde en düşük seviyesinde olduğu görülmektedir. Bu durumda en yüksek bilginin bu ölçekte -.8 yetenek düzeylerinde sağlandığı söylenebilir. \nBu durumda her iki bileşenin de en çok 0 yetenek düzeyinde bilgi sağladığı ve -2 ile +2 aralığında yüksek düzeyde bilgi sağladığı söylenebilir. Ancak bu aralığın ötesinde sağlanan bilginin hızla azaldığı düşünülebilir.\n\n\n2.e. Yetenek Puanları\nİki kategorili puanlanan maddelere yönelik olarak yürütülen analizlerin son aşamasında bireylere ait yetenek puanları hesaplanmış ve bunların dağılımı incelenmiştir. Şu kod satırları ile hesaplanan yetenek puanları veri setine eklenmiştir.\n\nscore_d5&lt;- factor.scores.ltm(model2)\noruntu_d5 &lt;- score_d5[[1]]\noruntu_d5$toplam &lt;- rowSums((oruntu_d5[,1:12]))\ntheta_d5 &lt;- numeric()\nfor (i in 1:454){\n     for (j in 1:1000){\n         if (sum (oruntu_d5[i, 1: 20] == data5[j, 1:20])==20)\n             theta_d5[j] &lt;- oruntu_d5[i, 23] }}\ndata5$theta&lt;-theta_d5\ndata5$toplam&lt;-rowSums(data5[1:20])\n\nBunun ardından describe() fonksiyonu ile birey yetenek puanlarının betimsel istatikleri elde edilmiştir. Ayrıca hist() fonksiyonu ile histogram grafikleri oluşturulmuştur. Bu süreçlerde kullanılan kod satırları şunlardır:\n\n#BİLEŞEN-1\ndescribe(data5$theta)\n\n   vars    n  mean   sd median trimmed  mad   min  max range  skew kurtosis\nX1    1 1000 -0.03 0.81   0.05    0.04 1.24 -2.17 0.89  3.06 -0.45     -0.8\n     se\nX1 0.03\n\nhist(data5$theta, xlab= \"YETENEK\", cex.main = 1, ylab = \"FREKANS\" , col.main= \"red\", font.axis= 3, font.lab=2, main = \"BİREY YETENEK PUANLARI\")\n\n\n\n\nİncelenen iki kategorili puanlanan veriye ait betimsel istatikleri incelendiğinde, ortalamanın -.03, standart sapmanın da .81 olduğu görülmektedir. Bu durum verinin normal dağıldığı şeklinde yorumlanabilir, ancak histogram grafiği incelendiğinde yetenek puanlarının -2 ile 1 arasında dağılım gösterdiği ve 1 yetenek puanında bir yığılma olduğu görülmektedir. Bu durum, verilerin çok kategorili puanlanan maddelerden ortalamaları doğrultusunda iki kategorili puanlanan maddelere çevrilmesiyle ilişkisi olduğu düşünülmektedir."
  },
  {
    "objectID": "posts/Item Analysis with Item Response Theory (in Turkish)/index.html#sonuç",
    "href": "posts/Item Analysis with Item Response Theory (in Turkish)/index.html#sonuç",
    "title": "Item Analysis with Item Response Theory (in Turkish)",
    "section": "SONUÇ",
    "text": "SONUÇ\nBu çalışma simüle edilmiş veri ile yürütülmüştür. Çıktılarında bu nedenle temel problemler görülebilmektedir. Yine de aşama aşama MTK ile madde analizinin R ile nasıl yapılacağına dair bana güzel bir referans olmaktadır. Bu çalışma, bir ders raporu olarak hazırlanmıştır."
  },
  {
    "objectID": "posts/Automated Essay Scoring (AES) with R/index.html",
    "href": "posts/Automated Essay Scoring (AES) with R/index.html",
    "title": "Automated Essay Scoring (AES) with R",
    "section": "",
    "text": "In the realm of education and standardized testing, technological advancements have brought forth a significant transformation. Among the noteworthy innovations in this field is the adoption of Automated Essay Scoring (AES), an approach that leverages the capabilities of natural language processing (NLP) and machine learning. AES holds the potential to redefine the essay evaluation and grading process, offering efficiency, consistency, and accessibility in a way previously unattainable.\nAt its essence, AES relies on sophisticated algorithms to examine written text, subjecting it to a predefined set of criteria. These criteria encompass various aspects, including grammar, sentence structure, vocabulary, coherence, and the quality of argumentation. In this regard, AES algorithms function akin to meticulous digital assessors, diligently seeking out patterns and features within the text. They assess elements such as the presence of persuasive thesis statements, the adept use of supporting evidence, and the logical flow of ideas within the essay. The result of this is a numerical score that reflects the overall quality of the essay in question.\nToday we will develop a linear regression model to predict essay scores using the famous ASAP dataset. There are eight different essay types in this dataset to explore. Here we will use the essay set 2. Before you continue, please go and read the description and the details on Kaggle. As we will use only one essay set, I have preapared the data as a separate csv file and if you are already familiar with the ASAP data (or you have taken a look at the Kaggle page), please download the data for our use case from here. The packages and some of their functions that we will be using in this topic are:\n\n\n\n\n\n\nA robot drawn by Dall-e\n\n\n\n\n\n\nLibraries\n\nlibrary(\"rmarkdown\") \n# paged_table() \nlibrary(\"readxl\") \n# read_excel()\nlibrary(\"tidyverse\") \nlibrary(\"textstem\") \n# lemmatize_strings()\nlibrary(\"stopwords\") \n# stopwords()\nlibrary(\"stringr\")   \n# str_squish()\nlibrary(\"tm\")        \n# remove_words()\nlibrary(\"quanteda\")  \n# nsentence()"
  },
  {
    "objectID": "posts/Automated Essay Scoring (AES) with R/index.html#introduction",
    "href": "posts/Automated Essay Scoring (AES) with R/index.html#introduction",
    "title": "Automated Essay Scoring (AES) with R",
    "section": "",
    "text": "In the realm of education and standardized testing, technological advancements have brought forth a significant transformation. Among the noteworthy innovations in this field is the adoption of Automated Essay Scoring (AES), an approach that leverages the capabilities of natural language processing (NLP) and machine learning. AES holds the potential to redefine the essay evaluation and grading process, offering efficiency, consistency, and accessibility in a way previously unattainable.\nAt its essence, AES relies on sophisticated algorithms to examine written text, subjecting it to a predefined set of criteria. These criteria encompass various aspects, including grammar, sentence structure, vocabulary, coherence, and the quality of argumentation. In this regard, AES algorithms function akin to meticulous digital assessors, diligently seeking out patterns and features within the text. They assess elements such as the presence of persuasive thesis statements, the adept use of supporting evidence, and the logical flow of ideas within the essay. The result of this is a numerical score that reflects the overall quality of the essay in question.\nToday we will develop a linear regression model to predict essay scores using the famous ASAP dataset. There are eight different essay types in this dataset to explore. Here we will use the essay set 2. Before you continue, please go and read the description and the details on Kaggle. As we will use only one essay set, I have preapared the data as a separate csv file and if you are already familiar with the ASAP data (or you have taken a look at the Kaggle page), please download the data for our use case from here. The packages and some of their functions that we will be using in this topic are:\n\n\n\n\n\n\nA robot drawn by Dall-e\n\n\n\n\n\n\nLibraries\n\nlibrary(\"rmarkdown\") \n# paged_table() \nlibrary(\"readxl\") \n# read_excel()\nlibrary(\"tidyverse\") \nlibrary(\"textstem\") \n# lemmatize_strings()\nlibrary(\"stopwords\") \n# stopwords()\nlibrary(\"stringr\")   \n# str_squish()\nlibrary(\"tm\")        \n# remove_words()\nlibrary(\"quanteda\")  \n# nsentence()"
  },
  {
    "objectID": "posts/Automated Essay Scoring (AES) with R/index.html#preprocessing",
    "href": "posts/Automated Essay Scoring (AES) with R/index.html#preprocessing",
    "title": "Automated Essay Scoring (AES) with R",
    "section": "1. Preprocessing",
    "text": "1. Preprocessing\nNow that you have downloaded the data, move it to your working directory and follow along while reading. Let’s load the data and see the head of them:\n\n\n\n\n\nessay_set_2 &lt;- read_excel(\"essay_set_2.xlsx\") \npaged_table(head(essay_set_2, 2)) # paged_table() function is for a beautified table view on this page. You don't need to use it on your own trial.\n\n\n\n\n  \n\n\n\nThe nine columns are:\n\nessay_id: a unique identifier for the record.\nessay_set: That is set to 2 for all records in this dataset as we are working on essay set 2. We will get rid of this soon.\nessay: The text of the essay written by a real person.\nThere are also 6 more columns that include rater1_domain1, rater2_domain1, domain1_score, rater1_domain2, rater2_domain2, and domain2_score.\n\nLets see a sample essay which is randomly selected:\n\n\n\n\n\n\nset.seed(1234)\nrandom_essay &lt;- sample(1:length(essay_set_2), 1)\npaged_table(essay_set_2[random_essay,3])\n\n\n\n\n  \n\n\n\nWe will use the essay and rater1_domain1 columns for our analysis as we are just doing a research for demonstration purposes (of course, when you are working on a real life research case, there are many things that you have to take into consideration). The essay column is the text of the essay written by a real person and the rater1_domain1 column is the score given to the essay by the first rater. We will rename these columns as response and score respectively. We will also create a new column called doc_id which will be a unique identifier for each essay in our case study.\n\n\n\n\n\nset &lt;- essay_set_2 %&gt;% \n  select(essay, rater1_domain1) %&gt;% \n  rename(response = essay, score = rater1_domain1)\nset$doc_id &lt;- paste0(\"doc\", 1:nrow(set))\n\n\nWhen you scrutinize the essays, you will see that there are some @words in the text. These are the words that are replaced with private information about the students such as names, places or dates etc. Also, there are some non-alphabetic characters and capital letters. In order to make the essays ready to analysis, we will convert all the text to lowercase and remove the non-alphabetic characters. We will also remove the stopwords and lemmatize the text.\n\n\n\n\n\nset$processedResponse &lt;- gsub(\"@\\\\w+ *\", \"\", set$response) #remove @words\nset$processedResponse &lt;- gsub(\"[^a-zA-Z]\", \" \", set$processedResponse) #remove non-alphabetic characters\nset$processedResponse &lt;- tolower(set$processedResponse) #convert to lowercase\nset$processedResponse &lt;- lemmatize_strings(set$processedResponse) #lemmatize\nen_stopwords &lt;- stopwords::stopwords(\"en\", source = \"stopwords-iso\") #get stopwords\nset$processedResponse &lt;- removeWords(set$processedResponse, words = en_stopwords) #remove stopwords\nset$processedResponse &lt;- str_squish(set$processedResponse) #remove extra whitespaces\n\n\nHere how the new processed text looks like (the one we have seen above):\n\n\n\n\n\nsample_essay&lt;-merge(set[random_essay,4], set[random_essay,1])\npaged_table(sample_essay)"
  },
  {
    "objectID": "posts/Automated Essay Scoring (AES) with R/index.html#feature-extraction",
    "href": "posts/Automated Essay Scoring (AES) with R/index.html#feature-extraction",
    "title": "Automated Essay Scoring (AES) with R",
    "section": "2. Feature Extraction",
    "text": "2. Feature Extraction\nFeature extraction is a critical component of Automated Essay Scoring (AES), serving as the foundation upon which the system’s evaluation process is built. In AES, feature extraction procedures involve the identification and analysis of various linguistic and structural elements within the essay. These elements may include but are not limited to word frequency, sentence length, syntactic complexity, vocabulary richness, and the presence of specific content-related markers like thesis statements or evidence citations. The goal of feature extraction is to distill the complex nature of written language into a set of quantifiable, machine-readable attributes that can be used by the AES algorithms to assess the quality and coherence of the essay. Through a combination of natural language processing and statistical analysis, feature extraction empowers AES systems to objectively evaluate essays, providing educators and test administrators with consistent, efficient, and data-driven grading outcomes.\nFor the sake of simplicity of demonstration, we will extract the following features: number of sentences, number of paragraphs and number of contextual words.\n\n\n\n\n\nset$n_sentence &lt;- nsentence(set$response) \nset$n_paragraph &lt;- str_count(set$response, \"     \") + 1 \nset$n_contextWords &lt;- lengths(strsplit(set$processedResponse, ' '))\nhead(set)\n\n\n# A tibble: 6 × 7\n  response  score doc_id processedResponse n_sentence n_paragraph n_contextWords\n  &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;                  &lt;int&gt;       &lt;dbl&gt;          &lt;int&gt;\n1 Certain …     4 doc1   material remove …         20           4            126\n2 Write a …     1 doc2   write persuasive…          3           4             33\n3 Do you t…     2 doc3   library remove m…         15           3             59\n4 In @DATE…     4 doc4   offensive opinio…         31           5            111\n5 In life …     4 doc5   life offensive s…         35          11            131\n6 A lot of…     4 doc6   lot censor conte…         25           5             79\n\n\nAs stated before, there might be many other features that can be extracted from the essays. We will continue our study with these 3 features even if they are not enough for a real life research.\nBefore we start building a machine learning model, we need to divide our dataset as training and test data.\n\n\n\n\n\ndataset &lt;- set\ntrain_indices &lt;- sample(nrow(dataset), nrow(dataset) * 0.7)  # 70% for training\ntrain_data &lt;- dataset[train_indices, ]\ntest_data &lt;- dataset[-train_indices, ]"
  },
  {
    "objectID": "posts/Automated Essay Scoring (AES) with R/index.html#linear-regression-model-lr",
    "href": "posts/Automated Essay Scoring (AES) with R/index.html#linear-regression-model-lr",
    "title": "Automated Essay Scoring (AES) with R",
    "section": "3. Linear Regression Model (LR)",
    "text": "3. Linear Regression Model (LR)\n\nWhat is Linear Regression?\nLinear regression is a statistical model that examines the linear relationship between two (Simple Linear Regression ) or more (Multiple Linear Regression) variables — a dependent variable and independent variable(s). Linear relationship basically means that when one (or more) independent variables increases (or decreases), the dependent variable increases (or decreases) too. Linear regression models are used to show or predict the relationship between two variables or factors. The factor that is being predicted (the factor that the equation solves for) is called the dependent variable. The factors that are used to predict the value of the dependent variable are called the independent variables. The results of a LR model can be evaluated using statistical metrics such as Mean Squared Error (MSE), Root Mean Squared Error (RMSE) and R-squared.\nIn our case, the dependent variable is the score and the independent variables are n_sentence, n_paragraph and n_contextWords.\n\n\n\nLinear Model 1\n\nformula &lt;- as.formula(\"score ~ n_sentence + n_paragraph + n_contextWords\")\nmodel_1 &lt;- lm(formula, data = train_data)\n# Print the summary of the model\nsummary(model_1)\n\n\n\nCall:\nlm(formula = formula, data = train_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.52359 -0.36412  0.00356  0.40190  1.93435 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    2.1864591  0.0418320  52.268  &lt; 2e-16 ***\nn_sentence     0.0224310  0.0027105   8.276 3.23e-16 ***\nn_paragraph    0.0023062  0.0034157   0.675      0.5    \nn_contextWords 0.0076319  0.0005822  13.109  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5595 on 1256 degrees of freedom\nMultiple R-squared:  0.4701,    Adjusted R-squared:  0.4688 \nF-statistic: 371.4 on 3 and 1256 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nPerformance of the Model 1\nLet’s investigate the output from the model one by one.\n\nResiduals: This section provides statistics about the residuals, which are the differences between the observed values and the predicted values by the model.\n\nMin: The minimum residual value is -2.51929.\n1Q: The first quartile (25th percentile) of the residuals is -0.34048.\nMedian: The median of the residuals is 0.00576.\n3Q: The third quartile (75th percentile) of the residuals is 0.37803.\nMax: The maximum residual value is 1.95369.\n\nCoefficients: This section provides information about the coefficients of the linear regression model. Each row corresponds to a predictor variable (independent variable) in the model.\nEstimate: This is the estimated coefficient for each predictor. Std. Error: It represents the standard error of the coefficient estimate. t value: The t-value is a measure of how many standard errors the coefficient estimate is away from zero. Pr(&gt;|t|): This is the p-value associated with the t-value, which tells you whether the coefficient is statistically significant. In our model, we have three predictor variables: n_sentence, n_paragraph, and n_contextWords. The Estimate column represents the estimated coefficients for each of these predictors. The *** symbols indicate that these coefficients are highly statistically significant. In other words, the number of paragraph is not significantly valuable for the model while the number of sentences and the number of contextual words are highly significant.\nMultiple R-squared: This is a measure of how well the model fits the data. It tells you the proportion of the variance in the dependent variable that is explained by the independent variables. In our case, R-squared is approximately 0.4737, which means that about 47.37% of the variance in the dependent variable is explained by our predictors.\nAdjusted R-squared: This is a version of R-squared that adjusts for the number of predictors in the model. It is a slightly more conservative measure of goodness of fit.\nF-statistic: This is a measure of the overall significance of the model. It tests whether at least one of the predictor variables is significantly related to the dependent variable. A high F-statistic and a very low p-value (which is the case here) indicate that the model is significant.\np-value: The p-value associated with the F-statistic. In this case, it’s extremely small, indicating that the overall model is highly significant.\n\n\n\nComparison of Model 1 and Model 2\nLet’s build another model without the feature n_paragraph and compare the results.\n\n\n\nLinear Model 2\n\nformula_2 &lt;- as.formula(\"score ~ n_sentence + n_contextWords\")\nmodel_2 &lt;- lm(formula_2, data = train_data)\n# Print the summary of the model\nsummary(model_2)\n\n\n\nCall:\nlm(formula = formula_2, data = train_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.51924 -0.36470  0.00476  0.39940  1.93587 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    2.195033   0.039849  55.083   &lt;2e-16 ***\nn_sentence     0.022599   0.002698   8.375   &lt;2e-16 ***\nn_contextWords 0.007637   0.000582  13.123   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5594 on 1257 degrees of freedom\nMultiple R-squared:  0.4699,    Adjusted R-squared:  0.4691 \nF-statistic: 557.1 on 2 and 1257 DF,  p-value: &lt; 2.2e-16\n\n\nResidual standard error: The residual standard error is very close in both models, indicating that they have similar predictive accuracy. Multiple R-squared: The R-squared values are also very close. Model 2 has a slightly lower R-squared, but the difference is minimal. F-statistic: Model 2 has a higher F-statistic compared to Model 1, indicating that the predictors collectively have more explanatory power in Model 2.\nIn summary, both models are quite similar in terms of their coefficient estimates and predictive accuracy. Model 2 has a slightly higher F-statistic. Personally I agree with the model. The number of paragraphs should not be a predictor while estimating the score. The number of sentences and the number of contextual words are much more important.\n\n\nEvaluation of Model 2 with the Test Dataset\nAs we would like to continue with the second model, we can use the predict() to make predictions on the test data. The predict() function takes the model and the new data set as arguments. It returns a vector of predictions, which we will save in a new column in the test data set to make comparisons in the next stage. Now, let’s use the mean() and sqrt() functions to calculate the MSE and RMSE of the model. We can also use the summary() function to get the R-squared value of the model.\n\n\n\n\n\n# Make predictions on the test data\npredictions &lt;- predict(model_2, newdata = test_data)\n# Evaluate the model\nmse &lt;- mean((test_data$score - predictions)^2)\nrmse &lt;- sqrt(mse)\nr_squared &lt;- summary(model_2)$r.squared\n# Printing evaluation metrics beautifully :D\ncat(paste0(\"Mean Squared Error (MSE): \", sprintf(\"%.2f\", mse), \"\\n\",\n            \"Root Mean Squared Error (RMSE): \", sprintf(\"%.2f\", rmse), \"\\n\",\n            \"R-squared (R²): \", sprintf(\"%.2f\", r_squared), \"\\n\"))\n\n\nMean Squared Error (MSE): 0.30\nRoot Mean Squared Error (RMSE): 0.54\nR-squared (R²): 0.47\n\n\n\nMSE (Mean Squared Error): MSE is a measure of the average squared difference between the observed (actual) values and the predicted values by the model. It’s a measure of the model’s accuracy, with lower values indicating a better fit.\nRMSE (Root Mean Squared Error): RMSE is the square root of the MSE and provides a measure of the average error in the same units as the dependent variable. It’s a commonly used metric to quantify the prediction error of the model.\nR-squared (R²): R-squared is a measure of how well the independent variables explain the variability in the dependent variable. It ranges from 0 to 1, with higher values indicating a better fit. In the output, the R-squared is approximately 0.4736732, which means that about 47.37% of the variance in the dependent variable is explained by the independent variables in the model.\n\n\n\nPredictions of the Test Dataset\nLet’s take a look at the predictions of the test data. We can use the head() function to print the first 10 predictions.\n\n\n\n\n\n#merge predictions to test data:\ntest_data$predictions &lt;- round(predictions)\nhead(test_data[,c(\"score\",\"predictions\")], 10)\n\n\n# A tibble: 10 × 2\n   score predictions\n   &lt;dbl&gt;       &lt;dbl&gt;\n 1     4           4\n 2     1           3\n 3     2           3\n 4     4           4\n 5     5           4\n 6     2           4\n 7     4           3\n 8     3           3\n 9     3           3\n10     3           2\n\n\nBut looking at the raw data would never be enough to get some insights. Let’s also print the confusion matrix to get a better overall picture. The confusion matrix is a table that is often used to describe the performance of a classification model on a set of test data for which the true values are known. It allows the visualization of the performance of an algorithm. The confusion matrix shows the ways in which the model is confused when it makes predictions. It gives us insight not only into the errors being made by a classifier but more importantly the types of errors that are being made.\n\n\n\n\n\n# Create the confusion matrix and print it\nconfusion_matrix &lt;- table(test_data$predictions, test_data$score)\nconfusion_matrix\n\n\n   \n      1   2   3   4   5   6\n  2   7  13   1   0   0   0\n  3   2  35 186  85   1   0\n  4   0   1  30 148  14   0\n  5   0   0   0   7   8   0\n  6   0   0   0   0   1   1\n\n\nIn this context, the confusion matrix helps us understand the model’s performance in grading essays. It indicates which score points are frequently confused with others, providing insights into where the model might need improvement. For instance, score points 3 and 4 appear to be frequently confused with each other. Yet, I believe that is not a big deal. Confusing a high score such as 6 with a lower score such as 2 would be a catastrophe in AES context, but here we don’t even have one such a case. Besides confusion matrix, one can calculate performance metrics such as accuracy, precision, recall, and F1-score to get a more comprehensive assessment of the model’s grading performance."
  },
  {
    "objectID": "posts/Automated Essay Scoring (AES) with R/index.html#conclusion",
    "href": "posts/Automated Essay Scoring (AES) with R/index.html#conclusion",
    "title": "Automated Essay Scoring (AES) with R",
    "section": "Conclusion",
    "text": "Conclusion\nOur exploration into AES has shed light on the impressive potential of this technology in revolutionizing the evaluation of written content. Our application of a Linear Regression model, even when considering a limited set of extracted features such as the number of words, sentences, and paragraphs, demonstrated the robustness of this approach. It is evident that by harnessing machine learning algorithms, we can achieve consistent and objective grading, streamlining the assessment process for educators and administrators.\nHowever, it’s important to note that Linear Regression is just one of the many models available for AES. The field of automated essay scoring continues to evolve, and researchers are exploring a range of models and techniques to enhance accuracy and broaden the scope of assessment. Some alternatives to LR include Support Vector Machines (SVM), Random Forests, Neural Networks, and Natural Language Processing models like Recurrent Neural Networks (RNNs) or Transformers, such as BERT and GPT-3. These models bring their unique strengths and capabilities to the table, offering a diverse array of tools for essay evaluation.\nAs AES advances, the synergy of these models with increasingly sophisticated feature extraction procedures promises to further elevate the quality and reliability of automated essay scoring. With this, we look to a future where technology and human expertise collaborate seamlessly to offer more efficient, accurate, and comprehensive assessment in the realm of written expression."
  },
  {
    "objectID": "about-me.html",
    "href": "about-me.html",
    "title": "Ali Emre Karagül",
    "section": "",
    "text": "GREETINGS!\n\nYou are visiting my personal blog, so here is a little bit of information about me and my research interests:\nI am passionate about learning and exploring new ideas in the fields of Data Science, Psychometrics, Data Analytics, and Educational Assessment.\nCurrently, I work on my PhD in Educational Assessment and Evaluation. My current research interest is on Automated Essay Scoring, which has led me through machine learning and natural language processing.\nAlways feel free to contact me about any feedback, questions, or maybe just to say “hi”.\n\n\nEDUCATION\n\nGazi University\nFaculty of Education (2022-ongoing) Ph.D. student in the Department of Educational Assessment and Evaluation\n\nAnkara University\nFaculty of Educational Sciences (2016-2020) Master’s Degree in Assessment and Evaluation in Education\n\nGazi University\nFaculty of Education (2008-2014) Bachelor’s degree in the Department of Foreign Language Education"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Psychometrics & Data Visualization",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nAutomated Essay Scoring (AES) with R\n\n\n14 min\n\n\n\npsychometrics\n\n\nBig data\n\n\nnatural language processing\n\n\nautomated essay scoring\n\n\n\nAt the core of Automated Essay Scoring (AES) is natural language processing (NLP) and machine learning. These systems are designed to analyze the text based on a set of…\n\n\n\nOct 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTop 5 Data Sources\n\n\n6 min\n\n\n\npsychometrics\n\n\nBig data\n\n\n\nThis blog post explains some of the sources that I personally prefer to obtain data. I became aware of most of these sources from Google Data Analytics Certificate program.…\n\n\n\nMar 27, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nItem difficulty & discrimination: Exploring the psychometric properties using R\n\n\n9 min\n\n\n\npsychometrics\n\n\nCTT\n\n\ndifficulty\n\n\ndiscrimination\n\n\n\nThis blog post explains the importance of examining item difficulty and discrimination in the development and validation of psychological measures and provides a…\n\n\n\nMar 4, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeb scraping with Rvest package\n\n\n7 min\n\n\n\nRvest\n\n\nWordcloud\n\n\nData-viz\n\n\nnatural language processing\n\n\n\nIn this post, we will delve into harvesting a web page, Ekşi Sözlük. This doesn’t include the automation of the process. Yet, a completed web application for harvesting Ekşi…\n\n\n\nJan 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualisation of My Personal Google Data (2): My exercise routine\n\n\n9 min\n\n\n\nGoogle Data\n\n\nData-viz\n\n\n\nThis post is a part of a series that demonstrates how to gain insights from personal Google data. Its purpose is to show how to visualize personal movement data.\n\n\n\nDec 11, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualisation of My Personal Google Data (1): My Locations\n\n\n12 min\n\n\n\nGoogle Data\n\n\nMaps\n\n\nData-viz\n\n\n\nThis post is a part of a series that demonstrates how to gain insights from personal Google data. Its purpose is to show how to visualize the locations I visited within a…\n\n\n\nNov 16, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStructural Equation Modelling (in Turkish)\n\n\n76 min\n\n\n\npsychometrics\n\n\nSEM\n\n\n\nYEM: ‘Your View on Science’ ölçeğinden elde edilen 2015 PISA Turkiye örneklemi ile bir örnek çalışma. Bu çalışma Gazi üniversitesi’nde ‘Yapısal Eşitlik Modellemeleri’ dersi…\n\n\n\nOct 30, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTest Equating\n\n\n5 min\n\n\n\npsychometrics\n\n\nCTT\n\n\ntest equating\n\n\n\nEquating two test forms: This is a simple test equating study. The data used in this study is simulated from real data. We don’t use the real data for privacy purposes here.\n\n\n\nSep 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nItem Analysis with Item Response Theory (in Turkish)\n\n\n15 min\n\n\n\npsychometrics\n\n\nIRT\n\n\n\nMadde Tepki Kuramı temelli madde analizi. Hem çok kategorili hem de iki kategorili maddeler için MTK temelli madde analizi uygulama süreçleri…\n\n\n\nNov 20, 2021\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Exploring the psychometric properties using R/index.html",
    "href": "posts/Exploring the psychometric properties using R/index.html",
    "title": "Item difficulty & discrimination: Exploring the psychometric properties using R",
    "section": "",
    "text": "Developing and validating psychological measures requires examining their psychometric properties, including their reliability and validity. One aspect of a measure’s validity is its item difficulty, which refers to how easy or difficult each individual item is for respondents to answer correctly. Another important aspect is item discrimination, which measures the extent to which each item distinguishes between participants who have high or low levels of the construct being measured.\nUnderstanding item difficulty and item discrimination is crucial for several reasons. First, items that are too easy or too difficult can limit the variability of responses, making it harder to discriminate between participants who have different levels of the construct being measured. Second, items with low discrimination may not effectively differentiate between participants with different levels of the construct, leading to decreased validity.\nIn this blog post, we’ll explore how to calculate item difficulty and item discrimination in R using an example dataset. We’ll explain what each of these psychometric properties are, why they’re important, and how to interpret the results. We’ll also discuss some limitations and considerations when examining these properties. So let’s get started!\nThe dataset is generated via this web application based on Item Response Theory 2-Parameter Logistic Model. The dataset consists of randomly generated 40 items with a sample size of 500. The a-parameters of the items vary between 0.8 and 1.3 while b-parameters vary between -3 and 3. The c and d parameters are fixed to 0 and 1 respectively for all items.\nHere are the package(s) we use in this post:\n\nlibrary(multilevel)\nlibrary(ShinyItemAnalysis)"
  },
  {
    "objectID": "posts/Exploring the psychometric properties using R/index.html#introduction",
    "href": "posts/Exploring the psychometric properties using R/index.html#introduction",
    "title": "Item difficulty & discrimination: Exploring the psychometric properties using R",
    "section": "",
    "text": "Developing and validating psychological measures requires examining their psychometric properties, including their reliability and validity. One aspect of a measure’s validity is its item difficulty, which refers to how easy or difficult each individual item is for respondents to answer correctly. Another important aspect is item discrimination, which measures the extent to which each item distinguishes between participants who have high or low levels of the construct being measured.\nUnderstanding item difficulty and item discrimination is crucial for several reasons. First, items that are too easy or too difficult can limit the variability of responses, making it harder to discriminate between participants who have different levels of the construct being measured. Second, items with low discrimination may not effectively differentiate between participants with different levels of the construct, leading to decreased validity.\nIn this blog post, we’ll explore how to calculate item difficulty and item discrimination in R using an example dataset. We’ll explain what each of these psychometric properties are, why they’re important, and how to interpret the results. We’ll also discuss some limitations and considerations when examining these properties. So let’s get started!\nThe dataset is generated via this web application based on Item Response Theory 2-Parameter Logistic Model. The dataset consists of randomly generated 40 items with a sample size of 500. The a-parameters of the items vary between 0.8 and 1.3 while b-parameters vary between -3 and 3. The c and d parameters are fixed to 0 and 1 respectively for all items.\nHere are the package(s) we use in this post:\n\nlibrary(multilevel)\nlibrary(ShinyItemAnalysis)"
  },
  {
    "objectID": "posts/Exploring the psychometric properties using R/index.html#item-difficulty",
    "href": "posts/Exploring the psychometric properties using R/index.html#item-difficulty",
    "title": "Item difficulty & discrimination: Exploring the psychometric properties using R",
    "section": "Item Difficulty",
    "text": "Item Difficulty\nItem difficulty is a psychometric property that measures how easy or difficult an item is for respondents to answer correctly. Examining item difficulty is important because it can help identify items that are too easy or too difficult, which can limit the variability of responses and make it harder to discriminate between participants who have different levels of the construct being measured.\nThe proportion of correct responses for each item is calculated and reported as the item difficulty value. This calculation can be done manually using spreadsheet software or programmatically using statistical software such as R or SPSS. R has many packages and functions out there that we can use to calculate item difficulties. Yet we will calculate them simply with colMeans() function.\nLet’s start by introducing the dataset to R environment.\n\nmy_data&lt;-read.csv(\"data for post about item difficulty and discrimination.csv\",sep=\";\", header = TRUE)\nhead(my_data) \n\n  i1 i2 i3 i4 i5 i6 i7 i8 i9 i10 i11 i12 i13 i14 i15 i16 i17 i18 i19 i20 i21\n1  0  1  1  1  0  0  1  0  0   1   1   0   1   1   1   1   0   0   0   0   0\n2  0  1  1  1  0  0  1  0  0   1   0   0   1   0   1   1   1   1   0   1   0\n3  0  1  1  1  0  0  1  1  1   1   1   0   1   1   1   1   1   1   1   1   0\n4  1  1  0  0  0  0  0  0  0   1   0   0   1   1   0   1   1   0   0   0   0\n5  0  1  1  1  0  0  0  0  1   1   1   1   1   1   1   1   1   1   0   0   0\n6  0  1  1  1  1  1  1  1  1   1   1   1   1   1   0   1   1   0   0   0   1\n  i22 i23 i24 i25 i26 i27 i28 i29 i30 i31 i32 i33 i34 i35 i36 i37 i38 i39 i40\n1   0   0   1   1   1   0   1   0   0   0   1   0   1   0   1   0   1   1   0\n2   0   0   1   1   1   0   0   0   0   0   0   1   1   0   1   0   0   1   1\n3   0   0   1   1   1   0   1   0   0   0   1   1   1   1   1   0   1   1   1\n4   0   0   0   1   1   0   1   0   0   0   0   0   1   0   0   0   0   1   1\n5   0   0   1   1   1   1   0   0   0   0   1   1   1   1   1   0   0   1   1\n6   0   0   1   1   1   0   1   1   0   0   1   1   1   1   1   1   1   1   1\n\n\nNow let’s get the item difficulties:\n\n# Calculate item difficulty\nitem_difficulty &lt;- colMeans(my_data)\nitem_difficulty\n\n   i1    i2    i3    i4    i5    i6    i7    i8    i9   i10   i11   i12   i13 \n0.212 0.918 0.910 0.782 0.112 0.394 0.742 0.400 0.862 0.870 0.774 0.386 0.882 \n  i14   i15   i16   i17   i18   i19   i20   i21   i22   i23   i24   i25   i26 \n0.682 0.594 0.928 0.722 0.384 0.174 0.346 0.512 0.176 0.100 0.682 0.900 0.882 \n  i27   i28   i29   i30   i31   i32   i33   i34   i35   i36   i37   i38   i39 \n0.264 0.536 0.148 0.044 0.046 0.918 0.638 0.946 0.610 0.800 0.146 0.566 0.928 \n  i40 \n0.952 \n\n\nInterpreting the results of item difficulty is straightforward. Items with higher difficulty values indicate that they were easier for participants to answer correctly, while items with lower difficulty values were more difficult. In our example dataset, the output shows that item 40 had the highest difficulty value of 0.952, meaning that 95.2% of participants answered this item correctly. Item 30, on the other hand, had the lowest difficulty value of 0.044, meaning that only 4.4% of participants answered this item correctly.\nIt’s important to note that each construct should be evaluated within its own concept while interpreting item difficulties. Yet, for achievement tests, a generic classification might be defined as “easy” if the index is 0.85 or above; “moderate” if it is between 0.41 and 0.84; and “hard” if it is 0.40 or below. Also, item difficulty is not the only factor to consider when evaluating the quality of a measure. Items that are too easy or too difficult may still be valid and reliable, depending on the construct being measured and the purpose of the measure. However, examining item difficulty can provide valuable insights into the psychometric properties of the measure and inform decisions about item selection and revision."
  },
  {
    "objectID": "posts/Exploring the psychometric properties using R/index.html#item-discrimination",
    "href": "posts/Exploring the psychometric properties using R/index.html#item-discrimination",
    "title": "Item difficulty & discrimination: Exploring the psychometric properties using R",
    "section": "Item Discrimination",
    "text": "Item Discrimination\nItem discrimination is another important psychometric property that measures the extent to which each item differentiates between participants who have high or low levels of the construct being measured. It indicates how well an item distinguishes between participants with different levels of the construct.\nIt’s important to note that (just like item difficulties) each construct should be evaluated within its own concept while interpreting item discriminations. Yet, for achievement tests, a generic classification might be defined as “good” if the index is above 0.30; “fair” if it is between 0.10 and 0.30; and “poor” if it is below 0.10.\nTo obtain a value for item discrimination, there are several statistical approaches that we can utilize. Here we will discuss (1) correlation between item and total score with the item, (2) correlation between item and total score without the item, and (3) upper-lower groups index.\n\n1. Correlation between item and total score with the item\nThis approach is based on calculating the point-biserial correlation coefficient (rpb) between each item and the total score of the measure. The total score is calculated by summing the scores of all items. The rpb ranges from -1 to 1, with values closer to 1 indicating higher discrimination.\nLet’s first calculate the total score for each participant in our example dataset. Then, use a for loop to calculate rpb coefficients for each item:\n\n#get the total score for each participant\ntotal_score &lt;- rowSums(my_data)\n\n#There are 40 items in the test:\nitem_discrimination1 &lt;- 40  \n\n#calculate rpb for each item:\nfor(i in 1:40){        \n  item_discrimination1[i] &lt;- cor(total_score, my_data[,i])  \n}\nround(item_discrimination1,4)\n\n [1] 0.3984 0.2162 0.2308 0.4485 0.3019 0.4847 0.4148 0.4548 0.2796 0.3335\n[11] 0.4498 0.5138 0.2735 0.4336 0.4423 0.1956 0.4288 0.4133 0.3297 0.4214\n[21] 0.4909 0.3174 0.3222 0.4098 0.3312 0.3464 0.3737 0.4876 0.2481 0.2257\n[31] 0.1783 0.3536 0.4094 0.2837 0.3893 0.3888 0.3962 0.5025 0.2866 0.1906\n\n\n\n\n2. Correlation between item and total score without the item\nThis approach is very similar to the first one. The only difference is that when we calculate the correlation between an item and the total score, we do not include the item. This type of an approach will result in slightly reduced index values when compared to the first approach. Therefore, it is usually a more-preferred approach by test developers (we would love to stay in the safe-zone).\nThe package multilevel has a specific function for this index. The item.total() function takes only the dataset as input and provides us with a dataframe with four columns: item name, discrimination index, a reliability index without the item and the sample size. Yet, we only need the discrimination index. Here how we get it:\n\nitem_discrimination2&lt;-multilevel::item.total(my_data)\nitem_discrimination2$Item.Total\n\n [1] 0.3365024 0.1703597 0.1832279 0.3887597 0.2511883 0.4160659 0.3493047\n [8] 0.3837360 0.2232991 0.2803945 0.3892913 0.4477891 0.2207879 0.3648121\n[15] 0.3701690 0.1521115 0.3624807 0.3399690 0.2694241 0.3503333 0.4211616\n[22] 0.2564267 0.2746220 0.3395723 0.2838892 0.2959223 0.3053936 0.4177549\n[29] 0.1892085 0.1916937 0.1429237 0.3110586 0.3368093 0.2471547 0.3143658\n[36] 0.3277659 0.3428951 0.4342855 0.2447900 0.1546114\n\n\n\n\n3. Upper-lower groups index\nPersonally, I feel that this approach is the most meaningfully-related approach in terms of “discrimination”. That’s because while calculating it, we divide the whole group into sub-groups (usually three groups) according to their total scores. Then, we calculate the discrimination index for an item by comparing these groups’ responses to that item. This definition feels more like a discrimination index to my illiterate ears.\nIn R environment, ShinyItemAnalysis package has a specific function for this index. The gDiscrim() function has several arguements such as Data(the data set), k (the number of sub groups and 3 is default), l and u (numeric values to define the lower and upper groups and the defaults are 1 and 3 consecutively). There are other arguments that should be checked on the manual before using the function.\nWe simply use the function as:\n\nitem_discrimination3&lt;-ShinyItemAnalysis::gDiscrim(my_data)\nitem_discrimination3\n\n        i1         i2         i3         i4         i5         i6         i7 \n0.35177700 0.11253051 0.12801251 0.42537370 0.20042709 0.57908786 0.40470561 \n        i8         i9        i10        i11        i12        i13        i14 \n0.56204240 0.19718578 0.26586333 0.43982611 0.56566504 0.21888347 0.50640635 \n       i15        i16        i17        i18        i19        i20        i21 \n0.49603417 0.11253051 0.45530811 0.44428768 0.26346095 0.46442190 0.56829622 \n       i22        i23        i24        i25        i26        i27        i28 \n0.26449054 0.20145668 0.43875839 0.20908328 0.25709274 0.36420836 0.59567572 \n       i29        i30        i31        i32        i33        i34        i35 \n0.17720409 0.08419768 0.07748627 0.19100824 0.45732916 0.12957596 0.41599298 \n       i36        i37        i38        i39        i40 \n0.37633466 0.29804759 0.54766626 0.13163514 0.09601891 \n\n\nNote that if you change the number of subgroups, you should be careful while interpreting the results.\nNo matter what statistical approach you use to estimate discrimination indexes, it’s also important to note that item discrimination can be influenced by factors such as the sample size, the range of scores, and the homogeneity of the sample. Therefore, it’s recommended to examine item discrimination in conjunction with other psychometric properties such as item difficulty and reliability."
  },
  {
    "objectID": "posts/Exploring the psychometric properties using R/index.html#a-package-of-personal-preferrance",
    "href": "posts/Exploring the psychometric properties using R/index.html#a-package-of-personal-preferrance",
    "title": "Item difficulty & discrimination: Exploring the psychometric properties using R",
    "section": "A Package of Personal Preferrance",
    "text": "A Package of Personal Preferrance\nWhile we can calculate both difficulty and discrimination indexes manually or by using different functions from different packages, my totally-subjective opinion is that ItemAnalysis() function from the ShinyItemAnalysis package gives a well-groomed output for many item statistics. The following code snippet simply provides us with many indexes including difficulty and three types of discrimination indexes:\n\n#round is for rounding the values in the results.\nitem_stats&lt;-round(ShinyItemAnalysis::ItemAnalysis(my_data),2)\n#to see all the output in a table:\nknitr::kable(item_stats)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDifficulty\nMean\nSD\nCut.score\nobs.min\nMin.score\nobs.max\nMax.score\nProp.max.score\nRIR\nRIT\nCorr.criterion\nULI\ngULI\nAlpha.drop\nIndex.rel\nIndex.val\nPerc.miss\nPerc.nr\n\n\n\n\ni1\n0.21\n0.21\n0.41\nNA\n0\n0\n1\n1\n0.21\n0.34\n0.40\nNA\n0.35\nNA\n0.83\n0.16\nNA\n0\n0\n\n\ni2\n0.92\n0.92\n0.27\nNA\n0\n0\n1\n1\n0.92\n0.17\n0.22\nNA\n0.11\nNA\n0.84\n0.06\nNA\n0\n0\n\n\ni3\n0.91\n0.91\n0.29\nNA\n0\n0\n1\n1\n0.91\n0.18\n0.23\nNA\n0.13\nNA\n0.84\n0.07\nNA\n0\n0\n\n\ni4\n0.78\n0.78\n0.41\nNA\n0\n0\n1\n1\n0.78\n0.39\n0.45\nNA\n0.43\nNA\n0.83\n0.19\nNA\n0\n0\n\n\ni5\n0.11\n0.11\n0.32\nNA\n0\n0\n1\n1\n0.11\n0.25\n0.30\nNA\n0.20\nNA\n0.84\n0.10\nNA\n0\n0\n\n\ni6\n0.39\n0.39\n0.49\nNA\n0\n0\n1\n1\n0.39\n0.42\n0.48\nNA\n0.58\nNA\n0.83\n0.24\nNA\n0\n0\n\n\ni7\n0.74\n0.74\n0.44\nNA\n0\n0\n1\n1\n0.74\n0.35\n0.41\nNA\n0.40\nNA\n0.83\n0.18\nNA\n0\n0\n\n\ni8\n0.40\n0.40\n0.49\nNA\n0\n0\n1\n1\n0.40\n0.38\n0.45\nNA\n0.56\nNA\n0.83\n0.22\nNA\n0\n0\n\n\ni9\n0.86\n0.86\n0.35\nNA\n0\n0\n1\n1\n0.86\n0.22\n0.28\nNA\n0.20\nNA\n0.84\n0.10\nNA\n0\n0\n\n\ni10\n0.87\n0.87\n0.34\nNA\n0\n0\n1\n1\n0.87\n0.28\n0.33\nNA\n0.27\nNA\n0.83\n0.11\nNA\n0\n0\n\n\ni11\n0.77\n0.77\n0.42\nNA\n0\n0\n1\n1\n0.77\n0.39\n0.45\nNA\n0.44\nNA\n0.83\n0.19\nNA\n0\n0\n\n\ni12\n0.39\n0.39\n0.49\nNA\n0\n0\n1\n1\n0.39\n0.45\n0.51\nNA\n0.57\nNA\n0.83\n0.25\nNA\n0\n0\n\n\ni13\n0.88\n0.88\n0.32\nNA\n0\n0\n1\n1\n0.88\n0.22\n0.27\nNA\n0.22\nNA\n0.84\n0.09\nNA\n0\n0\n\n\ni14\n0.68\n0.68\n0.47\nNA\n0\n0\n1\n1\n0.68\n0.36\n0.43\nNA\n0.51\nNA\n0.83\n0.20\nNA\n0\n0\n\n\ni15\n0.59\n0.59\n0.49\nNA\n0\n0\n1\n1\n0.59\n0.37\n0.44\nNA\n0.50\nNA\n0.83\n0.22\nNA\n0\n0\n\n\ni16\n0.93\n0.93\n0.26\nNA\n0\n0\n1\n1\n0.93\n0.15\n0.20\nNA\n0.11\nNA\n0.84\n0.05\nNA\n0\n0\n\n\ni17\n0.72\n0.72\n0.45\nNA\n0\n0\n1\n1\n0.72\n0.36\n0.43\nNA\n0.46\nNA\n0.83\n0.19\nNA\n0\n0\n\n\ni18\n0.38\n0.38\n0.49\nNA\n0\n0\n1\n1\n0.38\n0.34\n0.41\nNA\n0.44\nNA\n0.83\n0.20\nNA\n0\n0\n\n\ni19\n0.17\n0.17\n0.38\nNA\n0\n0\n1\n1\n0.17\n0.27\n0.33\nNA\n0.26\nNA\n0.84\n0.13\nNA\n0\n0\n\n\ni20\n0.35\n0.35\n0.48\nNA\n0\n0\n1\n1\n0.35\n0.35\n0.42\nNA\n0.46\nNA\n0.83\n0.20\nNA\n0\n0\n\n\ni21\n0.51\n0.51\n0.50\nNA\n0\n0\n1\n1\n0.51\n0.42\n0.49\nNA\n0.57\nNA\n0.83\n0.25\nNA\n0\n0\n\n\ni22\n0.18\n0.18\n0.38\nNA\n0\n0\n1\n1\n0.18\n0.26\n0.32\nNA\n0.26\nNA\n0.84\n0.12\nNA\n0\n0\n\n\ni23\n0.10\n0.10\n0.30\nNA\n0\n0\n1\n1\n0.10\n0.27\n0.32\nNA\n0.20\nNA\n0.84\n0.10\nNA\n0\n0\n\n\ni24\n0.68\n0.68\n0.47\nNA\n0\n0\n1\n1\n0.68\n0.34\n0.41\nNA\n0.44\nNA\n0.83\n0.19\nNA\n0\n0\n\n\ni25\n0.90\n0.90\n0.30\nNA\n0\n0\n1\n1\n0.90\n0.28\n0.33\nNA\n0.21\nNA\n0.83\n0.10\nNA\n0\n0\n\n\ni26\n0.88\n0.88\n0.32\nNA\n0\n0\n1\n1\n0.88\n0.30\n0.35\nNA\n0.26\nNA\n0.83\n0.11\nNA\n0\n0\n\n\ni27\n0.26\n0.26\n0.44\nNA\n0\n0\n1\n1\n0.26\n0.31\n0.37\nNA\n0.36\nNA\n0.83\n0.16\nNA\n0\n0\n\n\ni28\n0.54\n0.54\n0.50\nNA\n0\n0\n1\n1\n0.54\n0.42\n0.49\nNA\n0.60\nNA\n0.83\n0.24\nNA\n0\n0\n\n\ni29\n0.15\n0.15\n0.36\nNA\n0\n0\n1\n1\n0.15\n0.19\n0.25\nNA\n0.18\nNA\n0.84\n0.09\nNA\n0\n0\n\n\ni30\n0.04\n0.04\n0.21\nNA\n0\n0\n1\n1\n0.04\n0.19\n0.23\nNA\n0.08\nNA\n0.84\n0.05\nNA\n0\n0\n\n\ni31\n0.05\n0.05\n0.21\nNA\n0\n0\n1\n1\n0.05\n0.14\n0.18\nNA\n0.08\nNA\n0.84\n0.04\nNA\n0\n0\n\n\ni32\n0.92\n0.92\n0.27\nNA\n0\n0\n1\n1\n0.92\n0.31\n0.35\nNA\n0.19\nNA\n0.83\n0.10\nNA\n0\n0\n\n\ni33\n0.64\n0.64\n0.48\nNA\n0\n0\n1\n1\n0.64\n0.34\n0.41\nNA\n0.46\nNA\n0.83\n0.20\nNA\n0\n0\n\n\ni34\n0.95\n0.95\n0.23\nNA\n0\n0\n1\n1\n0.95\n0.25\n0.28\nNA\n0.13\nNA\n0.84\n0.06\nNA\n0\n0\n\n\ni35\n0.61\n0.61\n0.49\nNA\n0\n0\n1\n1\n0.61\n0.31\n0.39\nNA\n0.42\nNA\n0.83\n0.19\nNA\n0\n0\n\n\ni36\n0.80\n0.80\n0.40\nNA\n0\n0\n1\n1\n0.80\n0.33\n0.39\nNA\n0.38\nNA\n0.83\n0.16\nNA\n0\n0\n\n\ni37\n0.15\n0.15\n0.35\nNA\n0\n0\n1\n1\n0.15\n0.34\n0.40\nNA\n0.30\nNA\n0.83\n0.14\nNA\n0\n0\n\n\ni38\n0.57\n0.57\n0.50\nNA\n0\n0\n1\n1\n0.57\n0.43\n0.50\nNA\n0.55\nNA\n0.83\n0.25\nNA\n0\n0\n\n\ni39\n0.93\n0.93\n0.26\nNA\n0\n0\n1\n1\n0.93\n0.24\n0.29\nNA\n0.13\nNA\n0.84\n0.07\nNA\n0\n0\n\n\ni40\n0.95\n0.95\n0.21\nNA\n0\n0\n1\n1\n0.95\n0.15\n0.19\nNA\n0.10\nNA\n0.84\n0.04\nNA\n0\n0\n\n\n\n\n\nThe same package also provides us with a nice visualization function (DDplot) for item difficulty and discrimination of any approach stated above. It takes discrim argument that can be defined as RIT (the first approach), RIR (the second approach above) or ULI (the third approach). Also, you can define a threshold value to draw a line on the plot via the thr argument. Here is a sample usage for our case:\n\nDDplot(my_data, discrim = 'ULI', k = 3, l = 1, u = 3, thr=0.1)\n\n\n\n\nIt can be seen that items 30, 31 and 40 from our simulated dataset are below our 0.1 threshold value in terms of discrimination. Interestingly, items 30 and 31 are the most difficult items while item 40 is the easiest one. What a weirdo… :D"
  },
  {
    "objectID": "posts/Exploring the psychometric properties using R/index.html#conclusion",
    "href": "posts/Exploring the psychometric properties using R/index.html#conclusion",
    "title": "Item difficulty & discrimination: Exploring the psychometric properties using R",
    "section": "Conclusion",
    "text": "Conclusion\nExamining item difficulty and item discrimination are important aspects of evaluating the psychometric properties of a measure. Item difficulty measures how easy or difficult each individual item is for respondents to answer correctly, while item discrimination measures the extent to which each item differentiates between participants who have high or low levels of the construct being measured.\nIn this blog post, we explored how to calculate item difficulty and item discrimination in R using an example dataset. We explained what each of these psychometric properties are, why they’re important, and how to interpret the results. We also discussed some limitations and considerations when examining these properties.\nOverall, understanding and evaluating the psychometric properties of a measure can help ensure its reliability and validity, and inform decisions about item selection and revision."
  },
  {
    "objectID": "posts/Structural Equation Modelling (in Turkish)/index.html",
    "href": "posts/Structural Equation Modelling (in Turkish)/index.html",
    "title": "Structural Equation Modelling (in Turkish)",
    "section": "",
    "text": "Bu çalışmada PISA 2015 “student questionnaire: paper based verison” içerisindeki en son alt ölçek olan ‘Your View on Science’ ölçeğinden elde edilen veriler kullanılmıştır. Türkiye örnekleminden faydalanılmıştır. Bu örneklem ile bahsi geçen ölçeğin yapısal eşitlik modelleri oluşturulmuş ve karşılaştırılmıştır. Adım adım veri setinin R ile analize hazır hale getirilmesi anlatılmaktadır. Doğrudan analiz ile ilgileniyorsanız aşağıda ‘Analiz Süreci’ başlığına ilerleyiniz."
  },
  {
    "objectID": "posts/Structural Equation Modelling (in Turkish)/index.html#pre-processing-süreci",
    "href": "posts/Structural Equation Modelling (in Turkish)/index.html#pre-processing-süreci",
    "title": "Structural Equation Modelling (in Turkish)",
    "section": "Pre-processing süreci",
    "text": "Pre-processing süreci\nÖncelikle OECD tarafından yayınlanan veri setini şuradan indiriyoruz. İndirilen bu dosya .sav uzantılı bbir dosya. Bu tür dosyaları açmak için haven paketi read_sav fonksiyonundan faydalanabiliriz. Veri setinin tamamına ihtiyacımız yok, zaten oldukça büyük bir veri. Ülke değilşkeni ve ilgili ölçeğin maddeleri yeterli olacaktır. İndirdiğimiz .sav uzantılı dosyayı working directory’mize taşıdıktan sonra çalıştıracağımız kod satırı şunlar:\n\n\nCode\nlibrary(haven) \ndata &lt;- na.omit(read_sav(\n  \"CY6_MS_CMB_STU_QQQ.sav\",\n  col_select = c(\n    \"CNT\",\n    \"ST092Q01TA\",\n    \"ST092Q02TA\",\n    \"ST092Q04TA\",\n    \"ST092Q05TA\",\n    \"ST092Q06NA\",\n    \"ST092Q08NA\",\n    \"ST092Q09NA\",\n    \n    \"ST094Q01NA\",\n    \"ST094Q02NA\",\n    \"ST094Q03NA\",\n    \"ST094Q04NA\",\n    \"ST094Q05NA\",\n    \n    \"ST113Q01TA\",\n    \"ST113Q02TA\",\n    \"ST113Q03TA\",\n    \"ST113Q04TA\",\n    \n    \"ST129Q01TA\",\n    \"ST129Q02TA\",\n    \"ST129Q03TA\",\n    \"ST129Q04TA\",\n    \"ST129Q05TA\",\n    \"ST129Q06TA\",\n    \"ST129Q07TA\",\n    \"ST129Q08TA\",\n    \n    \"ST131Q01NA\",\n    \"ST131Q03NA\",\n    \"ST131Q04NA\",\n    \"ST131Q06NA\",\n    \"ST131Q08NA\",\n    \"ST131Q11NA\"\n  )\n))\n\n\nÜlke değişkenine artık ihtiyacımız yok. O nedenle veri setimizi sadece ölçek maddelerini içerecek şekilde yeniden tanımlayalım:\n\n\nCode\nlibrary(dplyr)\nscience_data&lt;-filter(data, CNT==\"TUR\")[,-1]\n\n\nBu veri setini şu kod satırını yürüterek .csv uzantılı bir dosya olarak bilgisayarıma kaydediyorum. Böylece analiz aşamasında o halini de paylaşabileceğim:\n\n\nCode\nwrite.csv(science_data, \"science_data.csv\", row.names = FALSE)"
  },
  {
    "objectID": "posts/Structural Equation Modelling (in Turkish)/index.html#analiz-süreci",
    "href": "posts/Structural Equation Modelling (in Turkish)/index.html#analiz-süreci",
    "title": "Structural Equation Modelling (in Turkish)",
    "section": "Analiz Süreci",
    "text": "Analiz Süreci\nİlgili ölçekten elde edilen veriyi .csv uzantılı dosya olarak indirebilirsiniz. Bunu read.csv fonksiyonu ile R ortamına aktarabilirsiniz. Veri setimiz hazır. Analizimizde öncelikle doğrulayı faktör analizi kullanacağız. Daha sonra da bi-faktör modelleme yapacağız. Veri setimizi tekrar yükleyelim:\n\nDoğrulayıcı Faktör Analizi (DFA)\nÇalışmamıza konu olan ölçek beş faktörden oluşmaktadır. Bu yapı ile ölçek geliştirme sürecinde tanımlanmıştır. Her faktörde farklı sayılarda maddeler yer almaktadır. Veri setinde bunlar ST ön eki ve faktör numarası ile tanımlanmıştır. Faktörleri ve veri setindeki kodlamalarını şu şekilde listeleyelim:\n\nST092: How informed are you about the following environmental issues?\nST094: How much do you disagree or agree with the statements about yourself below?\nST113: How much do you agree with the statements below?\nST129: How easy do you think it would be for you to perform the following tasks on your own?\nST131: How much do you disagree or agree with the statements below?\n\nBu beş faktörün altında tanımlanan yapıya göre DFA uygulayacağız ve bir geçerlilik çalışması yürüteceğiz. Bunun için öncelikle modelimizi R’a tanıtalım:\n\n\nCode\nScience.model &lt;- 'ST092 =~ 1* ST092Q01TA+\n                              ST092Q02TA+\n                              ST092Q04TA+\n                              ST092Q05TA+\n                              ST092Q06NA+\n                              ST092Q08NA+\n                              ST092Q09NA\n                              \n                  ST094 =~ 1* ST094Q01NA+\n                              ST094Q02NA+\n                              ST094Q03NA+\n                              ST094Q04NA+\n                              ST094Q05NA\n                              \n                  ST113 =~ 1* ST113Q01TA+\n                              ST113Q02TA+\n                              ST113Q03TA+\n                              ST113Q04TA\n                  \n                  ST129 =~ 1* ST129Q01TA+\n                              ST129Q02TA+\n                              ST129Q03TA+\n                              ST129Q04TA+\n                              ST129Q05TA+\n                              ST129Q06TA+\n                              ST129Q07TA+\n                              ST129Q08TA\n                              \n                  ST131 =~ 1* ST131Q01NA+\n                              ST131Q03NA+\n                              ST131Q04NA+\n                              ST131Q06NA+\n                              ST131Q08NA+\n                              ST131Q11NA            \n                    \n                           \n                            ST092 ~~ ST094\n                            ST092 ~~ ST113\n                            ST092 ~~ ST129\n                            ST092 ~~ ST131\n                            \n                            ST094 ~~ ST113\n                            ST094 ~~ ST129\n                            ST094 ~~ ST131\n                            \n                            ST113 ~~ ST129\n                            ST113 ~~ ST131\n                            \n                            ST129 ~~ ST131'\n\n\nŞimdi Türkiye örnekleminden elde edilen verimizin bu modele uyum sağlayıp sağlamadığına bakacağız. Bu aşamada çeşitli modelleme yaklaşımlarından çıktılar alarak bunları karşılaştıracağız. Bu modelleme yaklaşımları şunlardır:\n\nmaximum likelihood model (MLM)\nweighted least squares (WLS)\nrobust maximum likelihood model (RMLM)\ndiagonally weighted least squares (DWLS)\n\nYukarıdaki lsitede görüldüğü sıra ile modellerimizi lavaan paketiyle oluşturalım:\n\n\nCode\nlibrary(lavaan)\nmodel_mlm &lt;- cfa(Science.model, data = science_data)\nmodel_wls &lt;- cfa(Science.model, WLS.V = TRUE, data = science_data)\nmodel_rml &lt;- cfa(Science.model, estimator = \"MLM\", se = \"robust.mlm\", data = science_data)\nmodel_dwls&lt;-cfa(Science.model, data = science_data, estimator=\"DWLS\")\n\n\nHer biri için de ayrı ayrı analiz çıktılarını summary() fonksiyonu ile tanımlayalım:\n\n\nCode\nscience.mlm&lt;-summary(\n  model_mlm,\n  fit.measures = TRUE,\n  standardized = TRUE,\n  rsquare = TRUE,\n  modindices = TRUE\n)\nscience.wls&lt;-summary(\n  model_wls,\n  fit.measures = TRUE,\n  standardized = TRUE,\n  rsquare = TRUE,\n  modindices = TRUE\n)\nscience.rml&lt;-summary(\n  model_rml,\n  fit.measures = TRUE,\n  standardized = TRUE,\n  rsquare = TRUE,\n  modindices = TRUE\n)\nscience.dwls&lt;-summary(\n  model_dwls,\n  fit.measures = TRUE,\n  standardized = TRUE,\n  rsquare = TRUE,\n  modindices = TRUE\n)\n\n\nTabi ki hem literatürde en yaygın kullanılan hem de alan uzmanları tarafından en ok önerilen yöntem olması sebebiyle MLM yöntemi önceliğimiz. Bu yöntemde normallik varsayımı karşılandığı sürece güçlü analizler elde edilebilmektedir. Veri setimiz de büyük bir örneklemden elde edildiği için bu varsayımın karşılandığı düşünülmektedir. Grafik incelemelerinde de bu durum görülecektir. Dolayısıyla öncelikle MLM’ye ait uyum indekslerini görelim:\n\n\nCode\nscience.mlm[[\"fit\"]]\n\n\n\n\nCode\nlibrary(lavaan)\nScience.model &lt;- 'ST092 =~ 1* ST092Q01TA+\n                              ST092Q02TA+\n                              ST092Q04TA+\n                              ST092Q05TA+\n                              ST092Q06NA+\n                              ST092Q08NA+\n                              ST092Q09NA\n                              \n                  ST094 =~ 1* ST094Q01NA+\n                              ST094Q02NA+\n                              ST094Q03NA+\n                              ST094Q04NA+\n                              ST094Q05NA\n                              \n                  ST113 =~ 1* ST113Q01TA+\n                              ST113Q02TA+\n                              ST113Q03TA+\n                              ST113Q04TA\n                  \n                  ST129 =~ 1* ST129Q01TA+\n                              ST129Q02TA+\n                              ST129Q03TA+\n                              ST129Q04TA+\n                              ST129Q05TA+\n                              ST129Q06TA+\n                              ST129Q07TA+\n                              ST129Q08TA\n                              \n                  ST131 =~ 1* ST131Q01NA+\n                              ST131Q03NA+\n                              ST131Q04NA+\n                              ST131Q06NA+\n                              ST131Q08NA+\n                              ST131Q11NA            \n                    \n                           \n                            ST092 ~~ ST094\n                            ST092 ~~ ST113\n                            ST092 ~~ ST129\n                            ST092 ~~ ST131\n                            \n                            ST094 ~~ ST113\n                            ST094 ~~ ST129\n                            ST094 ~~ ST131\n                            \n                            ST113 ~~ ST129\n                            ST113 ~~ ST131\n                            \n                            ST129 ~~ ST131'\nmodel_mlm &lt;- cfa(Science.model, data = science_data)\nscience.mlm&lt;-summary(\n  model_mlm,\n  fit.measures = TRUE,\n  standardized = TRUE,\n  rsquare = TRUE,\n  modindices = TRUE\n)\nscience.mlm[[\"fit\"]]\n\n\n             npar              fmin             chisq                df \n           70.000             0.652          6055.193           395.000 \n           pvalue    baseline.chisq       baseline.df   baseline.pvalue \n            0.000         96221.017           435.000             0.000 \n              cfi               tli              logl unrestricted.logl \n            0.941             0.935       -135531.110       -132503.514 \n              aic               bic            ntotal              bic2 \n       271202.220        271653.284          4646.000        271430.850 \n            rmsea    rmsea.ci.lower    rmsea.ci.upper      rmsea.pvalue \n            0.056             0.054             0.057             0.000 \n             srmr \n            0.039 \n\n\nBu çıktılar incelendiğinde, 70 parametreli 395 serbestlik derecesinde bir model oluştuğu görülmektedir. CFI ve TLI uyum indekleri .90 eşik değerin üzerindeyken, RMSEA ve SRMR .06’nın altında yer almaktadır. Bu durumda modelimizin uyumlu olduğu düşünülebilir. Yine de diğer modeller ile de karşılaştırmak gerekir. Onların uyum indeklerini de görelim:\nWLS model:\n\n\nCode\nscience.wls[[\"fit\"]]\n\n\n\n\n             npar              fmin             chisq                df \n           70.000             0.652          6055.193           395.000 \n           pvalue    baseline.chisq       baseline.df   baseline.pvalue \n            0.000         96221.017           435.000             0.000 \n              cfi               tli              logl unrestricted.logl \n            0.941             0.935       -135531.110       -132503.514 \n              aic               bic            ntotal              bic2 \n       271202.220        271653.284          4646.000        271430.850 \n            rmsea    rmsea.ci.lower    rmsea.ci.upper      rmsea.pvalue \n            0.056             0.054             0.057             0.000 \n             srmr \n            0.039 \n\n\nRML model:\n\n\nCode\nscience.rml[[\"fit\"]]\n\n\n\n\n                         npar                          fmin \n                       70.000                         0.652 \n                        chisq                            df \n                     6055.193                       395.000 \n                       pvalue                  chisq.scaled \n                        0.000                      4484.352 \n                    df.scaled                 pvalue.scaled \n                      395.000                         0.000 \n         chisq.scaling.factor                baseline.chisq \n                        1.350                     96221.017 \n                  baseline.df               baseline.pvalue \n                      435.000                         0.000 \n        baseline.chisq.scaled            baseline.df.scaled \n                    75815.224                       435.000 \n       baseline.pvalue.scaled baseline.chisq.scaling.factor \n                        0.000                         1.269 \n                          cfi                           tli \n                        0.941                         0.935 \n                   cfi.scaled                    tli.scaled \n                        0.946                         0.940 \n                   cfi.robust                    tli.robust \n                        0.942                         0.936 \n                         logl             unrestricted.logl \n                  -135531.110                   -132503.514 \n                          aic                           bic \n                   271202.220                    271653.284 \n                       ntotal                          bic2 \n                     4646.000                    271430.850 \n                        rmsea                rmsea.ci.lower \n                        0.056                         0.054 \n               rmsea.ci.upper                  rmsea.pvalue \n                        0.057                         0.000 \n                 rmsea.scaled         rmsea.ci.lower.scaled \n                        0.047                         0.046 \n        rmsea.ci.upper.scaled           rmsea.pvalue.scaled \n                        0.048                         1.000 \n                 rmsea.robust         rmsea.ci.lower.robust \n                        0.055                         0.053 \n        rmsea.ci.upper.robust           rmsea.pvalue.robust \n                        0.056                            NA \n                         srmr \n                        0.039 \n\n\nDWLS model:\n\n\nCode\nscience.dwls[[\"fit\"]]\n\n\n\n\n           npar            fmin           chisq              df          pvalue \n         70.000           0.188        1744.348         395.000           0.000 \n baseline.chisq     baseline.df baseline.pvalue             cfi             tli \n     120867.135         435.000           0.000           0.989           0.988 \n          rmsea  rmsea.ci.lower  rmsea.ci.upper    rmsea.pvalue            srmr \n          0.027           0.026           0.028           1.000           0.034 \n\n\nTüm bu çıktılar incelendiğinde, en uygun modelin diagonally weighted least squares (DWLS) olduğu düşünülmektedir. Veri setinin farklı ölçek düzeyinde olması ve kategorik olması bu durumun sebebi olabilir. DWLS modeli, liteatürde yaygın bir şekilde bu tür veri setleri için önerilmektedir.\n\n\nBI-FACTOR MODELLER\nBi faktör modellemede yapıyı oluşturan faktörlerin tüm maddelerden oluşan genel faktör ile ilişkisi incelenerek karar verilir. Bu amaçla DFA örneğinde olduğu gibi model tanımlamamızı yapıyoruz:\n\n\nCode\nScience.bifactormodel &lt;- 'general.factor =~\n                              ST092Q01TA+\n                              ST092Q02TA+\n                              ST092Q04TA+\n                              ST092Q05TA+\n                              ST092Q06NA+\n                              ST092Q08NA+\n                              ST092Q09NA+\n                              ST094Q01NA+\n                              ST094Q02NA+\n                              ST094Q03NA+\n                              ST094Q04NA+\n                              ST094Q05NA+\n                              ST113Q01TA+\n                              ST113Q02TA+\n                              ST113Q03TA+\n                              ST113Q04TA+\n                              ST129Q01TA+\n                              ST129Q02TA+\n                              ST129Q03TA+\n                              ST129Q04TA+\n                              ST129Q05TA+\n                              ST129Q06TA+\n                              ST129Q07TA+\n                              ST129Q08TA+\n                              ST131Q01NA+\n                              ST131Q03NA+\n                              ST131Q04NA+\n                              ST131Q06NA+\n                              ST131Q08NA+\n                              ST131Q11NA\n\n                  ST092 =~ 1* ST092Q01TA+\n                              ST092Q02TA+\n                              ST092Q04TA+\n                              ST092Q05TA+\n                              ST092Q06NA+\n                              ST092Q08NA+\n                              ST092Q09NA\n\n                  ST094 =~ 1* ST094Q01NA+\n                              ST094Q02NA+\n                              ST094Q03NA+\n                              ST094Q04NA+\n                              ST094Q05NA\n\n                  ST113 =~ 1* ST113Q01TA+\n                              ST113Q02TA+\n                              ST113Q03TA+\n                              ST113Q04TA\n\n                  ST129 =~ 1* ST129Q01TA+\n                              ST129Q02TA+\n                              ST129Q03TA+\n                              ST129Q04TA+\n                              ST129Q05TA+\n                              ST129Q06TA+\n                              ST129Q07TA+\n                              ST129Q08TA\n\n                  ST131 =~ 1* ST131Q01NA+\n                              ST131Q03NA+\n                              ST131Q04NA+\n                              ST131Q06NA+\n                              ST131Q08NA+\n                              ST131Q11NA\n                              \n\n                general.factor  ~~ 0*ST092\n                general.factor  ~~ 0*ST094\n                general.factor  ~~ 0*ST113\n                general.factor  ~~ 0*ST129\n                general.factor  ~~ 0*ST131\n                \n                            ST092 ~~ ST094\n                            ST092 ~~ ST113\n                            ST092 ~~ ST129\n                            ST092 ~~ ST131\n\n                            ST094 ~~ ST113\n                            ST094 ~~ ST129\n                            ST094 ~~ ST131\n\n                            ST113 ~~ ST129\n                            ST113 ~~ ST131\n\n                            ST129 ~~ ST131'\n\n\nArdından modellerimizi veri setimiz ile sınıyoruz.\n\n\nCode\nScience.bifactormodel_mlm &lt;-\n  cfa(\n    Science.bifactormodel,\n    data = science_data,\n    std.lv = TRUE,\n    information = \"observed\"\n  )\nScience.bifactormodel_rml &lt;-\n  cfa(\n    Science.bifactormodel,\n    data = science_data,\n    estimator = \"MLM\",\n    se = \"robust.mlm\",\n    std.lv = TRUE,\n    information = \"observed\"\n  )\nbifactor_mlm &lt;- summary(Science.bifactormodel_mlm ,\n        fit.measures = TRUE,\n        standardized = TRUE)\nbifactor_rml &lt;- summary(Science.bifactormodel_rml ,\n        fit.measures = TRUE,\n        standardized = TRUE)\n\nbifactor_mlm[['fit']]\n\n\n             npar              fmin             chisq                df \n           95.000             0.634          5891.081           370.000 \n           pvalue    baseline.chisq       baseline.df   baseline.pvalue \n            0.000         96221.017           435.000             0.000 \n              cfi               tli              logl unrestricted.logl \n            0.942             0.932       -135449.054       -132503.514 \n              aic               bic            ntotal              bic2 \n       271088.109        271700.266          4646.000        271398.392 \n            rmsea    rmsea.ci.lower    rmsea.ci.upper      rmsea.pvalue \n            0.057             0.055             0.058             0.000 \n             srmr \n            0.328 \n\n\nCode\nbifactor_rml[['fit']]\n\n\n                         npar                          fmin \n                       95.000                         0.634 \n                        chisq                            df \n                     5891.081                       370.000 \n                       pvalue                  chisq.scaled \n                        0.000                      4166.363 \n                    df.scaled                 pvalue.scaled \n                      370.000                         0.000 \n         chisq.scaling.factor                baseline.chisq \n                        1.414                     96221.017 \n                  baseline.df               baseline.pvalue \n                      435.000                         0.000 \n        baseline.chisq.scaled            baseline.df.scaled \n                    19393.334                       435.000 \n       baseline.pvalue.scaled baseline.chisq.scaling.factor \n                        0.000                         4.962 \n                          cfi                           tli \n                        0.942                         0.932 \n                   cfi.scaled                    tli.scaled \n                        0.800                         0.765 \n                   cfi.robust                    tli.robust \n                        0.943                         0.933 \n                         logl             unrestricted.logl \n                  -135449.054                   -132503.514 \n                          aic                           bic \n                   271088.109                    271700.266 \n                       ntotal                          bic2 \n                     4646.000                    271398.392 \n                        rmsea                rmsea.ci.lower \n                        0.057                         0.055 \n               rmsea.ci.upper                  rmsea.pvalue \n                        0.058                         0.000 \n                 rmsea.scaled         rmsea.ci.lower.scaled \n                        0.047                         0.046 \n        rmsea.ci.upper.scaled           rmsea.pvalue.scaled \n                        0.048                         1.000 \n                 rmsea.robust         rmsea.ci.lower.robust \n                        0.056                         0.054 \n        rmsea.ci.upper.robust           rmsea.pvalue.robust \n                        0.057                            NA \n                         srmr \n                        0.328 \n\n\nBI-FAKTÖR ML model:\n\n\n             npar              fmin             chisq                df \n           95.000             0.634          5891.081           370.000 \n           pvalue    baseline.chisq       baseline.df   baseline.pvalue \n            0.000         96221.017           435.000             0.000 \n              cfi               tli              logl unrestricted.logl \n            0.942             0.932       -135449.054       -132503.514 \n              aic               bic            ntotal              bic2 \n       271088.109        271700.266          4646.000        271398.392 \n            rmsea    rmsea.ci.lower    rmsea.ci.upper      rmsea.pvalue \n            0.057             0.055             0.058             0.000 \n             srmr \n            0.328 \n\n\nBI-FAKTÖR ROBUST ML model:\n\n\n                         npar                          fmin \n                       95.000                         0.634 \n                        chisq                            df \n                     5891.081                       370.000 \n                       pvalue                  chisq.scaled \n                        0.000                      4166.363 \n                    df.scaled                 pvalue.scaled \n                      370.000                         0.000 \n         chisq.scaling.factor                baseline.chisq \n                        1.414                     96221.017 \n                  baseline.df               baseline.pvalue \n                      435.000                         0.000 \n        baseline.chisq.scaled            baseline.df.scaled \n                    19393.334                       435.000 \n       baseline.pvalue.scaled baseline.chisq.scaling.factor \n                        0.000                         4.962 \n                          cfi                           tli \n                        0.942                         0.932 \n                   cfi.scaled                    tli.scaled \n                        0.800                         0.765 \n                   cfi.robust                    tli.robust \n                        0.943                         0.933 \n                         logl             unrestricted.logl \n                  -135449.054                   -132503.514 \n                          aic                           bic \n                   271088.109                    271700.266 \n                       ntotal                          bic2 \n                     4646.000                    271398.392 \n                        rmsea                rmsea.ci.lower \n                        0.057                         0.055 \n               rmsea.ci.upper                  rmsea.pvalue \n                        0.058                         0.000 \n                 rmsea.scaled         rmsea.ci.lower.scaled \n                        0.047                         0.046 \n        rmsea.ci.upper.scaled           rmsea.pvalue.scaled \n                        0.048                         1.000 \n                 rmsea.robust         rmsea.ci.lower.robust \n                        0.056                         0.054 \n        rmsea.ci.upper.robust           rmsea.pvalue.robust \n                        0.057                            NA \n                         srmr \n                        0.328 \n\n\nBu çıktılar incelendiğinde de bi faktör modelin verimize DWLS model kadar uyumlu olmadığı görülmektedir. Bu nedenle DWLS modelin en uyumlu model olduğu düşünülmektedir. Bu modelin çıktılarını tablolaştıralım.\n\n\nEN UYUMLU MODEL:DWLS çıktıları\nNOT: Faktör varyansları sabitlenerek model oluşturulduğu için faktör varyansları tablosu raporlaştırılmamıştır.\n\nModel uyum indeksleri\n\nParametre Tahminleri\n\n\n\nFaktör Kovaryansları\n\n\n\nArtık (residual) Varyanslar\n\n\n\nModel gösterimi"
  },
  {
    "objectID": "posts/Top 5 Data Sources/index.html",
    "href": "posts/Top 5 Data Sources/index.html",
    "title": "Top 5 Data Sources",
    "section": "",
    "text": "In the world of data analysis, having access to quality data sources can make all the difference. Whether you’re a researcher, a business owner, or just someone who loves playing with numbers, finding the right data can help you uncover insights and make better decisions. With so many data sources available online, it can be hard to know where to start. In this blog post, I’ll be sharing my top 5 favorite data sources, each of which I’ve found to be reliable, user-friendly, and full of valuable information. From the Cloud Google Marketplace to the Bureau of Labor Statistics, these data sources offer a wealth of insights that I hope you’ll find as useful and inspiring as I have. So without further ado, let’s dive in!"
  },
  {
    "objectID": "posts/Top 5 Data Sources/index.html#introduction",
    "href": "posts/Top 5 Data Sources/index.html#introduction",
    "title": "Top 5 Data Sources",
    "section": "",
    "text": "In the world of data analysis, having access to quality data sources can make all the difference. Whether you’re a researcher, a business owner, or just someone who loves playing with numbers, finding the right data can help you uncover insights and make better decisions. With so many data sources available online, it can be hard to know where to start. In this blog post, I’ll be sharing my top 5 favorite data sources, each of which I’ve found to be reliable, user-friendly, and full of valuable information. From the Cloud Google Marketplace to the Bureau of Labor Statistics, these data sources offer a wealth of insights that I hope you’ll find as useful and inspiring as I have. So without further ado, let’s dive in!"
  },
  {
    "objectID": "posts/Top 5 Data Sources/index.html#cloud-google-marketplace",
    "href": "posts/Top 5 Data Sources/index.html#cloud-google-marketplace",
    "title": "Top 5 Data Sources",
    "section": "1. Cloud Google Marketplace",
    "text": "1. Cloud Google Marketplace\nThe Cloud Google Marketplace is a collection of datasets, APIs, and applications that are hosted on the Google Cloud Platform. The marketplace offers a wide range of data sources from different industries, including finance, healthcare, and transportation. Some of the popular datasets available on the Cloud Google Marketplace include:\n\nCOVID-19 Open Research Dataset (CORD-19)\nNew York City Taxi and Limousine Commission (TLC) Trip Data\nChicago Crime Data\nOpenFDA Drug Label API\nUS Census Bureau Data\n\nThe platform offers an intuitive interface that allows users to search for datasets, preview data, and access documentation. The marketplace also provides access to cloud-based tools for data exploration, visualization, and analysis."
  },
  {
    "objectID": "posts/Top 5 Data Sources/index.html#drought.gov",
    "href": "posts/Top 5 Data Sources/index.html#drought.gov",
    "title": "Top 5 Data Sources",
    "section": "2. Drought.gov",
    "text": "2. Drought.gov\nDrought.gov is a comprehensive resource for drought-related data, maps, and tools in the United States. The website provides a variety of data sources, including:\n\nDrought Monitor\nSoil Moisture Active Passive (SMAP) data\nEvaporative Demand Drought Index (EDDI)\nU.S. Drought Atlas\n\nThe website offers easy access to a wide range of data, including both historical and real-time data. Additionally, the site provides a number of interactive maps and visualizations that make it easy to explore the data and gain insights."
  },
  {
    "objectID": "posts/Top 5 Data Sources/index.html#unicef-statistical-tables",
    "href": "posts/Top 5 Data Sources/index.html#unicef-statistical-tables",
    "title": "Top 5 Data Sources",
    "section": "3. UNICEF Statistical Tables",
    "text": "3. UNICEF Statistical Tables\nUNICEF Statistical Tables is a data source that provides a wide range of data on child welfare and development across the globe. The data is collected from various sources, including national surveys and administrative records. Some of the data available on the UNICEF Statistical Tables includes:\n\nChild mortality rates\nNutrition indicators\nEducation indicators\nChild protection indicators\n\nThe platform offers a user-friendly interface that allows users to browse data by country, indicator, or year. For instance, policymakers looking to design effective policies to improve child welfare in their countries could use data from UNICEF Statistical Tables to identify areas where interventions are most needed, such as high child mortality rates or low school enrollment rates."
  },
  {
    "objectID": "posts/Top 5 Data Sources/index.html#openpolicing.stanford.edu",
    "href": "posts/Top 5 Data Sources/index.html#openpolicing.stanford.edu",
    "title": "Top 5 Data Sources",
    "section": "4. OpenPolicing.Stanford.edu",
    "text": "4. OpenPolicing.Stanford.edu\nOpenPolicing.Stanford.edu is a data source that provides access to millions of records of police stops and searches across the United States. The data is collected from various law enforcement agencies and compiled into a standardized format for analysis. The platform also provides tools for exploring the data, including interactive maps and dashboards. As this source of data contains pretty complicate information, you should definitely check the Read Me File before you delve into it.\nThe platform provides detailed data on police stops and searches, including the race and ethnicity of the individuals involved, the reason for the stop, and the outcome of the encounter.\nOne example of how this data could be used is to identify patterns of racial bias in policing. By analyzing the data on police stops and searches, researchers can identify whether certain racial or ethnic groups are more likely to be stopped or searched than others, and whether these disparities are driven by legitimate factors (such as crime rates) or by bias."
  },
  {
    "objectID": "posts/Top 5 Data Sources/index.html#bureau-of-labor-statistics-bls-tables",
    "href": "posts/Top 5 Data Sources/index.html#bureau-of-labor-statistics-bls-tables",
    "title": "Top 5 Data Sources",
    "section": "5. Bureau of Labor Statistics (BLS) Tables",
    "text": "5. Bureau of Labor Statistics (BLS) Tables\nThe Bureau of Labor Statistics (BLS) Tables is a data source that provides access to a wide range of data on the U.S. labor market. The platform offers a wealth of data, including:\n\nUnemployment rates\nEmployment by industry and occupation\nLabor force participation rates\nAverage wages and salaries\nJob openings and hires\n\nThe platform provides a variety of tools for data exploration and analysis, including charts, tables, and interactive visualizations. For example, if you’re a policymaker looking to design policies that promote job growth and reduce unemployment, you could use BLS Tables data to identify which industries are experiencing job growth, which occupations are in high demand, and which regions have the highest unemployment rates."
  },
  {
    "objectID": "posts/Top 5 Data Sources/index.html#an-easter-egg-openpsychometrics.org",
    "href": "posts/Top 5 Data Sources/index.html#an-easter-egg-openpsychometrics.org",
    "title": "Top 5 Data Sources",
    "section": "An Easter Egg: OpenPsychometrics.org",
    "text": "An Easter Egg: OpenPsychometrics.org\nAs a surprise addition to my list of favorite data sources, I would like to introduce OpenPsychometrics.org. I came across with this website during a Ph.D. course on Item Response Theory carried by Associate Professor Ergül Demir. This website provides access to a collection of free psychological tests and assessments, which can be used for a wide range of purposes, from self-reflection to academic research.\nSome of the psychological tests available on the site include:\n\nThe Big Five personality traits\nEmotional intelligence\nCognitive abilities\nMoral reasoning\n\nAs someone who is interested in both data science and psychology, I have found OpenPsychometrics.org to be an incredibly interesting and valuable resource. The website offers a wide range of psychological tests that can help individuals gain insights into their own personality traits, cognitive abilities, and emotional intelligence. Additionally, researchers can use the site to conduct studies on these topics and collect data for analysis.\nIn conclusion, OpenPsychometrics.org is a valuable resource for anyone interested in psychology, personal development, or academic research. The platform offers a variety of free psychological tests that can help individuals gain insights into themselves, as well as collect data for research purposes."
  },
  {
    "objectID": "posts/Top 5 Data Sources/index.html#conclusion",
    "href": "posts/Top 5 Data Sources/index.html#conclusion",
    "title": "Top 5 Data Sources",
    "section": "Conclusion",
    "text": "Conclusion\nThese are just a few of my favorite data sources that have proven to be invaluable for my work as a researcher. Each of these data sources provides a unique perspective on different aspects of the world, from labor markets to child welfare to police behavior. By combining and analyzing data from these sources, we can gain a more comprehensive understanding of the world around us and make better-informed decisions."
  },
  {
    "objectID": "posts/Visualisation of My Personal Google Data (2)/index.html",
    "href": "posts/Visualisation of My Personal Google Data (2)/index.html",
    "title": "Visualisation of My Personal Google Data (2): My exercise routine",
    "section": "",
    "text": "If you give permission to Google to store your location data, they will keep them in their databases forever. You can also allow them to store it for a while and then ask them delete it. They will directly do so.\nWhat makes this study a fun project is its being very personal. I decided to analyze my personal data in August, 2022. Therefore, I granted many new permissions to Google along with many previously granted permissions. They keep them in various formats including .csv, .json, .mbox etc. When you query for your personal data, they provide it within a couple of days depending on the size of the data you queried.\nUsually, I provide the readers with the data in my posts. However, in this series, the data are very personal and so I will not."
  },
  {
    "objectID": "posts/Visualisation of My Personal Google Data (2)/index.html#introduction-to-the-series-visualisation-of-my-personal-google-data",
    "href": "posts/Visualisation of My Personal Google Data (2)/index.html#introduction-to-the-series-visualisation-of-my-personal-google-data",
    "title": "Visualisation of My Personal Google Data (2): My exercise routine",
    "section": "",
    "text": "If you give permission to Google to store your location data, they will keep them in their databases forever. You can also allow them to store it for a while and then ask them delete it. They will directly do so.\nWhat makes this study a fun project is its being very personal. I decided to analyze my personal data in August, 2022. Therefore, I granted many new permissions to Google along with many previously granted permissions. They keep them in various formats including .csv, .json, .mbox etc. When you query for your personal data, they provide it within a couple of days depending on the size of the data you queried.\nUsually, I provide the readers with the data in my posts. However, in this series, the data are very personal and so I will not."
  },
  {
    "objectID": "posts/Visualisation of My Personal Google Data (2)/index.html#introduction-my-exercise-routine",
    "href": "posts/Visualisation of My Personal Google Data (2)/index.html#introduction-my-exercise-routine",
    "title": "Visualisation of My Personal Google Data (2): My exercise routine",
    "section": "Introduction: “My exercise routine”",
    "text": "Introduction: “My exercise routine”\nIn this part of the series, we will investigate my personal location data. We will visualize the spots I visited within a period of time. This way, I personally will gain insights about how boring my days are :)\nThe R packages that we use in this post are as follows:\n\nlibrary(\"psych\")\nlibrary(\"dplyr\")\nlibrary(\"plotly\")"
  },
  {
    "objectID": "posts/Visualisation of My Personal Google Data (2)/index.html#understand-the-data-pre-processing",
    "href": "posts/Visualisation of My Personal Google Data (2)/index.html#understand-the-data-pre-processing",
    "title": "Visualisation of My Personal Google Data (2): My exercise routine",
    "section": "Understand the Data & Pre-processing",
    "text": "Understand the Data & Pre-processing\nLet’s start the procedure by reading the data into R. The head() function provides us with the first six observations of the data frame. There are 16 variables some of which are filled with many many NAs. such as Biking.duration..ms. The reason for that is pretty simple; I do not ride around. The variables are mostly numeric, yet Dategives us information about the date of the observation. The variable Running.duration..ms. has many NAs too. But it also has some observations because I run only a few days a week. I will use this variable to filter my data later.\n\ndaily_metrics&lt;-read.csv(\"Daily activity metrics.csv\",sep=\",\", header = TRUE)\nhead(daily_metrics) \n\n        Date Move.Minutes.count Calories..kcal. Distance..m. Heart.Points\n1 2021-05-16                 77        687.6241     4090.742           40\n2 2021-05-17                 79       2016.4717     3978.947           13\n3 2021-05-18                 31       1752.2711     2267.061           21\n4 2021-05-19                140       2201.5012     9714.473           74\n5 2021-05-20                 50       1842.7984     2618.885           16\n6 2021-05-21                 63       1902.6128     4571.006           43\n  Heart.Minutes Average.speed..m.s. Max.speed..m.s. Min.speed..m.s. Step.count\n1            40           0.9532187        1.017549       0.8888889       7654\n2            13           0.3553377        1.328190       0.2391036       6161\n3            21           0.4802522        1.643308       0.2550294       3450\n4            74           0.5155224        1.711053       0.2550294      14000\n5            16           0.3804749        1.516558       0.2588491       4078\n6            43           0.5348663        1.492037       0.2656556       6768\n  Average.weight..kg. Max.weight..kg. Min.weight..kg. Biking.duration..ms.\n1                  80              80              80                   NA\n2                  NA              NA              NA                   NA\n3                  NA              NA              NA                   NA\n4                  NA              NA              NA                   NA\n5                  NA              NA              NA                   NA\n6                  NA              NA              NA                   NA\n  Walking.duration..ms. Running.duration..ms.\n1               4664026                    NA\n2               4692410                424601\n3               1610096                    NA\n4               8268890                 93935\n5               2994689                    NA\n6               3909532                    NA\n\n\nAs the data contains some variables with decimal numbers, I would love to round them to increase meaningfulness. The mutate_if function combined with is.numeric gives me the opportunity to pick only the numeric variables to round.\n\ndaily_metrics &lt;- daily_metrics%&gt;% mutate_if(is.numeric, round, digits=2)\n\nSo far so good. Now I want to select the variables that I want to visualize in my project and filter my data accordingly. Let’s see the variable names:\n\ncolnames(daily_metrics)\n\n [1] \"Date\"                  \"Move.Minutes.count\"    \"Calories..kcal.\"      \n [4] \"Distance..m.\"          \"Heart.Points\"          \"Heart.Minutes\"        \n [7] \"Average.speed..m.s.\"   \"Max.speed..m.s.\"       \"Min.speed..m.s.\"      \n[10] \"Step.count\"            \"Average.weight..kg.\"   \"Max.weight..kg.\"      \n[13] \"Min.weight..kg.\"       \"Biking.duration..ms.\"  \"Walking.duration..ms.\"\n[16] \"Running.duration..ms.\"\n\n\nIn this project, I will use following variables: date, Move.Minutes.count,Calories..kcal.,Distance..m.,Average.speed..m.s.,Biking.duration..ms.,Step.caoun and walking.duration.ms.. Let’s use subset function to get rid of the unnecessary part of the data.\n\ndaily_metrics &lt;- subset(daily_metrics, select = -c(5,6,8,9,14, 13,12,11) ) #idk why so randomly ordered numbers\nhead(daily_metrics)\n\n        Date Move.Minutes.count Calories..kcal. Distance..m.\n1 2021-05-16                 77          687.62      4090.74\n2 2021-05-17                 79         2016.47      3978.95\n3 2021-05-18                 31         1752.27      2267.06\n4 2021-05-19                140         2201.50      9714.47\n5 2021-05-20                 50         1842.80      2618.88\n6 2021-05-21                 63         1902.61      4571.01\n  Average.speed..m.s. Step.count Walking.duration..ms. Running.duration..ms.\n1                0.95       7654               4664026                    NA\n2                0.36       6161               4692410                424601\n3                0.48       3450               1610096                    NA\n4                0.52      14000               8268890                 93935\n5                0.38       4078               2994689                    NA\n6                0.53       6768               3909532                    NA\n\n\nAs mentioned before, I would like to work on the days that I run. That’s why I will also get rid of observations that do not contain any running data and then my data will be ready for the visualization.\nFinally, let’s get the scatter plots of whatever we have left to see a bigger picture. There are some outliers; you will easily recognize some bubles sitting alone in their plots. However, they don’t look like they need further investigation.\n\ndata_for_plotting&lt;-daily_metrics[!is.na(daily_metrics$Running.duration..ms.),]\nplot(data_for_plotting)"
  },
  {
    "objectID": "posts/Visualisation of My Personal Google Data (2)/index.html#data-visualization",
    "href": "posts/Visualisation of My Personal Google Data (2)/index.html#data-visualization",
    "title": "Visualisation of My Personal Google Data (2): My exercise routine",
    "section": "Data Visualization",
    "text": "Data Visualization\nplotly is an amazing alternative to ggplot2. Today we will work on this package with our data. Our first chart contains information about the number of steps I took each day that I ran from May 2021 to November 2022.\n\nfig&lt;-plotly::plot_ly(data=data_for_plotting, type =\"scatter\", mode=\"lines+markers\",  \n                     y=data_for_plotting$Step.count, x=~Date)\nfig\n\n\n\n\n\nWhen you hover over the chart you might see the date and the number of steps that I took. For instance on 12th and 28th of August, I took more than 16k steps.\nNow let’s add two dimensions to our chart: the duration of walking and running in a day using the addtrace function. Also layout function helps us name our plot.\n\n########################\n# walking VS runing duration bars\n#########################\nay&lt;- list(\n  tickfont =list(color =\"red\"),\n  overlaying = \"y\",\n  side= \"right\",\n  title= \"&lt;b&gt; secondary&lt;/b&gt; y axis\"\n)\n\n\nfig&lt;-plotly::plot_ly()\nfig&lt;- fig %&gt;%\n  add_trace(type =\"bar\",  \n            y =data_for_plotting$Walking.duration..ms.,\n            x=data_for_plotting$Date, \n            name=\"walking duration (ms)\"\n  )\nfig&lt;- fig %&gt;% add_trace (type =\"scatter\", mode=\"lines+markers\" , #yaxis=\"y2\",\n                         y=data_for_plotting$Running.duration..ms., \n                         x=data_for_plotting$Date , \n                         name=\"running duration (ms)\")\n\n\nfig &lt;- fig %&gt;% layout(\n  title=\"two axis\",\n  yaxis2 = ay,\n  xaxis = list( title= \"Days\"),\n  yaxis = list( title= \"Time I moved in miliseconds\")\n)\nfig\n\n\n\n\n\nThis chart brings another aspect of my exercise routine. The days I ran in the last year, I walked a lot more than I ran, obviously. One exception to that might be 25th of July when I almost walked and ran same amount of time.\nNow, besides time, let’s add another dimension of distance. This chart will have another y axis. We need to let plotly know that we will use another dimension. overlaying = \"y\" argument will do it. Also, we use another addtrace function to add the second y axis properties. yaxis=\"y2\" argument will link this trace to our new dimension. y2 here is defined by plotly as default setting.\n\nay&lt;- list(\n  tickfont =list(color =\"red\"),\n  overlaying = \"y\",\n  side= \"right\",\n  title= \"&lt;b&gt; Distance I walked in meters &lt;/b&gt;\"\n)\n\n\nfig&lt;-plotly::plot_ly()\n\nfig&lt;- fig %&gt;%\n  add_trace(y =data_for_plotting$Move.Minutes.count,\n            x=data_for_plotting$Date, \n            name=\"movemnt(min.)\", \n            type =\"bar\" )\n\n\nfig&lt;- fig %&gt;% add_trace (y=data_for_plotting$Distance..m., \n                         x=data_for_plotting$Date , \n                         name=\"distance(m.)\", yaxis=\"y2\",\n                         type =\"scatter\", mode=\"lines+markers\")\n\nfig &lt;- fig %&gt;% layout(\n  title=\"My movement VS distance  &lt;b&gt;in 2022&lt;/b&gt;\",\n  yaxis2 = ay,\n  xaxis = list( title= \"&lt;b&gt;Days&lt;/b&gt;\"),\n  yaxis = list( title= \"&lt;b&gt; Time I moved in minutes &lt;/b&gt; \")\n)\nfig\n\n\n\n\n\nAs the final result, you can see that on the first y axis there are the number of minutes (from 0 to 200) when I was in a moving state. On the other side, we have the distance I walked or run in meters (from 0 to 10k). For example; on sixth of Auust in 2022, I walked almost 7k meters in 150 minutes.\nAs a final step, let’s see the calories I burnt during that time. Another addtrace would help us draw the line for the calories that I burnt throughout these days:\n\nfig&lt;-plotly::plot_ly()\n\nfig&lt;- fig %&gt;%\n  add_trace(y =data_for_plotting$Step.count,\n            x=data_for_plotting$Date, \n            name=\"Steps \", \n            type =\"scatter\", mode=\"lines\")\n\nfig&lt;- fig %&gt;% add_trace (y=data_for_plotting$Distance..m., \n                         x=data_for_plotting$Date , \n                         name=\"distance\", \n                         type =\"bar\" )\n\n\nfig&lt;- fig %&gt;% add_trace (y=data_for_plotting$Calories..kcal., \n                         x=data_for_plotting$Date , \n                         name=\"calories\",\n                         type =\"scatter\", mode=\"lines\")\n\n\n\nfig &lt;- fig %&gt;% layout(\n  title=\" &lt;b&gt;My movement in 2022&lt;/b&gt;\",\n  xaxis = list( title= \"&lt;b&gt;Days&lt;/b&gt;\"),\n  yaxis = list( title= \"&lt;b&gt; steps(n)&lt;/b&gt; / &lt;b&gt; distance(m)&lt;/b&gt; / &lt;b&gt; calories(kcal) &lt;/b&gt; \")\n)\nfig\n\n\n\n\n\nThe reason why there are not many diversity in the calories that I burnt might be because of two reasons: either the calories are not counted correctly by Google, which I believe is the case, or as the charts contain information for only the days that I run, I spent similar amounts of calories. Either way, we can see that the calories don’t change much even though took much more steps and spent more time moving on some days than some other days."
  },
  {
    "objectID": "posts/Visualisation of My Personal Google Data (2)/index.html#conclusion",
    "href": "posts/Visualisation of My Personal Google Data (2)/index.html#conclusion",
    "title": "Visualisation of My Personal Google Data (2): My exercise routine",
    "section": "Conclusion",
    "text": "Conclusion\nThe plotly package have many useful embedded features. Especially the hovering property of the charts make it much more appealing easily. Another beautiful feature of plotly is that you can use certain HTML tags on your charts. For example here in the last chart, we used &lt;b&gt; bolt &lt;/b&gt;tag in order for making our titles stand out in the layout function.\nFinally I must also mention that plotly have some features to create different types of graphs, animations and many other. Their website for R worths a visit."
  }
]