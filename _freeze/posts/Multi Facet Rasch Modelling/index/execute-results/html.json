{
  "hash": "d751c49bac3689484f3d9e25da1ce7d3",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Multi-Facet Rasch Models with R\"\ndescription: |\n  In this post, we’ll explore the Multi-Facet Rasch Model, understand how it works, and demonstrate how to fit this model using R. We'll also visualize the results to better interpret them. \nauthor:\n  - name: Ali Emre Karagül\n    orcid: 0000-0002-5820-8643\n    email: aliemrekaragul@gmail.com\n    affiliations:\n      - name: University of Economics & Technology\n        city: Ankara\n        url: https://www.etu.edu.tr/tr\n      - name: Gazi University\n        city: Ankara\n        url: https://gazi.edu.tr/\nlightbox: true\nreference-location: margin\ncrossref:\n  fig-labels: alpha a    \n  tbl-labels: alpha a    \n  subref-labels: roman i \n  chapters: true\ndate: 2024-10-17\ncategories: [readr, tidyr, ggplot2,dplyr,tam,cowplot\n  ]\nimage: \"image.png\"\noutput:\n    self_contained: false\n    code_folding: false\nexecute:\n  freeze: auto\n---\n\n\n\n\n\n## Introduction\n\nWhile a basic Rasch model focuses on item difficulty and person ability, the **Multi-Facet Rasch Model** (MFRM) allows us to incorporate additional factors, or facets, such as:\n\n-   **Person Ability** (e.g., the skill level of test-takers),\n\n-   **Item Difficulty** (e.g., how hard the test items are),\n\n-   **Rater Severity** (e.g., how lenient or strict raters are),\n\n-   **Task or Stimulus Differences** (e.g., variation in tasks given).\n\nMFRM is said to be IRT version of generalizability theory and it is particularly useful when assessments involve subjective judgments, like in essay grading or performance evaluation, where raters' subjectivity can introduce bias.\n\n::: {layout-ncol=\"2\"}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# Load the libraries\nlibrary(readr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(dplyr)\n\nlibrary(tidyverse)\nlibrary(TAM)\nlibrary(cowplot)\n```\n:::\n\n\n\n\n:::\n\n## 1. Understand the data\n\nFor MFRM analysis, we are going to use a dataset of essay scores scored on an analytical rubric. There are four domains of the rubric: Content, Prompt Adherence, Language, and Narrativity. Let's load the data and see the head of them. You can download the data for your own use from [here](https://drive.google.com/file/d/1cmfJd_h68J5JpBIPX6N-iISgESAVgvlI/view?usp=drive_link).\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\ndata <- read_csv(\"MFRM_data.csv\", show_col_types = FALSE)\nhead(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 6\n    Con ProAd  Lang   Nar rater essayId\n  <dbl> <dbl> <dbl> <dbl> <dbl>   <dbl>\n1     4     4     3     3     1       1\n2     2     2     4     3     1       2\n3     4     4     4     4     1       3\n4     3     2     4     4     1       4\n5     4     4     4     4     1       5\n6     1     1     1     2     1       6\n```\n\n\n:::\n:::\n\n\n\n\n\nLet's see the structure and summary of the data too.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nstr(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nspc_tbl_ [5,400 × 6] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ Con    : num [1:5400] 4 2 4 3 4 1 1 2 4 3 ...\n $ ProAd  : num [1:5400] 4 2 4 2 4 1 1 2 4 3 ...\n $ Lang   : num [1:5400] 3 4 4 4 4 1 1 4 4 4 ...\n $ Nar    : num [1:5400] 3 3 4 4 4 2 1 4 4 2 ...\n $ rater  : num [1:5400] 1 1 1 1 1 1 1 1 1 1 ...\n $ essayId: num [1:5400] 1 2 3 4 5 6 7 8 9 10 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   Con = col_double(),\n  ..   ProAd = col_double(),\n  ..   Lang = col_double(),\n  ..   Nar = col_double(),\n  ..   rater = col_double(),\n  ..   essayId = col_double()\n  .. )\n - attr(*, \"problems\")=<externalptr> \n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"false\"}\nsummary(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      Con            ProAd            Lang            Nar            rater  \n Min.   :0.000   Min.   :0.000   Min.   :0.000   Min.   :0.000   Min.   :1  \n 1st Qu.:1.000   1st Qu.:1.000   1st Qu.:1.000   1st Qu.:1.000   1st Qu.:1  \n Median :2.000   Median :2.000   Median :2.000   Median :2.000   Median :2  \n Mean   :1.878   Mean   :1.784   Mean   :2.062   Mean   :1.958   Mean   :2  \n 3rd Qu.:3.000   3rd Qu.:3.000   3rd Qu.:3.000   3rd Qu.:3.000   3rd Qu.:3  \n Max.   :4.000   Max.   :4.000   Max.   :4.000   Max.   :4.000   Max.   :3  \n    essayId      \n Min.   :   1.0  \n 1st Qu.: 450.8  \n Median : 900.5  \n Mean   : 900.5  \n 3rd Qu.:1350.2  \n Max.   :1800.0  \n```\n\n\n:::\n:::\n\n\n\n\n\nThere are 1800 rows of data. Each domain is scored between 0 and 4. Perfect as zero must exist in the ordinal data for MFRM. The data set consists of scores from three raters on four domains, so we need to account for the three key facets: **person ability** (the essays), **item difficulty** (the domains), and **rater severity** (the three raters).\n\n## 2. Fit the model\n\nNow that we've explored our data set, it's time to fit the Multi-Facet Rasch Model (MFRM). To do this, we'll use the `TAM` package in R, which provides functions for fitting various Rasch models, including MFRM. The `formulaA` provided into the mfr function decides on the model. For PCM, we define interaction between item and step along with the rater facet:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nfacets <- data[, \"rater\", drop=FALSE]  # define facet (rater)\npid <- data$essayId  # define person identifier\nresp <- data[, -c(5:6)]  # item response data\nformulaA <- ~item*step + rater   # formula for PCM\n\n\nmod <- TAM::tam.mml.mfr(resp=resp, facets=facets, formulaA=formulaA, pid=data$essayId)\n```\n:::\n\n\n\n\n\n## 3. Model diagnostics\n\nNow that we've fitted our Multi-Facet Rasch Model (MFRM) with PCM, let's take a closer look at the results and explore some diagnostics.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nfit_stats <- TAM::tam.fit(mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nItem fit calculation based on 15 simulations\n|**********|\n|----------|\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"false\"}\nprint(fit_stats)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$itemfit\n     parameter    Outfit   Outfit_t      Outfit_p  Outfit_pholm     Infit\n1          Con 2.0591522  39.361636  0.000000e+00  0.000000e+00 2.1979266\n2        ProAd 1.5301160  21.894545 2.928160e-106 2.635344e-105 1.6156500\n3         Lang 2.2535201  47.334138  0.000000e+00  0.000000e+00 2.3403618\n4          Nar 1.8429272  33.147291 6.196043e-241 6.815647e-240 1.9492346\n5        step1 0.9494058  -3.647927  2.643645e-04  5.287290e-04 0.9369712\n6        step2 0.4917571 -52.934061  0.000000e+00  0.000000e+00 0.4102024\n7        step3 0.2640096 -96.962995  0.000000e+00  0.000000e+00 0.1816552\n8        step4 0.9177011  -6.474657  9.502735e-11  3.801094e-10 0.5978796\n9       rater1 0.2665086 -83.397247  0.000000e+00  0.000000e+00 0.2045924\n10      rater2 0.2126247 -96.365947  0.000000e+00  0.000000e+00 0.1305165\n11      rater3 0.3471616 -70.514947  0.000000e+00  0.000000e+00 0.1985980\n12   Con:step1 0.8398256  -8.668278  4.387050e-18  2.193525e-17 0.8568111\n13 ProAd:step1 0.6746766 -17.811336  5.771558e-71  4.617246e-70 0.7091680\n14  Lang:step1 0.5180443 -29.210127 1.442217e-187 1.442217e-186 0.5202205\n15   Nar:step1 0.6858151 -16.202919  4.808891e-59  3.366224e-58 0.7063609\n16   Con:step2 0.5296627 -33.996303 2.526439e-253 3.031727e-252 0.4427496\n17 ProAd:step2 0.4289933 -42.461731  0.000000e+00  0.000000e+00 0.3497186\n18  Lang:step2 0.1969640 -75.560345  0.000000e+00  0.000000e+00 0.1640074\n19   Nar:step2 0.3698636 -46.072478  0.000000e+00  0.000000e+00 0.3082262\n20   Con:step3 0.2804214 -66.678683  0.000000e+00  0.000000e+00 0.2265406\n21 ProAd:step3 0.2855422 -62.035576  0.000000e+00  0.000000e+00 0.2089630\n22  Lang:step3 0.1888063 -83.889912  0.000000e+00  0.000000e+00 0.1150833\n23   Nar:step3 0.2510021 -64.460639  0.000000e+00  0.000000e+00 0.1836688\n24   Con:step4 0.9436611  -3.155082  1.604530e-03  1.604530e-03 0.5763329\n25 ProAd:step4 0.8958462  -5.556046  2.759535e-08  8.278605e-08 0.5200940\n26  Lang:step4 0.4643230 -36.676871 1.707308e-294 2.219500e-293 0.2799968\n27   Nar:step4 0.7677798 -12.393932  2.818774e-35  1.691264e-34 0.4609557\n       Infit_t       Infit_p   Infit_pholm\n1    43.404732  0.000000e+00  0.000000e+00\n2    24.924232 4.064309e-137 2.032154e-136\n3    49.864878  0.000000e+00  0.000000e+00\n4    36.540091 2.561982e-292 2.818181e-291\n5    -4.563129  5.039684e-06  5.039684e-06\n6   -64.563804  0.000000e+00  0.000000e+00\n7  -117.289175  0.000000e+00  0.000000e+00\n8   -36.103138 2.024933e-285 2.024933e-284\n9   -96.095647  0.000000e+00  0.000000e+00\n10 -117.784465  0.000000e+00  0.000000e+00\n11  -98.836404  0.000000e+00  0.000000e+00\n12   -7.698737  1.374172e-14  2.748343e-14\n13  -15.680881  2.043984e-55  8.175938e-55\n14  -29.043052 1.883337e-185 1.318336e-184\n15  -15.006236  6.683443e-51  2.005033e-50\n16  -42.353408  0.000000e+00  0.000000e+00\n17  -51.040009  0.000000e+00  0.000000e+00\n18  -81.785151  0.000000e+00  0.000000e+00\n19  -52.982695  0.000000e+00  0.000000e+00\n20  -75.326822  0.000000e+00  0.000000e+00\n21  -73.806223  0.000000e+00  0.000000e+00\n22 -101.040972  0.000000e+00  0.000000e+00\n23  -75.311205  0.000000e+00  0.000000e+00\n24  -27.699201 7.138532e-169 4.283119e-168\n25  -30.243052 6.438903e-201 5.151122e-200\n26  -56.207040  0.000000e+00  0.000000e+00\n27  -33.454655 2.202305e-245 1.982075e-244\n\n$time\n[1] \"2024-12-05 13:54:56 +03\" \"2024-12-05 13:54:56 +03\"\n\n$CALL\nTAM::tam.fit(tamobj = mod)\n\nattr(,\"class\")\n[1] \"tam.fit\"\n```\n\n\n:::\n:::\n\n\n\n\n\n-   **Infit and Outfit Values:**\n\n    -   **Infit and Outfit near 1:** Indicates that the item fits well with the model.\n\n    -   **Infit/Outfit significantly \\>1:** Indicates that the item is *underfitting*, meaning there is more variability in the responses than the model expects (perhaps caused by noise or misfitting responses).\n\n    -   **Infit/Outfit significantly \\<1:** Indicates that the item is *overfitting*, meaning the responses are too predictable, and there's less variability than expected by the model (possibly due to redundancy or lack of challenge).\n\n    Let's break down a few examples from the output:\n\n    -   **Content (Con):**\n\n        -   Outfit MNSQ: 2.05, Infit MNSQ: 2.18\n\n        -   These values are well above 1, indicating underfit. The item \"Con\" might be too noisy or not behaving consistently with the model.\n\n    -   **Prompt Adherence (ProAd):**\n\n        -   Outfit MNSQ: 1.54, Infit MNSQ: 1.63\n\n        -   These values are higher than 1 but still in the acceptable range, meaning there's some noise, but it’s not excessive.\n\n    -   **Language (Lang):**\n\n        -   Outfit MNSQ: 2.26, Infit MNSQ: 2.34\n\n        -   These values suggest significant underfit, similar to \"Con\", indicating that responses to this domain might be less consistent or more unpredictable than the model expects.\n\n    -   **Steps (step1 to step4):**\n\n        -   Some steps, such as **step1**, have Infit and Outfit values closer to 1 (e.g., Outfit MNSQ: 0.95, Infit MNSQ: 0.94). These are acceptable and suggest that step1 is fitting well.\n\n        -   However, **step2, step3, and step4** show extremely low values, especially for Outfit (e.g., **step3** has an Outfit MNSQ of **0.26**), indicating overfit, meaning these categories are too predictable and might not differentiate well between respondents.\n\n-   **t-statistics and p-values:**\n\n    -   The **t-statistics (Outfit_t, Infit_t)** are standardized fit statistics that test whether the Infit/Outfit values significantly differ from 1. Large positive or negative t-values indicate significant deviation from expected values.\n\n    -   The **p-values (Outfit_p, Infit_p)** show whether these deviations are statistically significant. Nearly all p-values are extremely low (close to 0), suggesting that most of the items are statistically misfitting according to the model.\n\n-   **Rater Severity:**\n\n    -   **Rater1, Rater2, Rater3** all have very low Infit and Outfit values (e.g., **Rater1 Outfit: 0.26**), which suggest that these raters may be overfitting. This could mean that they are scoring in a highly predictable way, possibly being too strict or lenient consistently.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nreliability <- mod$EAP.rel\nreliability\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9716057\n```\n\n\n:::\n:::\n\n\n\n\n\nThe final **WLE Reliability = 0.97** is an excellent reliability score, meaning that the person ability estimates are very consistent. WLE reliability, similar to other reliability coefficients like Cronbach’s alpha, indicates the precision of the estimates:\n\nA **0.97 reliability** means that 97% of the variance in the person ability estimates is due to true differences in ability rather than measurement error.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\npersons.mod <- TAM::tam.wle(mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nIteration in WLE/MLE estimation  1   | Maximal change  2.8113 \nIteration in WLE/MLE estimation  2   | Maximal change  2.8413 \nIteration in WLE/MLE estimation  3   | Maximal change  2.5741 \nIteration in WLE/MLE estimation  4   | Maximal change  2.1203 \nIteration in WLE/MLE estimation  5   | Maximal change  2.3278 \nIteration in WLE/MLE estimation  6   | Maximal change  0.4228 \nIteration in WLE/MLE estimation  7   | Maximal change  0.0637 \nIteration in WLE/MLE estimation  8   | Maximal change  0.0071 \nIteration in WLE/MLE estimation  9   | Maximal change  8e-04 \nIteration in WLE/MLE estimation  10   | Maximal change  1e-04 \n----\n WLE Reliability= 0.97 \n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"false\"}\nggplot(data.frame(theta = persons.mod$theta), aes(x = theta)) +\n  geom_histogram(binwidth = 0.2, fill = \"steelblue\", color = \"black\") +\n  labs(title = \"Distribution of Person Ability Estimates\", x = \"Ability (Theta)\", y = \"Frequency\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\n\n\nAlso see the theta distributions in the chart. They do not look nice as this is a study run on simulated data.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nthr <- TAM::tam.threshold(mod)\nthr\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                    Cat1       Cat2       Cat3     Cat4\nCon-rater1    -8.3634338 -4.8121033 -1.2173767 2.406464\nCon-rater2    -4.3898621 -0.8385315  2.7560120 6.380035\nCon-rater3    -0.5052795  3.0462341  6.6957092       NA\nProAd-rater1  -8.5340881 -4.4764709 -0.6612854 3.438995\nProAd-rater2  -4.5606995 -0.5028992  3.3122864 7.412567\nProAd-rater3  -0.6759338  3.3818665  7.2305603       NA\nLang-rater1  -10.0357361 -5.8611145 -1.8104553 2.433380\nLang-rater2   -6.0621643 -1.8877258  2.1631165 6.406952\nLang-rater3   -2.1775818  1.9970398  6.0768127       NA\nNar-rater1    -9.4873352 -5.3533630 -1.3221130 2.769928\nNar-rater2    -5.5137634 -1.3799744  2.6514587 6.743317\nNar-rater3    -1.6289978  2.5047913  6.5700989       NA\n```\n\n\n:::\n:::\n\n\n\n\n\n**Ordered thresholds** are crucial to ensure that the categories are functioning properly. For example, for **Con-rater1**, the thresholds are: -8.36, -4.81, -1.21, and 2.41\n\nThese thresholds are in increasing order, which indicates that the rating scale is working as intended for **Con-rater1**—the higher categories represent more difficult levels to achieve.\n\n**Raters 1, 2, and 3 Comparison:**\n\n-   **Rater Differences:** There are noticeable differences between raters in their thresholds. For example:\n\n    -   **Con-rater1** has very negative thresholds, starting at -8.36 for Cat1, while **Con-rater3** starts much higher, with thresholds beginning at -0.50.\n\n    -   This suggests that **Rater1** is much stricter or uses a harsher scale, while **Rater3** is more lenient, with easier transitions between categories. For instance, it is harder for essays to move from a \"1\" to a \"2\" under Rater1’s scoring compared to Rater3.\n\n    -   There are some NAs which actually I have no idea about. Peobably these inconsistancies occur due to the simulated data.\n\n## 4. Visualizing the Results\n\nTo make the interpretation more intuitive, we can visualize the item difficulty and rater severity using a **dot plot** for the difficulty estimates, which can help us compare how each domain and rater behaves. Here’s how we can generate these plots in R.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nfacet_params<-mod[[\"xsi.facets\"]][[\"parameter\"]]\ndomain_params<-facet_params[1:4]\n\nf1 <- ggplot(data = persons.mod, aes(x = theta))+\n  geom_dotplot(binwidth = .1, stackdir = \"down\") + \n  theme_bw()  +\n  scale_y_continuous(name = \"\", breaks = NULL) +\n  scale_x_continuous(breaks=seq(-6, 6, .6), limits=c(-6, 6), \n                     position =  \"top\") + \n  theme(axis.title.y = element_blank(), \n        axis.title.x=element_blank(),\n        axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())+\n  labs(title = \"Persons\") + \n  coord_flip()\n\nf2 <- mod$xsi.facets %>%\n  filter(str_starts(parameter, \"rater\")) %>%\n  ggplot(aes(x = xsi)) +\n  geom_text(aes(y = 0, label = parameter), nudge_y = 0.05, size = 3) +\n  theme_bw() +\n  scale_y_continuous(name = \"\", breaks = NULL) +\n  scale_x_continuous(breaks = seq(-6, 6, .5), limits = c(-6, 6), position = \"top\") + \n  theme(axis.title.y = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        axis.title.x = element_blank()) +\n  labs(title = \"Raters\") + \n  coord_flip()\n\nf3 <- mod$xsi.facets %>%\n  filter(parameter %in% domain_params) %>%\n  ggplot(aes(x = xsi)) +\n  geom_text(aes(y = 0, label = parameter), nudge_y = 0.05, size = 3) + \n    theme_bw()  +\n    scale_y_continuous(name = \"\", breaks = NULL) +\n    scale_x_continuous(breaks=seq(-2, 2, .2), limits=c(-2, 2), \n                       position =  \"top\") + \n    theme(axis.title.y=element_blank(),\n          axis.text.y=element_blank(),\n          axis.ticks.y=element_blank(),\n          axis.title.x= element_blank())+\n    labs(title = \"Domain\") + \n    coord_flip()\n\nplot_grid(f1, f2, f3, nrow = 1, rel_widths = c(0.7, .15, .15))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/Wright Map-1.png){width=672}\n:::\n:::\n\n\n\n\n\nThis final chart is developed as an alternative to `wrightmap`. Each facet can be seen easily on it. There are four grids. The first is the person thetas. We have seen this above. The second is the rater facet. The strictness of the raters are very distinctive. Actually the only real data here is rater 2 and the others were simulated using it to be stricter and lenient. So we exactly see what the data is about. The third grid is about the domains/item difficulty. Pompt Adherence is the most difficult domain. Content, Narrative and Language follows it respectively.\n\n## 5. Conclusion\n\nIn this post, we explored the **Multi-Facet Rasch Model (MFRM)** using simulated essay scores rated by multiple raters across four different domains: Content, Prompt Adherence, Language, and Narrativity. The model helped us account for the varying levels of item difficulty and the potential differences in rater severity. By fitting the MFRM and examining key model outputs—like **Infit/Outfit statistics** and **thresholds**—we identified areas where raters were either more lenient or more severe, and items that displayed more noise or predictability than expected.\n\nThe high **WLE reliability** of 0.97 indicates that the model provides consistent and accurate estimates of person abilities. However, the rater-specific thresholds revealed some important differences in how each rater scored the essays, with certain raters being significantly stricter or more lenient. This highlights the importance of accounting for rater bias in assessments that rely on subjective judgments, such as essay scoring.\n\nGoing forward, addressing these rater differences and ensuring well-functioning rating categories can further refine the assessment process. By doing so, we can ensure that the scores are fairer and more representative of true essay quality, free from the influence of individual rater biases. Overall, the MFRM proves to be a valuable tool in maintaining the validity and reliability of assessments involving subjective judgments.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}