{
  "hash": "49196967073f019012e0fe61d403c3a4",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Language Detection & Sentiment Analysis with Local LLMs\"\ndescription: |\n  In this post we will work on how to automate tasks such as language detection and sentiment analysis with R using an LLM on the local computer. \nauthor:\n  - name: Ali Emre KaragÃ¼l\n    orcid: 0000-0002-5820-8643\n    email: aliemrekaragul@gmail.com\n    affiliations:\n      - name: University of Economics & Technology\n        city: Ankara\n        url: https://www.etu.edu.tr/tr\n      - name: Gazi University\n        city: Ankara\n        url: https://gazi.edu.tr/\ndate: 2024-11-27\ncategories: [ollamar, mall, dplyr, stringr, purrr, tidyverse]\nimage: \"image.png\"\noutput:\n    self_contained: false\n    code_folding: false\nreference-location: margin\nlightbox: true\ncrossref:\n  fig-labels: alpha a    \n  tbl-labels: alpha a    \n  subref-labels: roman i \n  chapters: true\n---\n\n\n\n## Introduction\n\nI have recently received an e-mail from Posit about their recent developments. They have released the BETA version of their new IDE, Positron, for data scientists along with several R packages and a chat bot customized for shiny apps, [Shiny Assistant](https://shiny.posit.co/blog/posts/shiny-assistant/?mkt_tok=NzA5LU5YTi03MDYAAAGXLPFmi2C2rOPNBjAVW1SwVe8Qy-c6SQuWQRpE_vGdNHqd7B4AyQmuYvkCYDOH6eTQdWJNQBXM9mal0BiSbCCx1Za6LGqDU1bHUgkiTfEf9wq7). The packages are [elmer](https://elmer.tidyverse.org/?mkt_tok=NzA5LU5YTi03MDYAAAGXLPFmi3BulXH3PmcuNNnpj_xia93mA1SBo0ctMWi_dJF8-kNxZujHYP6JrQL8KemkBg-dLEqQ2jOd4NIfmwFLn0_FcQktFLjyYUeN8fApLCPN), [pal](https://github.com/simonpcouch/pal?mkt_tok=NzA5LU5YTi03MDYAAAGXLPFmi_FVwNiBAxVKZUFZ6hp7Sb71tc8kSSBlnYK_26jg7kx7JfrC5FDchiJAcsuN4TuZx3MnyRb-RKQf1pES_IQi8clt65qzJvUxbDF5Y4P6), and [mall](https://github.com/mlverse/mall?mkt_tok=NzA5LU5YTi03MDYAAAGXLPFmi4p_MDoSVwPizwPfCnziIInxDDGJOzYifsoOENazWrJEY8UUo_KmbkpRXmbEbTaXO6_XjSfEbjyB1AD4-_BpVMD0KWV8aBvmPkedBebu). Today, we will try `mall` with a local LLM to automate several tasks and run a sentiment analysis.\n\nA few years ago, running an LLM locally would be a dream for most of the data scientists. However, with the recent developments in the field, there are many free-to-use LLMs that can be run on your local machine. I am choosing my words carefully here. Since Meta released llama, they advertise it as open-source. Yet, it is not open-source in the sense that you can see the algorithm behind it or/and the data used to train it. It is open-source in the sense that you can use it for free. In this regard, it is more of a free-to-use tool than an open-source tool. However, I should also appreciate the effort behind it as we can use a strong LLM freely on our local computers.\n\n## Getting Started\n\nEnough of politics. Let's get started by downloading ollama to our local machine. \n\n### Download Ollama\n\nOllama is an open-source LLM service tool that helps users to utilize LLMs locally with a single line of command. You can download it from [here](https://ollama.com/download) depending on your operating system. Yet, I will run you through Windows rather than Linux or MacOS. \n\nClick `Windows` on the download page and then click the download button. Once the download is complete, open the downloaded installer. As far as I remember, the installer completes without any prompting.\n\nBefore we use R, we need to set some basics. A local LLM is highly dependent on your hardware. You need a good CPU, RAM, and VRAM to run it. Today, I will use `llama3.1 8b`. `8b` in the model name refers to the number of parameters that the model is trained with. The more parameters, the more accurate the model. However, the more parameters, the more hardware you need.\n\n::: {.callout-note title=\"Requirements for llama3.1 8b\"}\n\nCPU \\>= 8 cores\n\nRAM \\>= 16 GB \n\nVRAM \\>= 8GB \n\nNVIDIA RTX 3070 or better \n\n:::\n\nIf you do not have sufficient hardware, you can use a smaller model such as `llama3.2` which comes with `1b` and `3b`. `Gemma:2b` is also a viable option. If you have even a better computer, you can try other models with higher parameters. You can see the list of all available models [here](https://ollama.com/search).\n\n### Install the Model\n\nThere are many ways to install the model. We will discuss two here. You can either use the terminal or `mall` package in R. My personal preference is to use the terminal as we will need to terminate ollama to end the memory usage when we are done. However, if you are not familiar with the terminal, you can use R.\n\n#### Install the Model on the Terminal\n\nOnce you have decided on your model, open a terminal. You can do that by searching for `Windows PowerShell` and running it. If you are Rstudio user, you can also open a terminal in Rstudio. They will both work for our use case.\n\n::: column-margin\n![](images/empty_terminal.png){.lightbox fig-alt=\"PowerShell Terminal\"}\n:::\n\nOn the terminal, type the following command with the model name of your preference. `ollama run <model name>`\n\n::: column-margin\n![](images/empty_terminal_2.png){.lightbox fig-alt=\"RStudio Terminal\"}\n:::\n\n``` {.terminal}\nollama run llama3.1:8b\n```\n\nThis will install the model to your computer, if it is not installed already, and then it will start a chat session with the model. You can chat with the bot on the terminal directly. To end the session, simply type `/bye` or `CTRL + C`.\n\nYou can install as many models as you like as long as your hardware allows. To see all the models that you have installed so far, run `ollama list` command on the terminal.\n\n::: column-margin\n![](images/list_models.png){.lightbox}\n:::\n\n#### Install the Model via R\n\nTo install a model via R, you need to load the `mall` package. In the mall package, `pull(\"<model name>\")` function is used to install a model. `test_connection()` is used to see if your local LLM up and running. \n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nlibrary(ollamar)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'ollamar' was built under R version 4.4.2\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'ollamar'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:stats':\n\n    embed\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:methods':\n\n    show\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"false\"}\nmodel_name <- \"llama3.1:8b\"\nollamar::pull(model_name)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n<httr2_response>\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nPOST http://127.0.0.1:11434/api/pull\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nStatus: 200 OK\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nContent-Type: application/x-ndjson\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nBody: In memory (861 bytes)\n```\n\n\n:::\n:::\n\n\n\n`list_models()` is used to see the models that you have installed so far. That one is more informative than the terminal command as it gives us the parameter size and the quantization level too.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nollamar::list_models()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             name   size parameter_size quantization_level            modified\n1        gemma:2b 1.7 GB             3B               Q4_0 2024-12-05T01:49:40\n2     llama3.1:8b 4.9 GB           8.0B             Q4_K_M 2024-12-05T14:25:24\n3 llama3.2:latest   2 GB           3.2B             Q4_K_M 2024-11-29T20:35:04\n4       llama3:8b 4.7 GB           8.0B               Q4_0 2024-12-05T01:19:43\n5   llama3:latest 4.7 GB           8.0B               Q4_0 2024-11-30T00:58:23\n```\n\n\n:::\n:::\n\n\n\nYou can also test the model by giving a prompt. The `generate` function is used to generate a response from the model.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nollamar::generate(model_name, \"Tell me a joke about statistics.\", output = \"text\") \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Here's one:\\n\\nWhy did the statistician turn down the invitation to the party?\\n\\nBecause he already had a 99% probability of being bored and a 1% chance of meeting interesting people.\\n\\nHope that made you laugh!\"\n```\n\n\n:::\n:::\n\n\n\n## Task Automation\n\nIf you have come so far, you are ready to use your local LLMs for anything. Let's see some example usage. \n\n### Language Detection\nMost LLMs are multi-lingual. You can use them to detect the language of a given text such as a comment or a review. Let's build such an automation. We will use a dataset of global comments from YouTube videos on [Kaggle](# data source: https://www.kaggle.com/code/tanmay111/youtube-comments-sentiment-analysis/input). You can download the `csv` file directly from [my Google drive](https://drive.google.com/file/d/16uUKmQKbNOSffKr7X8qvYqHDVHANAAON/view?usp=sharing) too. \n\n#### Preparing the Data\nThe data is too large, so in this part, we will investigate and select a subset of 20 rows that contain multiple languages, emojis, urls etc. We will then detect the language of each comment in the subset using our local LLM.\n\nWe will be using packages such as `dplyr`, `stringr`, `tidyverse`, and `purrr` to manipulate the data. Also `mall` package, having very useful functions such as `llm_sentiment()`, `llm_classify()`, `llm_extract()`, `llm_custom()` etc, will be used to interact with the LLM.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(purrr) \nlibrary(mall)\nlibrary(tidyverse)\n\n# set a seed parameter to make the results reproducible\nset.seed(123)\n```\n:::\n\n\n\nLet's load the data and see the structure of it.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nglobal_comments <- read.csv(\"GBcomments.csv\")\nsummary(global_comments)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   video_id         comment_text          likes             replies         \n Length:273551      Length:273551      Length:273551      Length:273551     \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n```\n\n\n:::\n:::\n\n\n\nWe need to change the class of `likes` and `replies` to numeric. Also, I have checked the data and detected many duplicate rows. So, we need to get rid of the duplicate data entries. Also let's drop the rows with NA values.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nglobal_comments$likes <- as.numeric(global_comments$likes)\nglobal_comments$replies <- as.numeric(global_comments$replies)\n\n# drop rows with the same comment text in the same video\nglobal_comments <- global_comments %>% \n  distinct(video_id, comment_text, .keep_all = TRUE) %>% \n  drop_na()\n\nglobal_comments <- global_comments \n\nsummary(global_comments)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   video_id         comment_text           likes             replies       \n Length:152589      Length:152589      Min.   :    0.00   Min.   :   0.00  \n Class :character   Class :character   1st Qu.:    0.00   1st Qu.:   0.00  \n Mode  :character   Mode  :character   Median :    0.00   Median :   0.00  \n                                       Mean   :   16.94   Mean   :  13.95  \n                                       3rd Qu.:    0.00   3rd Qu.:   0.00  \n                                       Max.   :60630.00   Max.   :3498.00  \n```\n\n\n:::\n:::\n\n\n\nWe have 152589 rows of data. For the demonstration purposes, I will select a couple of videos with comments in multiple languages. There are many ways to detect videos with comments in multiple languages. My approach will be to search for specific strings in Korean, Arabic and German. For example, the first letter of \"hello\" in Korean is \"í•œ\" according to Google Translate. Let's search for this letter in the comments and select the video with the most comments. Also, utilize a similar approach for Arabic and German. Remember we are doing this to create the perfect subset of comments with multiple languages.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nkorean_letter <- \"í•œ\" \narabic_letter <- \"Ø§\" # the first letter of arabic letters\ngerman_word <- \" und \" # \"und\" is \"and\" in German\n\n\nvid_w_korean_comments <- global_comments %>% \n  filter(str_detect(comment_text,korean_letter)) %>% \n  group_by(video_id) %>% \n  summarise(n_comments = n()) %>% \n  arrange(desc(n_comments)) %>% \n  head(1)\n\nvid_w_arabic_comments <- global_comments %>%\n  filter(str_detect(comment_text, arabic_letter)) %>%\n  group_by(video_id) %>%\n  summarise(n_comments = n()) %>%\n  arrange(desc(n_comments)) %>%\n  head(1)\n\nvid_w_german_comments <- global_comments %>%\n  filter(str_detect(comment_text, german_word)) %>%\n  group_by(video_id) %>%\n  summarise(n_comments = n()) %>%\n  arrange(desc(n_comments)) %>%\n  head(1)\n\n# merge these rows\nvids_multiple_langs <- rbind(vid_w_korean_comments, vid_w_arabic_comments, vid_w_german_comments)\nprint(vids_multiple_langs)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 Ã— 2\n  video_id    n_comments\n  <chr>            <int>\n1 HuoOEry-Yc4         10\n2 jt2OHQh0HoQ         11\n3 11S5tcT2Tm0          1\n```\n\n\n:::\n:::\n\n\n\nWe have selected 3 videos with comments in multiple languages. Let's create a dataset with these videos and their comments called `dat`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\ndat <- global_comments %>% \n  filter(video_id %in% vids_multiple_langs$video_id)\nsummary(dat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   video_id         comment_text           likes            replies       \n Length:897         Length:897         Min.   : 0.0000   Min.   : 0.0000  \n Class :character   Class :character   1st Qu.: 0.0000   1st Qu.: 0.0000  \n Mode  :character   Mode  :character   Median : 0.0000   Median : 0.0000  \n                                       Mean   : 0.6722   Mean   : 0.1538  \n                                       3rd Qu.: 0.0000   3rd Qu.: 0.0000  \n                                       Max.   :31.0000   Max.   :12.0000  \n```\n\n\n:::\n:::\n\n\n\nThere are 897 comments in total. I checked it with a glimpse and it actually contains comments in many languages. For example:\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\ntail(dat$comment_text, 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"Marvelous, as Always!\"                                                                                                                                                                                                                                                                                  \n [2] \"Bellisimo :3Saludos desde Mexico cdmx\"                                                                                                                                                                                                                                                                  \n [3] \"Pure! Serene! and Marvelous!\"                                                                                                                                                                                                                                                                           \n [4] \"So diverse. Much ethnic. Wow.\"                                                                                                                                                                                                                                                                          \n [5] \"Enough said\"                                                                                                                                                                                                                                                                                            \n [6] \"ğŸ˜˜ğŸ˜˜ğŸ˜˜ğŸ˜±ğŸ˜±\"                                                                                                                                                                                                                                                                                                  \n [7] \"woow.. eres magnifica, por ti me apasione con el violin y compre el mio... soy tu mas grande admiradora, dios y algun dia me lo permita conocerte es mi mayor sueÃ±o y aprender a tocar el violin.. sigue triunfando y robando los carazones de las personas asi como me lo robaste a mi con tu melodia.\"\n [8] \"Cuando veniste a Guadalajara y escuche esta canciÃ³n I couldnt believe it was just like no mames ğŸ˜ğŸ˜ğŸ˜\"                                                                                                                                                                                                    \n [9] \"man kann richtig sehen wie dir das Tanze und Violine spielen spass und freude macht. ğŸ‘\\\\n\\\\nSo macht das zuschauen nochmal so viel spass.\\\\n\\\\nSuper Videoâ˜ºï¸ğŸ‘\"                                                                                                                                           \n[10] \"Heisses Outfit , dass der Sound gut ist, ist ja schon bald selbstverstÃ¤ndlich.\"                                                                                                                                                                                                                         \n```\n\n\n:::\n:::\n\n\n\nI can see many languages except for Korean and Arabic in the tail. :D \nLet's select a random subset of this data to test our language detection bot.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# select random 20 comments.\ndat <- dat %>% \n  sample_n(20)\nprint(dat$comment_text)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"YOU ARE SO CLOSE TO 10 MILLION!!!\"                                                                                                                                                                                                                                                                              \n [2] \"ğŸ˜\"                                                                                                                                                                                                                                                                                                              \n [3] \"Also watch https://youtu.be/7QkYTgDMpCs\"                                                                                                                                                                                                                                                                        \n [4] \"Iâ¤ï¸U ğŸ’‹ğŸ’‹ğŸ’‹ğŸ’‹ğŸ’‹ğŸ’‹ğŸ’‹ğŸ’‹ğŸ’‹ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜˜ğŸ˜˜ğŸ˜˜ğŸ˜˜ğŸ˜˜ğŸ˜˜ğŸ˜˜ğŸ˜˜ğŸ˜˜ğŸ’–ğŸ’–ğŸ’–ğŸ’–ğŸ’–ğŸ’–ğŸ’–ğŸ’–ğŸ˜»ğŸ˜»ğŸ˜»ğŸ˜»ğŸ˜»ğŸ˜»ğŸ‘¸ğŸ‘¸ğŸ‘¸ğŸ‘¸ğŸ‘¸ğŸ‘¸ğŸ’ŸğŸ’ŸğŸ’ŸğŸ’ŸğŸ’ğŸ’ğŸ’ğŸ’ğŸ’“ğŸ’“ğŸ’“ğŸ’—ğŸ’—ğŸ’—ğŸ’œğŸ’œğŸ’œğŸ’‹ğŸ’‹ğŸ’‹ğŸ‘°ğŸ‘°ğŸ‘°\"                                                                                                                                                                                                                                      \n [5] \"Ya lo tengo y estoy escribiendo de acÃ¡ del iPhone me anda de 10\"                                                                                                                                                                                                                                                \n [6] \"Amazing video I love the Bollywood touch in this video keep going Lindsey!!\"                                                                                                                                                                                                                                    \n [7] \"Where's iPhone 9? Just skips to iPhone x. Lol okay nice math there Apple\"                                                                                                                                                                                                                                       \n [8] \"Ø¹Ù…Ù„Øª Ù‚Ù†Ø§Øª Ø¬Ø¯ÙŠØ¯Ø© ØªØ®ØªØµ ÙÙŠ ØªÙ‚Ø¯ÙŠÙ… Ø£Ù†Ø¬Ø­ Ùˆ Ø£Ø³Ù‡Ù„ ÙˆØµÙØ§Øª Ø§Ù„Ø­Ù„ÙˆÙŠØ§ØªØŒ ØªØ¹Ø§Ù„Ùˆ Ø´ÙˆÙÙˆ Ø§Ù„ÙØ¯ÙŠÙˆÙ‡Ø§Øª Ø§Ù„ØªÙŠ Ø§Ø¹Ù…Ù„Ù‡Ø§ ÙˆØ§Ù„Ù„Ù‡ Ù…Ø§ Ø±Ø­ ØªÙ†Ø¯Ù…Ùˆ â¤\"                                                                                                                                                                                                 \n [9] \"Nothing new..\\\\nIphoneX is copy of essiential phone.\\\\nAndy Rubin has developed more advanced features than iPhoneX.\\\\nIPHONE X ğŸ‘ğŸ‘ğŸ‘\"                                                                                                                                                                            \n[10] \"Lol @22:46:09\\\\n\\\\nTim's kinda an ass\"                                                                                                                                                                                                                                                                          \n[11] \"3 years late on wireless charging, 3 years late on oled technology , old facial recognition tech enhanced by old IR tech. So your late on just about every front....what to do? I know, lets lose what makes our product instantly recognizable! This is Apples windows 8, Samsung are bound to be loving this.\"\n[12] \"https://youtu.be/Kp_DBWtS6SU\"                                                                                                                                                                                                                                                                                   \n[13] \"I love your music videos their always great and inspiring!\"                                                                                                                                                                                                                                                     \n[14] \"I like for Linsey. Love yours full themes. Great job.\"                                                                                                                                                                                                                                                          \n[15] \"I love that blond hair!!!!\"                                                                                                                                                                                                                                                                                     \n[16] \"é»’é«ªã‚‚ã‚‚å¯æ„›ã™ãğŸ˜ğŸ’•ğŸ’•\"                                                                                                                                                                                                                                                                                            \n[17] \"Sana is my damn bias wrecker, i swear. between her and Nayeon it's so hardğŸ˜‚â¤\"                                                                                                                                                                                                                                   \n[18] \"where is Steve Wozniak, fuck apple authority\"                                                                                                                                                                                                                                                                   \n[19] \"just one question, wtf?\"                                                                                                                                                                                                                                                                                        \n[20] \"She is BEA-UTIFUL. And this showed a side I've never seen before. Which is not surprising. She's always giving us surprises....she keeps me in my toes\"                                                                                                                                                         \n```\n\n\n:::\n:::\n\n\n\n#### Detecting languages\n\nThat subset looks good. I can see emojis, urls and multiple languages along with English. That's a perfect subset to test the language detection capabilities of our LLM. Here is what we will do:\n1. Attach the model with `llm_use()`.\n2. Define a system prompt for language detection.\n3. OPTIONAL: Define the valid responses that you expect from the LLM. If this is defined, any response that do not fit the valid responses will be replaced with `NA`.\n4. Detect the language of each comment in the data and add the results as a new column.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nllm_use(\"ollama\", model_name, seed = 100, .silent = TRUE)\n\nsys_prompt <- paste(\n  \"You are a language detection bot.\",\n  \"I will provide you with Youtube comments on a video.\",\n  \"Try to detect the language of the comment and reply with the ISO 639-1 language code used in the given comment.\",\n  \"Reply only with the language code.\",\n  \"If you cannot detect a language as the comments might contain emojis or urls only, reply with 'UNDETECTABLE' with uppercase\",\n  \"Some examples:\",\n  \"comment text: 'Thumbs up asap', your response: 'en'.\",\n  \"comment text: 'Hola, Â¿cÃ³mo estÃ¡s?', your response: 'es'.\", \n  \"Here is the comment:\"\n)\n\n# I am not adding 'UNDETECTABLE' to valid responses as the function will tag such cases as NA.\nvalid_responses <- c(\"aa\", \"ab\", \"ae\", \"af\", \"ak\", \"am\", \"an\", \"ar-ae\", \"ar-bh\", \"ar-dz\", \"ar-eg\", \"ar-iq\", \"ar-jo\", \"ar-kw\", \"ar-lb\", \"ar-ly\", \"ar-ma\", \"ar-om\", \"ar-qa\", \"ar-sa\", \"ar-sy\", \"ar-tn\", \"ar-ye\", \"ar\", \"as\", \"av\", \"ay\", \"az\", \"ba\", \"be\", \"bg\", \"bh\", \"bi\", \"bm\", \"bn\", \"bo\", \"br\", \"bs\", \"ca\", \"ce\", \"ch\", \"co\", \"cr\", \"cs\", \"cu\", \"cv\", \"cy\", \"da\", \"de-at\", \"de-ch\", \"de-de\", \"de-li\", \"de-lu\", \"de\", \"div\", \"dv\", \"dz\", \"ee\", \"el\", \"en-au\", \"en-bz\", \"en-ca\", \"en-cb\", \"en-gb\", \"en-ie\", \"en-jm\", \"en-nz\", \"en-ph\", \"en-tt\", \"en-us\", \"en-za\", \"en-zw\", \"en\", \"eo\", \"es-ar\", \"es-bo\", \"es-cl\", \"es-co\", \"es-cr\", \"es-do\", \"es-ec\", \"es-es\", \"es-gt\", \"es-hn\", \"es-mx\", \"es-ni\", \"es-pa\", \"es-pe\", \"es-pr\", \"es-py\", \"es-sv\", \"es-us\", \"es-uy\", \"es-ve\", \"es\", \"et\", \"eu\", \"fa\", \"ff\", \"fi\", \"fj\", \"fo\", \"fr-be\", \"fr-ca\", \"fr-ch\", \"fr-fr\", \"fr-lu\", \"fr-mc\", \"fr\", \"fy\", \"ga\", \"gd\", \"gl\", \"gn\", \"gu\", \"gv\", \"ha\", \"he\", \"hi\", \"ho\", \"hr-ba\", \"hr-hr\", \"hr\", \"ht\", \"hu\", \"hy\", \"hz\", \"ia\", \"id\", \"ie\", \"ig\", \"ii\", \"ik\", \"in\", \"io\", \"is\", \"it-ch\", \"it-it\", \"it\", \"iu\", \"iw\", \"ja\", \"ji\", \"jv\", \"jw\", \"ka\", \"kg\", \"ki\", \"kj\", \"kk\", \"kl\", \"km\", \"kn\", \"ko\", \"kok\", \"kr\", \"ks\", \"ku\", \"kv\", \"kw\", \"ky\", \"kz\", \"la\", \"lb\", \"lg\", \"li\", \"ln\", \"lo\", \"ls\", \"lt\", \"lu\", \"lv\", \"mg\", \"mh\", \"mi\", \"mk\", \"ml\", \"mn\", \"mo\", \"mr\", \"ms-bn\", \"ms-my\", \"ms\", \"mt\", \"my\", \"na\", \"nb\", \"nd\", \"ne\", \"ng\", \"nl-be\", \"nl-nl\", \"nl\", \"nn\", \"no\", \"nr\", \"ns\", \"nv\", \"ny\", \"oc\", \"oj\", \"om\", \"or\", \"os\", \"pa\", \"pi\", \"pl\", \"ps\", \"pt-br\", \"pt-pt\", \"pt\", \"qu-bo\", \"qu-ec\", \"qu-pe\", \"qu\", \"rm\", \"rn\", \"ro\", \"ru\", \"rw\", \"sa\", \"sb\", \"sc\", \"sd\", \"se-fi\", \"se-no\", \"se-se\", \"se\", \"sg\", \"sh\", \"si\", \"sk\", \"sl\", \"sm\", \"sn\", \"so\", \"sq\", \"sr-ba\", \"sr-sp\", \"sr\", \"ss\", \"st\", \"su\", \"sv-fi\", \"sv-se\", \"sv\", \"sw\", \"sx\", \"syr\", \"ta\", \"te\", \"tg\", \"th\", \"ti\", \"tk\", \"tl\", \"tn\", \"to\", \"tr\", \"ts\", \"tt\", \"tw\", \"ty\", \"ug\", \"uk\", \"ur\", \"us\", \"uz\", \"ve\", \"vi\", \"vo\", \"wa\", \"wo\", \"xh\", \"yi\", \"yo\", \"za\", \"zh-cn\", \"zh-hk\", \"zh-mo\", \"zh-sg\", \"zh-tw\", \"zh\", \"zu\")\n\ndat <- dat |>\n  llm_custom(comment_text, sys_prompt, \"language\", valid_resps = valid_responses)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n! There were 3 predictions with invalid output, they were coerced to NA\n```\n\n\n:::\n:::\n\n\n\nLet's see the results.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\ndat <- as_tibble(dat) # convert dat to tibble (optional)\nprint(dat %>% select(language, comment_text))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 20 Ã— 2\n   language comment_text                                                        \n   <chr>    <chr>                                                               \n 1 en       \"YOU ARE SO CLOSE TO 10 MILLION!!!\"                                 \n 2 <NA>     \"\\U0001f60d\"                                                        \n 3 <NA>     \"Also watch https://youtu.be/7QkYTgDMpCs\"                           \n 4 en       \"Iâ¤ï¸U \\U0001f48b\\U0001f48b\\U0001f48b\\U0001f48b\\U0001f48b\\U0001f48b\\Uâ€¦\n 5 es       \"Ya lo tengo y estoy escribiendo de acÃ¡ del iPhone me anda de 10\"   \n 6 en       \"Amazing video I love the Bollywood touch in this video keep going â€¦\n 7 en       \"Where's iPhone 9? Just skips to iPhone x. Lol okay nice math thereâ€¦\n 8 ar       \"Ø¹Ù…Ù„Øª Ù‚Ù†Ø§Øª Ø¬Ø¯ÙŠØ¯Ø© ØªØ®ØªØµ ÙÙŠ ØªÙ‚Ø¯ÙŠÙ… Ø£Ù†Ø¬Ø­ Ùˆ Ø£Ø³Ù‡Ù„ ÙˆØµÙØ§Øª Ø§Ù„Ø­Ù„ÙˆÙŠØ§ØªØŒ ØªØ¹Ø§Ù„Ùˆ Ø´Ùˆâ€¦\n 9 en       \"Nothing new..\\\\nIphoneX is copy of essiential phone.\\\\nAndy Rubin â€¦\n10 en       \"Lol @22:46:09\\\\n\\\\nTim's kinda an ass\"                             \n11 en       \"3 years late on wireless charging, 3 years late on oled technologyâ€¦\n12 <NA>     \"https://youtu.be/Kp_DBWtS6SU\"                                      \n13 en       \"I love your music videos their always great and inspiring!\"        \n14 en       \"I like for Linsey. Love yours full themes. Great job.\"             \n15 en       \"I love that blond hair!!!!\"                                        \n16 ja       \"é»’é«ªã‚‚ã‚‚å¯æ„›ã™ã\\U0001f60d\\U0001f495\\U0001f495\"                    \n17 ko       \"Sana is my damn bias wrecker, i swear. between her and Nayeon it'sâ€¦\n18 en       \"where is Steve Wozniak, fuck apple authority\"                      \n19 en       \"just one question, wtf?\"                                           \n20 en       \"She is BEA-UTIFUL. And this showed a side I've never seen before. â€¦\n```\n\n\n:::\n:::\n\n\n\nNice, we have detected the languages of the comments. We can also see that some comments are tagged as `NA`. These are the comments that contain emojis, urls, or gibberish. \n\n### Sentiment Analysis\n\nFor the sentiment analysis we will use another [Kaggle](https://www.kaggle.com/datasets/arhamrumi/amazon-product-reviews) dataset. You can download it from my Google Drive [here](https://drive.google.com/file/d/1Q5pFKuwwKrwcQnDyBixW98VwVt7cZYnT/view?usp=sharing). The dataset contains Amazon product reviews. We will select random 20 comments and run a sentiment analysis on them.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nreviews <- read.csv(\"reviews.csv\")\nsummary(reviews)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       Id         ProductId            UserId          ProfileName       \n Min.   :    1   Length:35173       Length:35173       Length:35173      \n 1st Qu.: 8794   Class :character   Class :character   Class :character  \n Median :17587   Mode  :character   Mode  :character   Mode  :character  \n Mean   :17587                                                           \n 3rd Qu.:26380                                                           \n Max.   :35173                                                           \n                                                                         \n HelpfulnessNumerator HelpfulnessDenominator     Score      \n Min.   :  0.000      Min.   :  0.000        Min.   :1.000  \n 1st Qu.:  0.000      1st Qu.:  0.000        1st Qu.:4.000  \n Median :  0.000      Median :  1.000        Median :5.000  \n Mean   :  1.558      Mean   :  2.002        Mean   :4.156  \n 3rd Qu.:  1.000      3rd Qu.:  2.000        3rd Qu.:5.000  \n Max.   :203.000      Max.   :219.000        Max.   :5.000  \n NA's   :1            NA's   :1              NA's   :1      \n      Time             Summary              Text          \n Min.   :9.617e+08   Length:35173       Length:35173      \n 1st Qu.:1.268e+09   Class :character   Class :character  \n Median :1.307e+09   Mode  :character   Mode  :character  \n Mean   :1.294e+09                                        \n 3rd Qu.:1.330e+09                                        \n Max.   :1.351e+09                                        \n NA's   :1                                                \n```\n\n\n:::\n:::\n\n\n\n#### Preparing the Data\n\nThe data actually contains a `score` column which is the rating of the product. We will select 4 random reviews for each score from 1 to 5 so that we can also test the LLM performance this time. \n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nreviews_sample <- reviews %>% \n  drop_na() %>%\n  group_by(Score) %>% \n  sample_n(4) %>% \n  ungroup()\n```\n:::\n\n\n#### Running Sentiment Analysis\n\nWe can use `llm_custom()` function again with a well developed system prompt by ourselves. Yet, the package `mall` already contains a function for sentiment analysis called `llm_sentiment()`. Let's try it out:\n\nFirst, attach the model. \nThen run the sentiment analysis on the reviews. We will use the `Text` column as the target variable and `comment_sentiment` as the new column name for the sentiment analysis results.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nllm_use(\"ollama\", model_name, seed = 100, .silent = TRUE)\n\nreviews_sample <- llm_sentiment(reviews_sample, Text, pred_name = \"comment_sentiment\")\n```\n:::\n\n\n\n\nLet's see the results.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nprint(reviews_sample %>% select(comment_sentiment, Score, Text))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 20 Ã— 3\n   comment_sentiment Score Text                                                 \n   <chr>             <int> <chr>                                                \n 1 negative              1 \"I have 2 chihuahua's and they are not at all intereâ€¦\n 2 negative              1 \"I am sure that this coffee tastes good, but I am noâ€¦\n 3 negative              1 \"I bought this gourmet popping corn believing I was â€¦\n 4 negative              1 \"I strongly suspect this caviar, which is widely avaâ€¦\n 5 negative              2 \"I ordered this for my birthday. I got birthday moneâ€¦\n 6 negative              2 \"I love raspberry and chocolate.  I could live on Seâ€¦\n 7 negative              2 \"As with most of the reviews here my tins arrived wiâ€¦\n 8 negative              2 \"These dried strawberries do taste good - indeed, thâ€¦\n 9 negative              3 \"It was my mistake - I thought there were 6 bags ratâ€¦\n10 neutral               3 \"I have been feeding Canidae for a long time, and whâ€¦\n11 negative              3 \"I usually buy this at our local Whole Foods or Harrâ€¦\n12 neutral               3 \"This carbonated product has a nice natural juice taâ€¦\n13 negative              4 \"If you read the first review and then read the compâ€¦\n14 positive              4 \"This Wolfgang Puck coffee tasted great. The vanillaâ€¦\n15 positive              4 \"Being a peanut butter lover, have to keep it out ofâ€¦\n16 positive              4 \"My dogs are bone lovers.  These are a little messy â€¦\n17 positive              5 \"Received aj&iacute; amarillo in a well-wrapped box.â€¦\n18 positive              5 \"This coffee is bold and strong, just how I like it.â€¦\n19 positive              5 \"Having been on a gluten free diet for less than a yâ€¦\n20 neutral               5 \"I have to admit, when I saw this available on vine â€¦\n```\n\n\n:::\n:::\n\n\nWe can see that although our bot is mostly successful, there are some false negative (where the detected sentiment is negative while the score is 4 or 5) results. A larger model would be more successful in this task. Yet, there are no false positive results as all 1 and 2 scores are detected as negative. Naturally, we expect scores 3 to be neutral, but the comment might be more on the negative or positive side. So it is ok if the model detects a 3 as negative or positive rather than neutral.\n\n### Terminate the Ollama Session\n\nWhatever the task is, after using a local LLM, it would be wise to terminate the session to free up the memory. You can do this by running the following command in the terminal.\n\n``` {.terminal}\n Get-Process | Where-Object {$_.ProcessName -like '*ollama*'} | Stop-Process\n```\n\nWhat I do to make sure that the session is terminated is to check the memory usage of the LLM on the Task Manager. Just press `CTRL + ALT + DEL` and select Task Manager on Windows. Then go to the `Processes` tab and order by `Memory`. You will see the memory usage of Ollama. If ollama is not in the list or its memory usage value is close to zero, then the session is terminated. See the screen shot before the termination below. The memory usage is up above of the list. After stopping the process, it was gone.\n\n::: column-margin\n![](images/task_manager_before.png){.lightbox}\n:::\n## Conclusion\nIn this post, we discussed how to use a local LLM for language detection and sentiment analysis. Ollama services were used to install `llama3.1:8b` We used the `mall` package to interact with the LLM. We also discussed how to install the model and how to terminate the session on the terminal. \n\nWe have seen that the LLM is quite successful in detecting the languages of the comments or the sentiments of the reviews. **An important final mark would be to remember that the larger the model, the better the accuracy.** However, the larger the model, the more hardware you need. So, it is always a trade-off between accuracy and resources.\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}