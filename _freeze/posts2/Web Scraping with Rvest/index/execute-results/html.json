{
  "hash": "1881e20eb1da6efcff0a7acac9a3ae38",
  "result": {
    "markdown": "---\ntitle: \"[under development] Web scraping with Rvest package\"\n#preview: images/paste-A04B6DB6.png\ndescription: |\n  \"Rvest\" is a package for web scraping and harvesting which is developed by Hadley Wickham. He states that he was inspired by python libraries such as \"beautiful soup\" and \"RoboBrowser\".\nauthor:\n  - name: Ali Emre Karagül\n    orcid: 0000-0002-5820-8643\n    email: aliemrekaragul@gmail.com\n    affiliations:\n      - name: TOBB ETU- University of Economics & Technology\ndate: 2023-01-06\ncategories: [Rvest, Wordcloud, Data-viz]\nimage: \"image.png\"\n---\n\n\n\n\n## Introduction\n\nIn this post, we will delve into harvesting a web page, [Ekşi Sözlük](https://eksisozluk.com/). This process won't include the automation of the process. Ekşi Sözlük is a reddit-like web site where users share their ideas on certain topics. Our target topic is \"veri bilimi\" (a.k.a. data science in English).\n\nThe R packages that we use in this post are as follows:\n\n\n::: {.cell code_folding='false'}\n\n```{.r .cell-code  code-fold=\"false\"}\nlibrary(rvest)\nlibrary(dplyr)\nlibrary(wordcloud)\nlibrary(tm)\n```\n:::\n\n\n## Processing\n\nFirst things first; we start by introducing the webpage that we want to harvest.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nhtml <- read_html(\"https://eksisozluk.com/veri-bilimi--3426406\")\n```\n:::\n\n\nRvest allos us to collect any type of HTML tag from the current page. Let's suppose we would like to collect all the links in a topic page:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nlinks <- html %>% html_nodes(\"a.url\")  %>%  html_attr(\"href\")\nlinks\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"http://e-k.in/introduction-to-data-science/\"                                                     \n[2] \"https://www.edx.org/course/introduction-big-data-apache-spark-uc-berkeleyx-cs100-1x#.VMK0q3v-t3U\"\n```\n:::\n:::\n\n\nOr maybe we would like to collect all the entries on the given page:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nentries        <- html %>% html_nodes(\".content\") %>%html_text()    \n\n#Let's see the first three entries:\nhead(entries, 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"\\r\\n    internette milyonlarca veri var. bu milyonlarca veriyi işleyen milyonlarca uygulama var. işte bu uygulamalar sayesinde var olan veriyi kullanıp yeni veri uygulamaları yaratma işine veri bilimi denir.\\r\\n  \"\n[2] \"\\r\\n    istatistik ile cok alakali bilim. (bkz: veri madenciligi)\\r\\n  \"                                                                                                                                              \n[3] \"\\r\\n    bana veri olmadan çalışan bir adet bilim türü gösterildiği anda varlığını ve özgünlüğünü kabul edeceğim ilim kategorisi.yoksa şununla yarışır bence:(bkz: deney bilimi) *\"                                    \n```\n:::\n:::\n\n\nNow that you have all the entries in a page, it is easy to carry out a text analysis with it. Let's simply create a word cloud:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n#turn entries into corpus\n\nentries<-Corpus(VectorSource(entries))\n\n#apply several functions such as remove punctuation or numbers etc.\n\nentries <- entries %>%\n  tm_map(removeNumbers) %>%\n  tm_map(removePunctuation) %>%\n  tm_map(stripWhitespace)\nentries <- tm_map(entries, content_transformer(tolower))\n\n#turn into a matrix\nterm_matrix <- as.matrix(TermDocumentMatrix(entries) )\n\n#frequency table:\nword_freqs <- sort(rowSums(term_matrix),decreasing=TRUE) \nword_freqs <- data.frame(word=names(word_freqs),freq=word_freqs )\n\n## word cloud:\nwordcloud(words = word_freqs$word, freq = word_freqs$freq, min.freq = 1,          \n          max.words=200, random.order=FALSE, rot.per=0.35,            \n          colors=brewer.pal(8, \"Dark2\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n## Conclusion\n\nThis post is not complete, yet will be completed soon.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}